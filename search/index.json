[{"content":"哎，本来30号就可以到广州的，因为广州的天气所以30号基本上所有到广州的飞机都取消了。真是没办法的，最近广州的雷雨天气太多了，也能理解吧。 索性我就直接改签到了5月1号早上最早的一班飞机飞广州了。\n来看看日出的北京国际大兴机场，被誉为“新世界七大奇迹”之首。 日出真的太美咯。 不多废话了，直接出发说一下这几天的行程。\n行程一览 我算了一下计划大概是这样的，因为五一当天才能到达广州然后路程就不计了。\nDay1: 去33小镇逛逛当地的一些文创 Day2: 鳒鱼洲文创园+工农8号+下坝坊+东莞植物园 33小镇打卡 当天就来到的33小镇进行打卡，攻略是在十六番上面看的,当然了我是纯纯自己看得，你也可以在十六番上用自带的一键应用 然后就可以按照当前规划的路线进行游玩了。其实总体我逛下来觉得还是不错的，但是没必要为了专门拍照去一趟哈哈哈。\n但是！！！ 咱如果就是想要去拍照，咱就为了拍照去一趟。里面大多数是一些文创的物品，还有一些音乐节和展会之类的。 大多数是西餐店和一些港式茶餐厅之类的。虽然我没进去品尝但是看门店感觉起来还是不错的(没打广告哈哈)\n其实我这个人出去玩儿有一个特点就是\n在自己能接受的范围之内想吃什么就吃什么，想买什么就买什么。 其实大家出去玩儿就是为了玩儿的开心玩儿的舒服，平常也会遇到不少宰客之类的，但我觉得一般来说都比较少数吧，只是希望大家自己不吃亏玩儿的开心就好啦。 \u003c!DOCTYPE html\u003e ","date":"2024-05-01T00:00:00Z","image":"https://img11.360buyimg.com/ddimg/jfs/t1/244924/12/7691/82854/663216bfF4f4f3c1a/985a4b4d1b72840a.jpg","permalink":"http://localhost:1313/tourists/dongguan/","title":"旅行日记-广东-东莞"},{"content":"配套Bilibili视频已经更新：点我观看\n准备SealOS 机器信息如下：\n服务器名称 IP Role ready-kubernetes-master1 10.1.11.100 Control-Plane ready-kubernetes-master2 10.1.11.101 Control-Plane ready-kubernetes-master3 10.1.11.102 Control-Plane ready-kubernetes-node1 10.1.11.103 Node ready-kubernetes-node2 10.1.11.104 Node ready-kubernetes-node3 10.1.11.105 Node 通过SealOS部署的前提条件 SealOS For Kubernetes 每个集群节点应该有不同的主机名。主机名不要带下划线。 所有节点的时间需要同步。 需要在 K8s 集群的第一个 master 节点上运行 sealos run 命令，目前集群外的节点不支持集群安装。 建议使用干净的操作系统来创建集群。不要自己装 Docker！ 支持大多数 Linux 发行版，例如：Ubuntu、CentOS、Rocky linux。 支持 Docker Hub 中的所有 Kubernetes 版本。 支持使用 Containerd 作为容器运行时。 在公有云上安装请使用私有 IP。 获取当前稳定版本的SealOS列表 # 获取非beta版本 curl --silent \u0026#34;https://api.github.com/repos/labring/sealos/releases\u0026#34; | jq -r \u0026#39;map(select(.tag_name | test(\u0026#34;beta\u0026#34;; \u0026#34;i\u0026#34;) | not)) | .[].tag_name\u0026#39; 下载最新稳定版本的SealOS，版本号为v4.3.7 # 在一台主机上执行就行了 VERSION=v4.3.7 wget https://mirror.ghproxy.com/https://github.com/labring/sealos/releases/download/${VERSION}/sealos_${VERSION#v}_linux_amd64.tar.gz \\ \u0026amp;\u0026amp; tar zxvf sealos_${VERSION#v}_linux_amd64.tar.gz sealos \u0026amp;\u0026amp; chmod +x sealos \u0026amp;\u0026amp; mv sealos /usr/bin 验证SealOS是否安装完成 [root@localhost ~]# sealos version SealosVersion: buildDate: \u0026#34;2023-10-30T16:19:05Z\u0026#34; compiler: gc gitCommit: f39b2339 gitVersion: 4.3.7 goVersion: go1.20.10 platform: linux/amd64 正常能显示出来版本号信息就表示安装正常。\n快速部署高可用集群 默认使用的容器运行时为Containerd 开始使用sealOS来部署多节点集群 sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 \\ --masters 10.1.11.100,10.1.11.101,10.1.11.102 \\ --nodes 10.1.11.103,10.1.11.104,10.1.11.105 -p 123..com --masters: kubernetes-master的节点地址列表 --nodes: kubernetes-node的节点地址列表 -p: 远程主机的SSH登录密码 注意部署的时候注意服务器的HostName必须唯一不冲突\n2. 检查是否安装成功 出现如下内容表示安装成功\n2024-04-11T15:20:01 info Executing pipeline RunGuest in CreateProcessor. ℹ️ Using Cilium version 1.13.4 🔮 Auto-detected cluster name: kubernetes 🔮 Auto-detected datapath mode: tunnel 🔮 Auto-detected kube-proxy has been installed 2024-04-11T15:20:03 info succeeded in creating a new cluster, enjoy it! 2024-04-11T15:20:03 info ___ ___ ___ ___ ___ ___ /\\ \\ /\\ \\ /\\ \\ /\\__\\ /\\ \\ /\\ \\ /::\\ \\ /::\\ \\ /::\\ \\ /:/ / /::\\ \\ /::\\ \\ /:/\\ \\ \\ /:/\\:\\ \\ /:/\\:\\ \\ /:/ / /:/\\:\\ \\ /:/\\ \\ \\ _\\:\\~\\ \\ \\ /::\\~\\:\\ \\ /::\\~\\:\\ \\ /:/ / /:/ \\:\\ \\ _\\:\\~\\ \\ \\ /\\ \\:\\ \\ \\__\\ /:/\\:\\ \\:\\__\\ /:/\\:\\ \\:\\__\\ /:/__/ /:/__/ \\:\\__\\ /\\ \\:\\ \\ \\__\\ \\:\\ \\:\\ \\/__/ \\:\\~\\:\\ \\/__/ \\/__\\:\\/:/ / \\:\\ \\ \\:\\ \\ /:/ / \\:\\ \\:\\ \\/__/ \\:\\ \\:\\__\\ \\:\\ \\:\\__\\ \\::/ / \\:\\ \\ \\:\\ /:/ / \\:\\ \\:\\__\\ \\:\\/:/ / \\:\\ \\/__/ /:/ / \\:\\ \\ \\:\\/:/ / \\:\\/:/ / \\::/ / \\:\\__\\ /:/ / \\:\\__\\ \\::/ / \\::/ / \\/__/ \\/__/ \\/__/ \\/__/ \\/__/ \\/__/ Website: https://www.sealos.io/ Address: github.com/labring/sealos Version: 4.3.7-f39b2339 简单的验证一下kubernetes工作是否正常 [root@ready-kubernetes-master1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION ready-kubernetes-master1 Ready control-plane 5m26s v1.27.7 ready-kubernetes-master2 Ready control-plane 4m50s v1.27.7 ready-kubernetes-master3 Ready control-plane 4m v1.27.7 ready-kubernetes-node1 Ready \u0026lt;none\u0026gt; 3m48s v1.27.7 ready-kubernetes-node2 Ready \u0026lt;none\u0026gt; 3m47s v1.27.7 ready-kubernetes-node3 Ready \u0026lt;none\u0026gt; 3m49s v1.27.7 ","date":"2024-04-15T00:00:00Z","image":"https://img10.360buyimg.com/ddimg/jfs/t1/244560/31/7618/14121/661de12eFfa7f6ba5/0239fa873abb0bd5.jpg","permalink":"http://localhost:1313/kubernetes/sealos%E2%80%94install/","title":"基于SealOS部署高可用的kubernetes集群"},{"content":"那一年，午后的阳光总是那么炽热，仿佛能将一切都融化。课间的操场上，同学们的欢声笑语此起彼伏，汗水在他们的额头上闪耀着光芒。我们在这里挥洒着青春，每一次奔跑都是对生活的热爱和对未来的追逐。\n放学后的路上，几个好朋友们一边走一边讨论着今天学到的新知识。我们的谈话中充满了好奇和探索，每一步都踏在充满希望的道路上。夏天的风，带着甜甜的花香，也带着哥几个对彼此的鼓励和笑声。\n这让我觉得青春是一首永不褪色的歌，而夏天，正是这首歌最美的高潮。在这个季节里，我将用我的笔，记录下每一刻的感动，用心去感受生活的每一分热度。这是我曾经学生时代，是我的青春篇章。\n是的，时间的车轮总是单向前行，带走了我们生命中的许多日子，留下了回忆和成长的痕迹。上学的的夏天，那些无忧无虑的日子，仿佛是一段美好的旅程，虽然无法再次经历，但它们在我心中留下了深刻的印记。\n摄影 希望这个夏天可以给我带来更加美好的回忆吧\n\u003c!DOCTYPE html\u003e 需要原图联系: beilanzhisen@163.com\n","date":"2024-03-17T00:00:00Z","image":"https://img13.360buyimg.com/ddimg/jfs/t1/227509/9/15108/245995/65f6f6e1F8dfc4427/2fce5d471f06a599.jpg","permalink":"http://localhost:1313/tourists/119/","title":"日记-又开始怀念夏天"},{"content":"时间过得真快啊，一转眼就是2024年了，窗外的鞭炮声断断续续的传来，似乎在诉说着新春来临的喜悦。\n我想这也是\u0026quot;年\u0026quot;带给人们最朴素和最深刻的记忆吧。\n我不由自主地回忆起童年欢度春节的情景。虽然人们常说现在的年味渐淡，变得简约而平淡，但仔细深思，其实变的不是年味，而是我们——从孩童成长为成人的我们。\n为什么我们会如此怀念儿时的年味？那是因为那时的年是家的团聚，是亲戚间热闹非凡的聚会；是母亲在厨房里忙碌的身影，烹饪出色香俱佳的年夜饭；是父亲和叔叔们门前贴春联、悬挂红灯笼的热闹景象；是我们穿着崭新的衣服，心怀期待地迎接新年；是街头小伙伴点燃的\u0026quot;黑蜘蛛炮\u0026quot;，以及深夜空中绚丽夺目的烟花秀。\n如今，尽管我们每个人或许都沉浸在各自生活的喧嚣中，春节的传统习俗可能也随着时间悄然发生了变化，但那份对美好事物的追求和渴望永远不会消失。我们仍然会在春节期间寻找那份年味，那份属于家的温暖和喜悦。\n在这个新春之际，我想我们都可以找到属于自己的庆祝方式，无论是与家人围坐一起，还是与朋友欢聚一堂，抑或是独自品味一本好书、听一曲悠扬的音乐。每一个细微的瞬间都是我们生命中的一部分，值得我们去珍惜和纪念。\n这一路遇上了很多风景我很喜欢下面看看照片咯\n这是在姨姥家门口的村头日落，就拍了两张没多拍哈哈哈\n当然了，还有两只超级可爱的咩咩羊,不知道小羊羔子现在是不是被宰了\u0026hellip;\u0026hellip;\n村口居然能看见雪山 但是不知道是什么山\n实际上今年一点都不开心，我姥爷的身体不是很好，可能也熬不过今年了。\n姥爷对我真的很好，可是真的没有办法，体会到了那种无奈的无力感。\n所以无论如何，时间会继续前行，岁月会留下印记，而我们所经历的每个春节都将成为我们回忆中最宝贵的一部分。 新的一年里面继续加油吧！！！\n","date":"2024-02-13T00:00:00Z","image":"https://img12.360buyimg.com/ddimg/jfs/t1/168052/22/37891/82207/65ccb958Fa2667100/adca896d87e12693.jpg","permalink":"http://localhost:1313/posts/2024/","title":"新春的序章"},{"content":"今天聊聊如何来管理我们的代码仓库 在软件开发过程中，代码仓库是一个非常重要的组成部分。它不仅是存储代码的地方，也是团队协作和版本控制的基础。因此，管理好自己的代码仓库至关重要。\n本文将介绍如何使用Git来管理自己的代码仓库。\n部署GitLab 相较于传动的部署方式比较繁琐，我这里直接采用docker的部署方式来部署gitlab以方便后续管理。\n说一下使用Docker来部署的一些痛点：\n数据备份：在Docker中运行Gitlab，需要定期备份数据以防止数据丢失。但是备份数据的过程可能会很麻烦，并且需要设置合适的策略来避免数据丢失。所以这是我觉得不管是GitLab还是其他的应用，保证数据的完整可靠性是至关重要的。 版本更新：Docker部署Gitlab需要时刻关注版本更新，需要进行升级或者迁移，主要是数据迁移的数据保障工作需要额外注意。 好了我们带着上面的两点问题，我们先来使用docker部署GitLab然后再慢慢探索。\nDockerCompose 我们使用docker-compose来部署gitlab仓库程序，注意镜像版本。如果你是尊贵的ee用户请修改镜像。如果你是普通的ce用户直接复制就行。 version: \u0026#39;3.6\u0026#39; services: web: image: \u0026#39;gitlab/gitlab-ce:latest\u0026#39; restart: always # 指定gitlab主机名称 hostname: \u0026#39;gitlab.example.com\u0026#39; environment: GITLAB_OMNIBUS_CONFIG: | # gitlab访问地址 external_url \u0026#39;http://10.1.6.100\u0026#39; ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;2212:22\u0026#39; # 防止与本地的22端口进行冲突 volumes: - \u0026#39;/data/gitlab-app/config:/etc/gitlab\u0026#39; - \u0026#39;/data/gitlab-app/logs:/var/log/gitlab\u0026#39; - \u0026#39;/data/gitlab-app/data:/var/opt/gitlab\u0026#39; shm_size: \u0026#39;256m\u0026#39; 启动gitlab程序 [root@localhost gitlab-app]# docker-compose up -d [+] Running 1/1 ✔ Container gitlab-app-web-1 Started 部署完成后的一些相关设置可以参考官方文档 实际上到此为止，我们的一个代码仓库就已经创建完成了，剩下的工作就是在UI界面点点点。我就不多赘述了主要讲讲如何备份的问题。\n如何高效的进行容器备份 当然了如果你想保证更安全的数据备份可以在深夜stop掉你的代码仓库从而进行停机备份(暂不采取) 简单备份和扩展备份 简单备份 如果您使用数据量少于 100 GB的可以使用一下三个步骤进行备份\n运行备份命令 GitLab 12.2 或更高版本： docker exec -t \u0026lt;container name\u0026gt; gitlab-backup create 输出示例\nDumping database tables: - Dumping table events... [DONE] - Dumping table issues... [DONE] - Dumping table keys... [DONE] - Dumping table merge_requests... [DONE] - Dumping table milestones... [DONE] - Dumping table namespaces... [DONE] - Dumping table notes... [DONE] - Dumping table projects... [DONE] - Dumping table protected_branches... [DONE] - Dumping table schema_migrations... [DONE] - Dumping table services... [DONE] - Dumping table snippets... [DONE] - Dumping table taggings... [DONE] - Dumping table tags... [DONE] - Dumping table users... [DONE] - Dumping table users_projects... [DONE] - Dumping table web_hooks... [DONE] - Dumping table wikis... [DONE] Dumping repositories: - Dumping repository abcd... [DONE] Creating backup archive: \u0026lt;backup-id\u0026gt;_gitlab_backup.tar [DONE] Deleting tmp directories...[DONE] Deleting old backups... [SKIPPING] 这里说明一下关于备份策略选项的选项\n更多地备份选项可以参考官方文档 tar默认备份策略本质上是使用 Linux 命令和将数据从相应的数据位置流式传输到备份gzip。这在大多数情况下工作正常，但当数据快速变化时可能会导致问题。\ntar当读取数据时数据发生变化，file changed as we read it可能会发生错误，并导致备份过程失败。在这种情况下，您可以使用名为 的备份策略copy。tar该策略在调用和之前将数据文件复制到临时位置gzip，以避免错误。\n副作用是备份过程会额外占用 1X 的磁盘空间。该过程会尽力清理每个阶段的临时文件，因此问题不会变得复杂，但对于大型安装来说，这可能是一个相当大的变化。 要使用该copy策略而不是默认的流策略，请 STRATEGY=copy在 Rake 任务命令中指定。\ndocker exec -t \u0026lt;container name\u0026gt; gitlab-backup create STRATEGY=copy 考虑将备份出来的相关文件上传到对象存储: 例如 S3、Minio等程序。\n手动备份gitlab.rb和gitlab-secrets.json。您可能还需要备份所有 TLS 密钥和证书 /etc/gitlab/ssl、/etc/gitlab/trusted-certs 以及 SSH 主机密钥。\n如果这两种文件丢失请参考官方文档\n恢复保存的数据 首先恢复数据要满足的一些前提条件\n目标 GitLab 实例必须已在运行 目标 GitLab 实例必须具有完全相同的版本 必须恢复 GitLab 机密配置文件 某些 GitLab 配置必须与原始备份环境匹配：例如TLS证书等内容 恢复作为挂载点的目录：详细参考 下面我们来具体看一下如何恢复已经从gitlab中备份的数据内容以及仓库等信息。\n如果使用Docker Swarm，容器可能会在恢复过程中重新启动，因为Puma已关闭，因此容器运行状况检查失败。要解决此问题，请暂时禁用运行状况检查机制。 # 修改docker-compose.yaml healthcheck: disable: true 部署堆栈信息(仅限于DockerSwarm)，关于为啥这样做请参考issuse6846 docker stack deploy --compose-file docker-compose.yml mystack 恢复步骤 # 首先停止puma和sidekiq docker exec -it \u0026lt;name of container\u0026gt; gitlab-ctl stop puma docker exec -it \u0026lt;name of container\u0026gt; gitlab-ctl stop sidekiq # 然后查看gitlab相关组件的状态 puma:down sidekiq:down docker exec -it \u0026lt;name of container\u0026gt; gitlab-ctl status # 开始恢复指定的备份文件 docker exec -it \u0026lt;name of container\u0026gt; gitlab-backup restore BACKUP=11493107454_2018_04_25_10.6.4-ce # 重启你的gitlab docker-compose restart # 检查相关的元数据信息 docker exec -it \u0026lt;name of container\u0026gt; gitlab-rake gitlab:check SANITIZE=true 这大概就是一个gitlab备份完整的恢复过程，当然本文没有涉及到的相关内容可以通过参考官网文档(上面有写)来进行扩充。\n","date":"2024-01-16T00:00:00Z","permalink":"http://localhost:1313/gitlab/GitLabRepositoryManagement/","title":"管理好内部的代码仓库-GitLab篇"},{"content":"Kubernetes 中比较流行的日志收集解决方案是 Elasticsearch、Fluentd 和 Kibana（EFK）技术栈，也是官方现在比较推荐的一种方案。\nElasticsearch 是一个实时的、分布式的可扩展的搜索引擎，允许进行全文、结构化搜索，它通常用于索引和搜索大量日志数据，也可用于搜索许多不同类型的文档。\nElasticsearch 通常与 Kibana 一起部署，Kibana 是 Elasticsearch 的一个功能强大的数据可视化 Dashboard，Kibana 允许你通过 web 界面来浏览Elasticsearch 日志数据。\nFluentd是一个流行的开源数据收集器，我们将在 Kubernetes 集群节点上安装 Fluentd，通过获取容器日志文件、过滤和转换日志数据，然后将数据传递到 Elasticsearch 集群，在该集群中对其进行索引和存储。\n我们先来配置启动一个可扩展的 Elasticsearch 集群，然后在 Kubernetes 集群中创建一个 Kibana 应用，最后通过 DaemonSet 来运行 Fluentd，以便它在每个 Kubernetes 工作节点上都可以运行一个 Pod。\n安装 Elasticsearch 集群 先创建一个命名空间，我们将在其中安装所有日志相关的资源对象。\nkubectl create ns kube-logging 环境准备 ElasticSearch 安装有最低安装要求，如果安装后 Pod 无法正常启动，请检查是否符合最低要求的配置，要求如下：\n节点 CPU最低要求 内存最低要求 elasticsearch-master 核心数\u0026gt;2 内存\u0026gt;2G elasticsearch-data 核心数\u0026gt;1 内存\u0026gt;2G elasticsearch-client 核心数\u0026gt;1 内存\u0026gt;2G 集群节点信息\n集群 节点类型 副本数目 存储大小 网络模式 描述 elasticsearch master 3 5Gi ClusterIP 主节点 elasticsearch-data data 3 50Gi ClusterIP 数据节点 elasticsearch-client client 2 无 NodePort 负责处理用户请求 建议使用 StorageClass 来做持久化存储，当然如果你是线上环境建议使用 Local PV 或者 Ceph RBD 之类的存储来持久化 Elasticsearch 的数据。\n由于 ElasticSearch 7.x 版本默认安装了 X-Pack 插件，并且部分功能免费，需要我们配置一些安全证书文件。\n准备生成证书文件 注意：由于我们采用的是containerd所以使用nerdctl来运行一个容器\nmkdir -p elastic-certs nerdctl run --name elastic-certs -v elastic-certs:/app -it -w /app elasticsearch:7.17.3 /bin/sh -c \\ \u0026#34;elasticsearch-certutil ca --out /app/elastic-stack-ca.p12 --pass \u0026#39;\u0026#39; \u0026amp;\u0026amp; \\ elasticsearch-certutil cert --name security-master --dns \\ security-master --ca /app/elastic-stack-ca.p12 --pass \u0026#39;\u0026#39; --ca-pass \u0026#39;\u0026#39; --out /app/elastic-certificates.p12\u0026#34; # 找到nerdctl挂载的目录 cd /var/lib/nerdctl/1935db59/volumes/default/elastic-certs/_data/ # 这个每个人是不一样的 可以自己搜索一下 mv * /root/elastic-certs/ cd /root/elastic-certs/ \u0026amp;\u0026amp; openssl pkcs12 -nodes -passin pass:\u0026#39;\u0026#39; -in elastic-certificates.p12 -out elastic-certificate.pem 添加证书到kubernetes kubectl create secret -n kube-logging generic elastic-certs --from-file=elastic-certificates.p12 # 设置集群用户名和密码 kubectl create secret -n kube-logging generic elastic-auth --from-literal=username=elastic --from-literal=password=elastic-master 准备安装Elastic集群 采用Helm的方式来添加elasticsearch仓库 helm repo add elastic https://helm.elastic.co helm repo update ElaticSearch 安装需要安装三次，分别安装 Master、Data、Client 节点，Master 节点负责集群间的管理工作；Data 节点负责存储数据；Client 节点负责代理 ElasticSearch Cluster 集群，负载均衡。 2. 拉取elasticsearch\nhelm pull elastic/elasticsearch --untar --version 7.17.3 cd elasticsearch/ 在Chart目录下创建对应节点节点的values文件 以下是master-value.yaml\n# 设置集群名称 clusterName: \u0026#34;elasticsearch\u0026#34; # 设置节点名称 nodeGroup: \u0026#34;master\u0026#34; # 设置角色 roles: master: \u0026#34;true\u0026#34; ingest: \u0026#34;false\u0026#34; data: \u0026#34;false\u0026#34; # 镜像 image: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch\u0026#34; imageTag: \u0026#34;7.17.3\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # 副本数 replicas: 3 # ---资源配置--- esJavaOpts: \u0026#34;-Xmx1g -Xms1g\u0026#34; resources: requests: cpu: \u0026#34;2000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; limits: cpu: \u0026#34;2000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; # 数据持久卷配置 persistence: enabled: true # 存储数据大小配置 volumeClaimTemplate: storageClassName: managed-nfs-storage # 定义你自己的存储类 accessModes: [\u0026#39;ReadWriteOnce\u0026#39;] resources: requests: storage: 5Gi # ---安全设置--- # 设置协议，可配置为 http、https protocol: http # 证书挂载配置，这里我们挂入上面创建的证书 secretMounts: - name: elastic-certs secretName: elastic-certs path: /usr/share/elasticsearch/config/certs defaultMode: 0755 # 允许您在/usr/share/elasticsearch/config/中添加任何自定义配置文件,例如 elasticsearch.yml、log4j2.properties # ElasticSearch 7.x 默认安装了 x-pack 插件，部分功能免费，这里我们配置下 # 下面注掉的部分为配置 https 证书，配置此部分还需要配置 helm 参数 protocol 值改为 https esConfig: elasticsearch.yml: | xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.enabled: true # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # 环境变量配置，这里引入上面设置的用户名、密码 secret 文件 extraEnvs: - name: ELASTIC_USERNAME valueFrom: secretKeyRef: name: elastic-auth key: username - name: ELASTIC_PASSWORD valueFrom: secretKeyRef: name: elastic-auth key: password # ---调度设置--- # 设置调度策略 # - hard：只有当有足够的节点时 Pod 才会被调度，并且它们永远不会出现在同一个节点上 # - soft：尽最大努力调度 antiAffinity: \u0026#39;soft\u0026#39; tolerations: - operator: \u0026#34;Exists\u0026#34; # 容忍全部污点 以下是data-value.yaml的内容\n# 设置集群名称 clusterName: \u0026#34;elasticsearch\u0026#34; # 设置节点名称 nodeGroup: \u0026#34;data\u0026#34; # 设置角色 roles: master: \u0026#34;false\u0026#34; ingest: \u0026#34;true\u0026#34; data: \u0026#34;true\u0026#34; # 镜像 image: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch\u0026#34; imageTag: \u0026#34;7.17.3\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # 副本数 replicas: 3 # ---资源配置--- esJavaOpts: \u0026#34;-Xmx1g -Xms1g\u0026#34; resources: requests: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; limits: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; # 数据持久卷配置 persistence: enabled: true # 存储数据大小配置 volumeClaimTemplate: storageClassName: managed-nfs-storage # 定义你自己的存储类 accessModes: [\u0026#39;ReadWriteOnce\u0026#39;] resources: requests: storage: 20Gi # ---安全设置--- # 设置协议，可配置为 http、https protocol: http # 证书挂载配置，这里我们挂入上面创建的证书 secretMounts: - name: elastic-certs secretName: elastic-certs path: /usr/share/elasticsearch/config/certs defaultMode: 0755 # 允许您在/usr/share/elasticsearch/config/中添加任何自定义配置文件,例如 elasticsearch.yml、log4j2.properties # ElasticSearch 7.x 默认安装了 x-pack 插件，部分功能免费，这里我们配置下 # 下面注掉的部分为配置 https 证书，配置此部分还需要配置 helm 参数 protocol 值改为 https esConfig: elasticsearch.yml: | xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.enabled: true # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # 环境变量配置，这里引入上面设置的用户名、密码 secret 文件 extraEnvs: - name: ELASTIC_USERNAME valueFrom: secretKeyRef: name: elastic-auth key: username - name: ELASTIC_PASSWORD valueFrom: secretKeyRef: name: elastic-auth key: password 以下是client-value.yaml\n# 设置集群名称 clusterName: \u0026#34;elasticsearch\u0026#34; # 设置节点名称 nodeGroup: \u0026#34;client\u0026#34; # 设置角色 roles: master: \u0026#34;false\u0026#34; ingest: \u0026#34;false\u0026#34; data: \u0026#34;false\u0026#34; # 镜像 image: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch\u0026#34; imageTag: \u0026#34;7.17.3\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # 副本数 replicas: 1 # ---资源配置--- esJavaOpts: \u0026#34;-Xmx1g -Xms1g\u0026#34; resources: requests: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; limits: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; # 数据持久卷配置 persistence: enabled: false # ---安全设置--- # 设置协议，可配置为 http、https protocol: http # 证书挂载配置，这里我们挂入上面创建的证书 secretMounts: - name: elastic-certs secretName: elastic-certs path: /usr/share/elasticsearch/config/certs defaultMode: 0755 # 允许您在/usr/share/elasticsearch/config/中添加任何自定义配置文件,例如 elasticsearch.yml、log4j2.properties # ElasticSearch 7.x 默认安装了 x-pack 插件，部分功能免费，这里我们配置下 # 下面注掉的部分为配置 https 证书，配置此部分还需要配置 helm 参数 protocol 值改为 https esConfig: elasticsearch.yml: | xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.enabled: true # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # 环境变量配置，这里引入上面设置的用户名、密码 secret 文件 extraEnvs: - name: ELASTIC_USERNAME valueFrom: secretKeyRef: name: elastic-auth key: username - name: ELASTIC_PASSWORD valueFrom: secretKeyRef: name: elastic-auth key: password # ----服务设置---- service: type: NodePort nodePort: \u0026#39;30200\u0026#39; 开始部署相关节点 helm upgrade --install elasticsearch-master -f master-values.yaml --namespace kube-logging ./ # 部署master helm upgrade --install elasticsearch-data -f data-values.yaml --namespace kube-logging ./ # 部署data helm upgrade --install elasticsearch-client -f client-values.yaml --namespace kube-logging ./ # 部署 client 正常情况下看到所有节点都处于running状态即可\n[root@Online-Beijing-master1 elasticsearch]# kubectl get pods -n kube-logging NAME READY STATUS RESTARTS AGE elasticsearch-client-0 1/1 Running 0 13m elasticsearch-data-0 1/1 Running 0 17m elasticsearch-data-1 1/1 Running 0 17m elasticsearch-data-2 1/1 Running 0 17m elasticsearch-master-0 1/1 Running 0 43m elasticsearch-master-1 1/1 Running 0 43m elasticsearch-master-2 1/1 Running 0 43m 安装Kibana 依旧使用helm的方式进行部署\n使用helm pull拉取Kibana包来进行解压 helm pull elastic/kibana --untar --version 7.17.3 cd kibana 定义一个名字为custom-value.yaml的文件 # 指定镜像与镜像版本 image: \u0026#39;kibana\u0026#39; imageTag: \u0026#39;7.17.3\u0026#39; # 配置 ElasticSearch 地址 elasticsearchHosts: \u0026#39;http://elasticsearch-client:9200\u0026#39; # 环境变量配置，这里引入上面设置的用户名、密码 secret 文件 extraEnvs: - name: \u0026#39;ELASTICSEARCH_USERNAME\u0026#39; valueFrom: secretKeyRef: name: elastic-auth key: username - name: \u0026#39;ELASTICSEARCH_PASSWORD\u0026#39; valueFrom: secretKeyRef: name: elastic-auth key: password resources: requests: cpu: \u0026#39;500m\u0026#39; memory: \u0026#39;1Gi\u0026#39; limits: cpu: \u0026#39;500m\u0026#39; memory: \u0026#39;1Gi\u0026#39; # kibana 配置中添加语言配置，设置 kibana 为中文 kibanaConfig: kibana.yml: | i18n.locale: \u0026#34;zh-CN\u0026#34; service: type: NodePort nodePort: \u0026#39;30601\u0026#39; 部署kibana helm install kibana -f custom-value.yaml --namespace kube-logging . 部署Fluentd Fluentd 是一个高效的日志聚合器，是用 Ruby 编写的，并且可以很好地扩展。对于大部分企业来说，Fluentd 足够高效并且消耗的资源相对较少，另外一个工具 Fluent-bit 更轻量级，占用资源更少，但是插件相对 Fluentd 来说不够丰富，所以整体来说，Fluentd 更加成熟，使用更加广泛，所以这里我们使用 Fluentd 来作为日志收集工具。\n工作原理 Fluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储等等。Fluentd 支持超过 300 个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下：\n首先 Fluentd 从多个日志源获取数据 结构化并且标记这些数据 然后根据匹配的标签将数据发送到多个目标服务去 官方文档 日志源配置 比如我们这里为了收集 Kubernetes 节点上的所有容器日志，就需要做如下的日志源配置：\nid：表示引用该日志源的唯一标识符，该标识可用于进一步过滤和路由结构化日志数据 type：Fluentd 内置的指令，tail 表示 Fluentd 从上次读取的位置通过 tail 不断获取数据，另外一个是 http 表示通过一个 GET 请求来收集数据。 path：tail 类型下的特定参数，告诉 Fluentd 采集 /var/log/containers 目录下的所有日志，这是 docker 在 Kubernetes 节点上用来存储运行容器 stdout 输出日志数据的目录。 pos_file：检查点，如果 Fluentd 程序重新启动了，它将使用此文件中的位置来恢复日志数据收集。 tag：用来将日志源与目标或者过滤器匹配的自定义字符串，Fluentd 匹配源/目标标签来路由日志数据。 \u0026lt;source\u0026gt; @id fluentd-containers.log @type tail # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志,类似于tail命令 path /var/log/containers/*.log # 挂载的宿主机容器日志地址 pos_file /var/log/es-containers.log.pos tag raw.kubernetes.* # 设置日志标签 read_from_head true \u0026lt;parse\u0026gt; # 多行格式化成JSON @type multi_format # 使用 multi-format-parser 解析器插件 \u0026lt;pattern\u0026gt; format json # JSON 解析器 time_key time # 指定事件时间的时间字段 time_format %Y-%m-%dT%H:%M:%S.%NZ # 时间格式 \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; 过滤 由于 Kubernetes 集群中应用太多，也还有很多历史数据，所以我们可以只将某些应用的日志进行收集，比如我们只采集具有 discovery-log=true 这个 Label 标签的 Pod 日志，这个时候就需要使用 filter。\n# 删除无用的属性 \u0026lt;filter kubernetes.**\u0026gt; @type record_transformer remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash \u0026lt;/filter\u0026gt; # 只保留具有discovery-log=true标签的Pod日志 \u0026lt;filter kubernetes.**\u0026gt; @id filter_log @type grep \u0026lt;regexp\u0026gt; key $.kubernetes.labels.discovery-log pattern ^true$ \u0026lt;/regexp\u0026gt; \u0026lt;/filter\u0026gt; 路由设置 \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true type_name fluentd host \u0026#34;#{ENV[\u0026#39;OUTPUT_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;OUTPUT_PORT\u0026#39;]}\u0026#34; logstash_format true \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size \u0026#34;#{ENV[\u0026#39;OUTPUT_BUFFER_CHUNK_LIMIT\u0026#39;]}\u0026#34; queue_limit_length \u0026#34;#{ENV[\u0026#39;OUTPUT_BUFFER_QUEUE_LIMIT\u0026#39;]}\u0026#34; overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; match：标识一个目标标签，后面是一个匹配日志源的正则表达式，我们这里想要捕获所有的日志并将它们发送给 Elasticsearch，所以需要配置成**。 id：目标的一个唯一标识符。 type：支持的输出插件标识符，我们这里要输出到 Elasticsearch，所以配置成 elasticsearch，这是 Fluentd 的一个内置插件。 log_level：指定要捕获的日志级别，我们这里配置成 info，表示任何该级别或者该级别以上（INFO、WARNING、ERROR）的日志都将被路由到 Elsasticsearch。 host/port：定义 Elasticsearch 的地址，也可以配置认证信息，我们的 Elasticsearch 不需要认证，所以这里直接指定 host 和 port 即可。 logstash_format：Elasticsearch 服务对日志数据构建反向索引进行搜索，将 logstash_format 设置为 true，Fluentd 将会以 logstash 格式来转发结构化的日志数据。 Buffer： Fluentd 允许在目标不可用时进行缓存，比如，如果网络出现故障或者 Elasticsearch 不可用的时候。缓冲区配置也有助于降低磁盘的 IO。 开始部署Fluentd 要收集 Kubernetes 集群的日志，直接用DasemonSet 控制器来部署 Fluentd 应用，这样，它就可以从 Kubernetes 节点上采集日志，确保在集群中的每个节点上始终运行一个 Fluentd 容器。当然可以直接使用 Helm 来进行一键安装，为了能够了解更多实现细节，我们这里还是采用手动方法来进行安装。\n安装文档 首先创建fluentd的configmap # fluentd-configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: fluentd-conf namespace: kube-logging data: # containerd的容器日志 containerd.input.conf: |- \u0026lt;source\u0026gt; @id containerd-fluentd-beta.log # 唯一Id：运行时+收集插件+环境 @type tail # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志 path /var/log/containers/*.log # Docker 容器日志路径 pos_file /var/log/es-containers.log.pos # 记录读取的位置 tag raw.kubernetes.* # 设置日志标签 read_from_head true # 从头读取 \u0026lt;parse\u0026gt; # 多行格式化成JSON # 可以使用我们介绍过的 multiline 插件实现多行日志 @type multi_format # 使用 multi-format-parser 解析器插件 \u0026lt;pattern\u0026gt; format json # JSON解析器 time_key time # 指定事件时间的时间字段 time_format %Y-%m-%dT%H:%M:%S.%NZ # 时间格式 \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; # 在日志输出中检测异常(多行日志)，并将其作为一条日志转发 # https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions \u0026lt;match raw.kubernetes.**\u0026gt; # 匹配tag为raw.kubernetes.**日志信息 @id raw.kubernetes @type detect_exceptions # 使用detect-exceptions插件处理异常栈信息 remove_tag_prefix raw # 移除 raw 前缀 message log multiline_flush_interval 5 \u0026lt;/match\u0026gt; \u0026lt;filter **\u0026gt; # 拼接日志 @id filter_concat @type concat # Fluentd Filter 插件，用于连接多个日志中分隔的多行日志 key message multiline_end_regexp /\\n$/ # 以换行符“\\n”拼接 separator \u0026#34;\u0026#34; \u0026lt;/filter\u0026gt; # 添加 Kubernetes metadata 数据 \u0026lt;filter kubernetes.**\u0026gt; @id filter_kubernetes_metadata @type kubernetes_metadata \u0026lt;/filter\u0026gt; # 修复 ES 中的 JSON 字段 # 插件地址：https://github.com/repeatedly/fluent-plugin-multi-format-parser \u0026lt;filter kubernetes.**\u0026gt; @id filter_parser @type parser # multi-format-parser多格式解析器插件 key_name log # 在要解析的日志中指定字段名称 reserve_data true # 在解析结果中保留原始键值对 remove_key_name_field true # key_name 解析成功后删除字段 \u0026lt;parse\u0026gt; @type multi_format \u0026lt;pattern\u0026gt; format json \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format none \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt; # 删除一些多余的属性 \u0026lt;filter kubernetes.**\u0026gt; @type record_transformer remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash \u0026lt;/filter\u0026gt; # 只保留具有kubernetes.log.kubernetes.log/fluentd标签的Pod日志 \u0026lt;filter kubernetes.**\u0026gt; @id filter_log @type grep \u0026lt;regexp\u0026gt; key $.kubernetes.labels.kubernetes.log/fluentd pattern ^true$ \u0026lt;/regexp\u0026gt; \u0026lt;/filter\u0026gt; ###### 监听配置，一般用于日志聚合用 ###### forward.input.conf: |- # 监听通过TCP发送的消息 \u0026lt;source\u0026gt; @id forward @type forward \u0026lt;/source\u0026gt; output.conf: |- \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true host elasticsearch-client port 9200 user elastic # FLUENT_ELASTICSEARCH_USER | FLUENT_ELASTICSEARCH_PASSWORD password elastic-master logstash_format true logstash_prefix kubernetes-cluster request_timeout 30s \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; 创建相关的Rbac权限 --- apiVersion: v1 kind: ServiceAccount metadata: name: fluentd namespace: kube-logging --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: fluentd rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods - namespaces verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd roleRef: kind: ClusterRole name: fluentd apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: fluentd namespace: kube-logging 创建fluentd的daemonset 这个是最新的版本还在研究中,用下面的版本。\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: kube-logging labels: k8s-app: fluentd-logging version: v1 spec: selector: matchLabels: k8s-app: fluentd-logging version: v1 template: metadata: labels: k8s-app: fluentd-logging version: v1 spec: serviceAccount: fluentd serviceAccountName: fluentd tolerations: - key: node-role.kubernetes.io/control-plane effect: NoSchedule - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch env: - name: K8S_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: FLUENT_ELASTICSEARCH_HOST value: \u0026#34;elasticsearch-client-headless.kube-logging.svc.cluster.local\u0026#34; - name: FLUENT_ELASTICSEARCH_PORT value: \u0026#34;9200\u0026#34; - name: FLUENT_ELASTICSEARCH_SCHEME value: \u0026#34;http\u0026#34; # Option to configure elasticsearch plugin with self signed certs # ================================================================ - name: FLUENT_ELASTICSEARCH_SSL_VERIFY value: \u0026#34;true\u0026#34; # Option to configure elasticsearch plugin with tls # ================================================================ - name: FLUENT_ELASTICSEARCH_SSL_VERSION value: \u0026#34;TLSv1_2\u0026#34; # X-Pack Authentication # ===================== - name: FLUENT_ELASTICSEARCH_USER value: \u0026#34;elastic\u0026#34; - name: FLUENT_ELASTICSEARCH_PASSWORD value: \u0026#34;elastic-master\u0026#34; - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX value: \u0026#34;kubernetes-cluster\u0026#34; resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: fluentconfig mountPath: /fluentd/etc/custom - name: dockercontainerlogdirectory mountPath: /var/log/pods readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: fluentconfig configMap: name: fluentd-conf - name: dockercontainerlogdirectory hostPath: path: /var/log/pods 麻烦用下面的进行部署\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: kube-logging labels: app: fluentd kubernetes.io/cluster-service: \u0026#39;true\u0026#39; spec: selector: matchLabels: app: fluentd template: metadata: labels: app: fluentd kubernetes.io/cluster-service: \u0026#39;true\u0026#39; spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule serviceAccountName: fluentd containers: - name: fluentd image: quay.io/fluentd_elasticsearch/fluentd:v3.4.0 volumeMounts: - name: fluentconfig mountPath: /etc/fluent/config.d - name: varlog mountPath: /var/log volumes: - name: fluentconfig configMap: name: fluentd-conf - name: varlog hostPath: path: /var/log 关于保留指定标签的问题 部署完成以后我发现一直有一个小问题，就是无论我如何设置label都无法让elasticsearch获取到正常的数据。\n根据这个问题，我进行了更细致的排查。现在得出了如下的结论。\n可能是由于我对知识的缺乏，我定义的是Deployment当中的label标签，但是这个label标识只作用于Deployment本身，通常用作kubernete集群中的选择器匹配，例如我们的Service要去匹配某个Deployment。\n关于spec.template.metadata.labels，我发现这个才是我们正确要匹配的label标签选项，因为这些标签用于标识Deployment所创建的Pod\n所以最后总结出来的问题就是，我们上面的fluentd中写的过滤插件 key $.kubernetes.labels.kubernetes.log/fluentd中所匹配的label标签应当是spec.template.metadata.labels的label\nspec: replicas: 2 selector: matchLabels: app: canary template: metadata: creationTimestamp: null labels: app: canary kubernetes.log/fluentd: \u0026#39;true\u0026#39; ","date":"2023-12-08T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/177175/31/35833/30306/64cb08eeF5ba90f46/1da6311bbbec8921.jpg","permalink":"http://localhost:1313/kubernetes/efk/","title":"kubernetes基于EFK的日志落地实现"},{"content":"人生总要去一趟远方吧 下一站行程 暂定西藏🚄\n实在是不想上班，我承认我被我强大的意念占领了，我没忍住。请假直接从北京去了阿坝甘孜。\n好了不开玩笑了哈哈哈\n起初我们打算自驾，奈何我是个小趴菜，先不说阿坝那边的路况好不好走，就平时开个市区我都费劲，所以这条直接被我PASS了。\n所以选择了最最最方便的飞机✈️出行,您还别说这交通工具有两把刷子(不好意思，我真是第一次坐飞机),其实坐飞机大家想得那么复杂 基本流程其实就如下几点\n买票: 不买票想吃霸王机🤪 取票：如果你需要报销什么的其实可以去对应的航空公司柜台去取登机牌,如果个人出行的话可以直接使用电子登机牌进行相关的登机操作 安检: 取完票之后就差不多要安检了,基本上安检也就是十分钟左右的事情,安检完成就正常进入候机楼找你飞机所对应的登机窗口就👌🏻 出发 一大早的我都来机场等着了，因为第一次看网上说要提前2小时到机场，我到了以后发现其实好像也并没有那么夸张 说说我这次准备的东西吧\n衣服方面衣🧣: 个人建议还是羽绒服+冲锋衣起步,带好围脖、围巾、手套、雪地靴等物品，因为那边其实还是很冷的，因为我在雪山上。 物品方面：我是个男孩子其实没有女生带的那么多物品，基本的换洗袜子一件外衣和一件裤子足够,然后就是准备摄影的东西，带着我年迈的SONY A6000和它的两个可拆卸镜头哈哈哈，氧气瓶务必准备充足,个人玩家一般情况下来说3瓶子足矣。团队玩家建议团购(保命的东西啊)。 药物方面：相关方面的药物尽量还是要带上的比如什么肠胃药、感冒药、咳嗽发烧药等等。有人说去高海拔地区要提前吃什么红景天之类的东西，只能说这个东西是因人而异的吧。 证件信息：身份证、护照、还有手机啥的这些东西老铁们应该忘不了吧。 我们买的是早上7.15的机票，大概提前一个半小时就到了机场，其实相对于来说还是挺早的，哎呀不说啦激动地我是一页都没睡。 我的行程大概就是在淘宝订了一个小团，价格是1340，行程是两天。大概的路线就是毕棚沟+达古冰川。 具体行程下面介绍哈哈哈\n登机了，耳机里直接走起经济舱的BGM：46A我靠窗边坐下，rapper坐经济舱面子伤不伤\n## 第一天行程 我报的是一个2-6人的小团，基本上人都是满的，大概7.00到7.30左右司机哥会上门接人，然后出发前往`毕棚沟景区`, 因为第一天的早中晚都是自理饭费，哥几个直接干脆路上找了个饭店一起AA的吃了一顿。 毕棚沟这个地方还挺难上的，司机给我们带到半山腰上,半山腰会有一个旅客服务大厅，在大厅外还需要再等大巴车把你们统统都带走！！！ 其实还是挺喜欢这种感觉的,在高海拔地区一定要慢慢的走，尽量不要去奔跑跳跃，因为部分人群很容易因此而高反。 一天了也玩儿累了，吃吃喝喝当时也准备睡觉了 顺带提一嘴：关于高反这个事情我好想没有太大的反应，因为只是单纯的感觉走路比平时喘的更快了，平时走十步喘一口，在这儿可能走五步就需要喘一口。 另外我带的氧气在毕棚沟还没用上，所以提醒一下大家去高海拔地区量力而行。 第二天行程 后续的行程因为住宿的地方在海拔三千米以上，我的天那叫一个真的冷，我们进了酒店 赶紧把所有能取暖的东西全部Open了。就这样度过了这一天。\n第二天一早要吃过早饭以后从羊茸哈德前往黑水县，大概也就半个小时左右的路程，因为达古冰川在黑水县境内 海拔大概5000米最高，冬天其实比较适合一些攀冰等极限运动(普通人请别作死啊啊啊)。\n老样子依旧是给你扔到半山腰，这一次到不是像上一次一样去做大巴了，这次是直接在半山腰坐缆车上山。我寻思呢这要是让我爬上去我得爬到什么时候哈哈哈 我们去的时候人特别多，缆车排队时间挺长的大概在1小时到两个小时之间左右的这个区间。因为从山下到山上坐缆车还需要15-20分钟左右。\n中国机长那个飞跃雪山🏔就是在这里拍摄的\n\u003c!DOCTYPE html\u003e 其实到这里呢我们也就连夜回到了成都住了一夜，然后找个酒店住了一晚，第二天一早赶回了北京。\n这一次旅行对于我来说挺震撼的，第一次去这么远的地方，也是第一次去这么放松的地方,很那定义我真正拥有过什么，于是昏沉当中这一年又要过去啦。\n下一次的旅途打算为期7天的西藏,坐上绿皮火车。希望尽快提上日程吧。生活加油,工作加油❤️。\n","date":"2023-11-23T00:00:00Z","image":"https://img13.360buyimg.com/ddimg/jfs/t1/230405/24/4739/81218/65635877F5b2d86e7/53cc1edf42100b84.jpg?width=300px\u0026height=240px","permalink":"http://localhost:1313/tourists/sichuan/","title":"旅行日记-四川·阿坝甘孜藏族自治州"},{"content":"为什么需要kubernetes？ 大规模多节点容器调度 快速扩缩容 故障自愈 弹性伸缩 技术趋势 一致性、不锁定 早期型多的一些服务都属于单体服务、单节点、单进程的一种单体服务架构，后续随着技术的发展衍生出了容器技术。容器技术其实也不能满足我们的多节点、分布式的应用架构体系，从而衍生出了kubernetes容器编排引擎。 那么我们来看一下早期单体容器架构\n其实对于容器化技术带来了那些优势呢?\n其实我觉得容器化带来的最大的优势就是交付和部署的优势 那么随之而来带来的问题是:\n那么由于Docker的容器镜像可以在A、B、C任意一台机器上运行,那么是否可以当A机器所运行的镜像挂掉以后自动的帮我在B机器上进行重启呢?\nokey 带着这个问题 一起往下进行。\nkubernes组件 先看一张官方给出的kubernetes的架构图 图中列出了kubernetes的组成以及相对应的组件\nControlPlane: 控制平面节点 Node: 工作节点 Kubelet: 用于控制staticPod,其主要就是用来控制静态Pod，因为静态Pod不受ApiServer的影响。 Oh 不插一句嘴 学到了一个新的命令\n# jq命令是一个用于处理json的命令 kubectl get deploy wecho-canary -o json | jq .spec okey 继续\u0026hellip; 我们时长谈起到的control-plane实际上并不是一台机器他只是一个抽象出来的概念,实际上我们是在说所谓的control-plane层面的组件。也就是说这些组件可以运行在控制面的机器上同时也可以运行在Node机器上\nkubernetes核心概念 ResourceObject: 是我认为相对而言kubernetes集群当中比较核心的资源对象,其实也就是我们所说的Pod、Deployment、Daemonset等kubernetes的资源类型 对于一个Pod而言,kubernetes对其定义的键值无非以下的几种 [root@Online-Beijing-master1 ~]# kubectl get deploy wecho-canary -o json | jq keys [ \u0026#34;apiVersion\u0026#34;, \u0026#34;kind\u0026#34;, \u0026#34;metadata\u0026#34;, \u0026#34;spec\u0026#34;, // spec描述的是Pod预期的状态 \u0026#34;status\u0026#34; ] 你可以通过kubectl api-resource来获取kubernetes相对应的资源类型。\nkubernets的资源提交 我们平时使用的kubectl run nginx-$RANDOM --image=\u0026quot;nginx:alpine\u0026quot;究竟是执行了什么样的内容？\n正常来说Api-Server本身就是服务,那么当我把请求发送给Api-Server的时候,我是以什么样的请求内容进行了提交?那Api-Server接收了我的请求内容又对我的请求内容做出了什么样的处理呢?\n首先我们来看实际作为客户端,也就是client端提交的请求\n# 可以通过--dry-run=client来模拟客户端提交的请求内容 kubectl run nginx-$RANDOM --image=\u0026#34;nginx:alpine\u0026#34; --dry-run=client -ojson -v6 正常的返回响应应该如下,这是一个我们正常通过kubelet创建一个Pod所发送的请求体内容,但作为clinet只会在我们本地进行处理,所以你也可以看到返回的结构内容中带有I1108 21:54:49.922270 1219589 loader.go:374] Config loaded from file: /root/.kube/config,也就证明了它并没有像Api-Server发送任何请求,只是读取了相关的配置信息。\n{ \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx-27147\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null, \u0026#34;labels\u0026#34;: { \u0026#34;run\u0026#34;: \u0026#34;nginx-27147\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;nginx-27147\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;nginx:alpine\u0026#34;, \u0026#34;resources\u0026#34;: {} } ], \u0026#34;restartPolicy\u0026#34;: \u0026#34;Always\u0026#34;, \u0026#34;dnsPolicy\u0026#34;: \u0026#34;ClusterFirst\u0026#34; }, \u0026#34;status\u0026#34;: {} } 很好上面我们模拟了一个client端所生产的内容,那么下面我们看看当实际发送给Api-Server的时候产生了哪些内容\nkubectl run nginx-$RANDOM --image=\u0026#34;nginx:alpine\u0026#34; --dry-run=server -ojson -v6 我们可以清晰地看到日志的输出\n首先第一步加载了kubernetes相关的配置文件信息。 发送了相关请求(盲猜应该是验证api-server是否正常) 向https://resk8s.api.beijing.io:8443/api/v1/namespaces/default/pods?dryRun=All\u0026amp;fieldManager=kubectl-run发送了POST请求用于创建Pod I1108 21:58:14.866131 1221713 loader.go:374] Config loaded from file: /root/.kube/config I1108 21:58:14.884329 1221713 round_trippers.go:553] GET https://resk8s.api.beijing.io:8443/openapi/v2?timeout=32s 200 OK in 15 milliseconds I1108 21:58:14.946553 1221713 round_trippers.go:553] POST https://resk8s.api.beijing.io:8443/api/v1/namespaces/default/pods?dryRun=All\u0026amp;fieldManager=kubectl-run 201 Created in 15 milliseconds 事实上我们由此可见发送到Api-Server的请求内容多了很多东西\n{ \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx-14483\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;d27e1836-8bac-40d4-805b-420f4bca4ee1\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2023-11-08T14:05:36Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;run\u0026#34;: \u0026#34;nginx-14483\u0026#34; }, \u0026#34;annotations\u0026#34;: { \u0026#34;kubernetes.customized/fabric-networks\u0026#34;: \u0026#34;default\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;volumes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;kube-api-access-vwtvp\u0026#34;, \u0026#34;projected\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;serviceAccountToken\u0026#34;: { \u0026#34;expirationSeconds\u0026#34;: 3607, \u0026#34;path\u0026#34;: \u0026#34;token\u0026#34; } }, { \u0026#34;configMap\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kube-root-ca.crt\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;ca.crt\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;ca.crt\u0026#34; } ] } }, { \u0026#34;downwardAPI\u0026#34;: { \u0026#34;items\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;namespace\u0026#34;, \u0026#34;fieldRef\u0026#34;: { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldPath\u0026#34;: \u0026#34;metadata.namespace\u0026#34; } } ] } } ], \u0026#34;defaultMode\u0026#34;: 420 } } ], \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;nginx-14483\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;nginx:alpine\u0026#34;, \u0026#34;resources\u0026#34;: {}, \u0026#34;volumeMounts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;kube-api-access-vwtvp\u0026#34;, \u0026#34;readOnly\u0026#34;: true, \u0026#34;mountPath\u0026#34;: \u0026#34;/var/run/secrets/kubernetes.io/serviceaccount\u0026#34; } ], \u0026#34;terminationMessagePath\u0026#34;: \u0026#34;/dev/termination-log\u0026#34;, \u0026#34;terminationMessagePolicy\u0026#34;: \u0026#34;File\u0026#34;, \u0026#34;imagePullPolicy\u0026#34;: \u0026#34;IfNotPresent\u0026#34; } ], \u0026#34;restartPolicy\u0026#34;: \u0026#34;Always\u0026#34;, \u0026#34;terminationGracePeriodSeconds\u0026#34;: 30, \u0026#34;dnsPolicy\u0026#34;: \u0026#34;ClusterFirst\u0026#34;, \u0026#34;serviceAccountName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;serviceAccount\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;securityContext\u0026#34;: {}, \u0026#34;schedulerName\u0026#34;: \u0026#34;default-scheduler\u0026#34;, \u0026#34;tolerations\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;node.kubernetes.io/not-ready\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;Exists\u0026#34;, \u0026#34;effect\u0026#34;: \u0026#34;NoExecute\u0026#34;, \u0026#34;tolerationSeconds\u0026#34;: 300 }, { \u0026#34;key\u0026#34;: \u0026#34;node.kubernetes.io/unreachable\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;Exists\u0026#34;, \u0026#34;effect\u0026#34;: \u0026#34;NoExecute\u0026#34;, \u0026#34;tolerationSeconds\u0026#34;: 300 } ], \u0026#34;priority\u0026#34;: 0, \u0026#34;dnsConfig\u0026#34;: { \u0026#34;options\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;single-request-reopen\u0026#34; } ] }, \u0026#34;enableServiceLinks\u0026#34;: true, \u0026#34;preemptionPolicy\u0026#34;: \u0026#34;PreemptLowerPriority\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;phase\u0026#34;: \u0026#34;Pending\u0026#34;, // 可以看到当前处于Pending阶段 \u0026#34;qosClass\u0026#34;: \u0026#34;BestEffort\u0026#34; } } 另外: 当你需要创建资源类型的时候,我并不建议你从头开始去编写相关文件,可以灵活的应用kubectl run来进行填充相关的字段信息。\nkubernetes设计理念 声明式：典型就是在资源文件中进行声明 无侵入性 可移植: 所有符合kubernetes标准的kubernetes平台都可以进行迁移 显示接口：所有的操作都是开放性的,不会存在私有接口,无论是Api-Server或者Clinet-go所操作的接口都是一模一样的。 创建资源的工作流程 首先当用户的请求进入到Api-Server后会进入到Authorization认证授权的处理接口实际上就是加载我们的config配置文件,其具体代码在loader.go:372进行实现 服务发现原理和应用 kubernetes中Pod的通信 每个Pod都有自己的IP分配 Pod间的可以通过IP进行通信 Pod的IP是可变的 Pod的IP通常不能被提前获取,一般都是网络插件进行分配 kubernetes的Service通信 它是一种抽象，帮助你将 Pod 集合在网络上公开出去。 每个 Service 对象定义端点的一个逻辑集合（通常这些端点就是 Pod）以及如何访问到这些 Pod 的策略。 我们通常可以使用以下命令来简单的暴露一个Service\nkubectl expose deploy/nginx --port=80 通常来说我们访问某个服务都是访问服务的IP地址，当然了,在kubernetes中访问Service对应的地址也可以访问到服务\n[root@Online-Beijing-master1 ~]# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wecho-canary ClusterIP 10.10.166.79 \u0026lt;none\u0026gt; 80/TCP,9113/TCP 69d kubernetes ClusterIP 10.10.0.1 \u0026lt;none\u0026gt; 443/TCP 279d 实际上也就是说当我需要访问wecho-canary这个服务的时候,我无需关心他后端end如何进行变化,我只需要记住访问wecho-canary的Service所给出的ClusterIP即可进行访问到相应的服务。\n另外的一种方式就是通过我们的Service名称来进行访问，下面说一下具体是如何通过Service名称进行访问的。\n我们假设使用上面的wecho-canary名称进行一次访问\ncurl -v wecho-canary # 假设访问正常 很简单,大家都知道当我们访问一个域名的时候背后肯定又DNS服务器来解析其所对应的IP地址,那么其实在kubernetes当中也有一个内部的DNS服务器,名字叫做kube-dns 我们可以通过kubectl get svc -A 进行查看,其中所指定的10.10.0.10就是我们kube-dns的IP地址.\nkube-system kube-dns ClusterIP 10.10.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 279d 看到我们kube-dns有以下端点\n[root@Online-Beijing-master1 ~]# kubectl get ep -n kube-system | grep kube-dns kube-dns 10.10.151.159:53,10.10.4.95:53,10.10.151.159:53 + 3 more... 279d kube-dns-upstream 10.10.151.159:53,10.10.4.95:53,10.10.151.159:53 + 1 more... 81d 其次可以看到我们容器内所对应的resolv.conf所写内容的配置文件\nsearch jiashicang.svc.cluster.local svc.cluster.local cluster.local nameserver 169.254.20.10 options ndots:5 single-request-reopen 由于我使用了NodeLocalDns所以我这个地方的nameserver不太一样,关于NodeLocalDns我们后续再说。 总结以下几点:\n当我们访问wecho-canary的时候实际上是访问了wecho-canary.default.cluster.local.svc,即ServiceName.Namespace.cluster.local.svc 所有的解析域名请求都会请求到kube-dns当中,当kube-dns无法完成解析的时候,我们会将请求forward到本地的解析文件当中,如果本地解析文件也无法解析则认为失败。典型的就是超时、无法解析等问题。 Service通过监视API server的数据变化来感知后端Pod的变化,并及时更新负载均衡规则。具体来说,Kubernetes ApiServer负责管理集群状态,它会记录每一个对象(包括Pod)的Spec和Status。Service对象使用ApiServer的watch接口,监视后端关联Pod对象的变化事件。比如Pod实例加入或销毁时,ApiServer会主动通知Service。一旦检测到Pod变化,Service会立即使用新的Pod列表,重新计算并更新自己负载均衡的端口转发规则。例如使用iptables规则或IPVS表更新后端目标地址。这样,无论Pod是动态伸缩还是故障转移,Service都能即时感知,保持负载均衡入口地址的高可靠性。这就是Service如何实现动态更新负载均衡规则的原理。 Service类型 ClusterIP: 通过集群的内部IP公开Service，选择该值时Service只能够在集群内部访问。这也是你没有为服务显式指定type时使用的默认值。 NodePort: 通过每个节点上的IP和静态端口NodePort公开 Service。为了让Service可通过节点端口访问，Kubernetes 会为 Service配置集群IP地址， 相当于你请求了type:ClusterIP的服务。 LoadBalancer:使用云平台的负载均衡器向外部公开Service,一般来说都用在云厂商才会使用LoadBalancer ExternalName:将服务映射到externalName字段的内容 例如，映射到主机名:api.foo.bar.example,该映射将集群的 DNS 服务器配置为返回具有该外部主机名值的CNAME记录。 常用的暴露Service的方法\nkubectl expose deploy/nginx --port=80 --type=NodePort 哦,对了,推荐一个开源的LB插件\nMetallb: Github 服务发现和流量路由 总的来说我们分为一下几点吧\nPod之间的流量通信: IP直连的方式: 不推荐这种方式,因为Pod的IP严格意义上来说对于服务版本更新生命周期相对较短。 ClusterIP： 相对于比较推荐通过指定ServiceName的方式来绑定活着监听某种服务 外部访问Pod的通信 NodePort: 通过暴露外部端口的方式来进行Pod的访问 LoadBalancer: 通过对Service绑定LB的方式进行Pod的访问 Pod访问外部通信: ExternalName: 将 Service 映射到 DNS 名称 HeadlessService: 当你不需要负载均衡，也不需要单独的 ServiceIP,你可以使用HeadlessService # 下面是ExternalName的简单实例 apiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com 一般来说Kubernetes的内部DNS记录有两种规范\nPodIP.Namespace.pod.cluster-domain.example --- ServiceName.Namespace.svc.cluster-domain.example ","date":"2023-11-07T00:00:00Z","image":"https://img13.360buyimg.com/ddimg/jfs/t1/164078/6/34165/22231/654ba81fFe1bf21d7/9d02bd791b795b3b.jpg","permalink":"http://localhost:1313/kubernetes/architectural_design/","title":"Kubernetes的架构设计和对象属性基本理解"},{"content":"简单功能 是男人，就来分享你拍的照片！！！ RESTFul API 一会儿再说。\n基本的后端技术栈 Gorm: 数据库工具 Gin: 速度极快的Go语言Web框架 Minio: 分布式存储 项目目录 一会儿再说\n项目缓存规范 RedisKey的规范 project:module:business:uk 项目名 模块名 业务名 唯一标识 缓存信息 这部分还没设计完成,等待完善吧。 Key 类型 过期时间 说明 wecho:user:access_token:{username} string 2天 存储用户生成的JWT wecho:userinfo:cache:{username} SET 3天 用户信息详情缓存 wecho:user:login_fail:{username} Incr 30Min 错误登录次数 常用代码片段 实现结构体 // UserDataService 用户管理服务 var UserDataService = newUserDataService() func newUserDataService() *userDataService { return \u0026amp;userDataService{} } type userDataService struct { } Minio启动命令 docker run -d \\ -p 9000:9000 \\ -p 9001:9001 \\ --name minio1 \\ -v ./data:/data \\ -v ./certs:/tmp/certs \\ -e \u0026#34;MINIO_ROOT_USER=xxx\u0026#34; \\ -e \u0026#34;MINIO_ROOT_PASSWORD=xxx\u0026#34; \\ -e \u0026#34;MINIO_SERVER_URL=xxx\u0026#34; \\ quay.io/minio/minio server /data --console-address \u0026#34;:9001\u0026#34; --certs-dir /tmp/certs 后端服务编译 # Windows编译Linux # 请在CMD中执行命令 set GOARCH=amd64 set GOOS=linux go build main.go # Mac编译Linux CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build main.go 接口统计 用户服务(普通用户) 获取用户详情信息 修改个人资料信息 修改个人密码 修改头像信息 绑定手机信息 绑定身份信息 关注用户 画板服务 获取画板详情信息 创建画板 更新画板 删除画板 获取画板列表信息 收藏画板 根据画板标签分类画板信息 审核服务 审核画板 审核画板图片 通信服务 发送私信 ","date":"2023-08-26T00:00:00Z","image":"https://ydschool-online.nosdn.127.net/tiku/f291c1c8d5dccacd2b95ef4fdc68d07cefd0e92562794e5bcdd21a9729b3455d.jpg","permalink":"http://localhost:1313/go/gin-wecho/","title":"维扣图元-专属你的拍照摄影社区"},{"content":"本主题内容题材取自bilibili-人生第一次(告别)\n漫长的告别 忘记我，也没事… 2019年12月9日\n巢文臻，73岁，四年前，老伴聂爱荣被确诊为阿尔兹海默症。\n随着老伴的病越来越严重…从一开始的忘记事情，到后来的忘记人…\n巢文臻在墙上的这块板子从来没有摘下。 而他也有在白板上，给自己写了一句大大的提示语： “别发火！”\n巢文臻看着熟睡的妻子，终于松了口气。\n他走进厨房，开始准备午餐。\n拿起菜刀的瞬间，他脑海中便浮现妻子曾经在厨房忙碌的身影。\n眼泪，止不住地从两颊滑落下来。\n从家到养护院，11站地铁，4站公交，步行800米。这样的路程老巢每周要走上四五次。\n路上的老巢，像是再赴一场甜蜜的约会。\n“聂爱荣” 我叫什么名字?\n从聂爱荣嘴里回答出的这句：“巢文臻”\n对于他来说，妻子回答的“巢文臻”三个字，便是世上最美的三字情书。\n“认识我！认识我真的非常开心”\n每当妻子说对了名字，巢文臻都会将她抱上许久，舍不得松手。\n妻子生病后，曾经内敛的巢文臻，变得越来越不吝啬于表达心中的爱意。\n只要见到她，便再也不放开紧握的手。\n老伴有骨质疏松，巢文臻每天都会带着聂爱荣慢慢的在院里散步。\n战友聚会、朋友聚会、巢文臻一概不去。\n他说：“我去了也不定心。”\n导演：“你这样不就没个人的生活了吗？”\n他说：“我这样也是一种开心，我没有觉得爱荣是我的负担。”\n我应如何爱你 照顾完聂爱荣后，老巢便回到了家。\n回到家后的老巢，沉默了许多。\n老伴离开了家，老巢就变成了\u0026quot;空巢\u0026quot;。\n千般难舍千般舍，万事不甩万事甩。\n2019年12月27日\n老巢病倒了…医生告诉他他的前列腺上有肿瘤\n老伴的身体每一天都比前一天更差，老巢说：“有一些事他要想到前面。”\n老巢去了中华遗嘱库，他要立下一些遗嘱，为以后做安排。\n立遗嘱，要自愿，头脑清晰，有表达能力，且不能有错别字，所以巢文臻只能一个人前来，既代表自己，也代表妻子。\n截止2019年底，中华遗嘱库已经保管了16.5万份遗嘱。\n这些立遗嘱的人，就像是小学生一样，趴在课桌上写着自己人生的作业。\n不能有一个错别字。\n这是巢文臻入伍五十年以来留下的感言\n半个世纪，感慨万千。\n如有来生，再续前缘？\n他不知道有没有来生。\n在立遗嘱时，别的老人都是写给子女一番话，而巢文臻提起笔后，则是思考再三，写下了一首告别诗：\n天堂之门向我开，不尽思绪滚滚来；\n千般难舍千般舍，万事不甩万事甩。\n幸喜寒门志不衰，频遇艰困仰众爱；\n愿把皮囊献杏林，魂归父母应节哀。\n请记住我 2020年，聂爱荣再次摔倒了。\n而且，是两次。\n巢文臻无法如往常一般去养老院，他整日魂不守舍，开始胡思乱想。\n他害怕，妻子摔倒后是否能正常生活；他害怕，妻子一日三餐饮食不规律。\n他更怕，下一次见面，妻子已经不记得他了。\n2020年初，新冠疫情大爆发。巢文臻已经两个月没有见到妻子聂爱荣了。\n为了缓解思念，巢文臻开始给妻子写信。\n因为妻子已经不识字，他只能对着手机录制视频，然后发给护士给妻子看这一封封饱含思念的“电子信”。 信上说：\n“爱荣啊…从年初至今已有两个月未能与你见面，心中十分想念。\n回想我们结婚以来的四十多年，同甘苦共患难，经历了八次搬家。\n其中有八年，你都是（跟我）睡在地板上的，但你从无怨言。\n在家里你是标准的贤妻良母，相夫教子，到老了没享多少福，你就生病了…”\n说到这里，巢文臻拿信的手已经开始颤抖，他强忍着情绪读完了信件。\n巢文臻一直有件事情瞒着家人。\n自从妻子患病之后，他也得了抑郁症，在这期间，他四处寻药，想尽一切办法不让自己倒下。\n因为他深知，只有自己不倒，才能减轻孩子的负担，妻子才能生活的更好一些。\n这些年，有九个字概括了他全部的生活：\n“睹物思人，魂不守舍，肝肠寸断。”\n他说：“这真的是生离死别，我根本不想放开她”\n“没办法，我知道后面会发生什么”\n或许，生的对立面并不是死亡，而是遗忘。\n纪录片的最后，巢文臻表示希望未来妻子比自己先走。\n这样，他就能照顾她一辈子了。\n截止2022年，我国阿尔茨海默病患者约1000万人，居全球之首！\n预计到2050年将突破4000万人。\n这些冰冷的数字背后，是一个个无比痛苦的家庭。\n作为普通人，我们无法阻止死亡的到来，但是正如皮克斯电影《寻梦环游记》插曲里唱到的一般：\n请记得我，虽然再见必须说。\n那或者又是…“当你老到忘了世界 我用什么来爱你。”\n","date":"2023-08-10T00:00:00Z","image":"https://img12.360buyimg.com/ddimg/jfs/t1/192726/25/35567/27205/64d4df78Ff9979209/ecdcae14b5eefcd1.jpg","permalink":"http://localhost:1313/documentary/valedictory/","title":"纪录片-人生第一次-告别"},{"content":"互联网服务发展至今，作为开发者阵营的我们，已经用实践证明了前后端分离开发模式正在逐渐成为越来越多互联网公司构建服务和应用的方式。\n前后端分离优势多多，其中一个很重要的优势是：对于后台服务（系统）来讲，只需提供一套统一的API接口，可被多个客户端所复用，分工和协作被细化，大大提高了效率。\n与此同时带来的一些副作用便是：\n接口文档管理混乱。之前很多公司管理API接口，有用Wiki的，有Word文档的，有Html的，经常遇到问题是接口因变了，比如增加参数，参数名变了，参数被删除了等都没有及时更新文档的情况 接口测试没有保障。毕竟前端开发依赖后端接口，如果前后端开发不同步，接口及时测试成了问题，因此需要随时提供一套可用的API接口数据测试服务。 资源分散，难以共享。每个开发者维护自己的一套测试接口集合，无法共用他人接口集合，开发过程中充斥着大量重复造数据、填接口的工作，效率不高 其他问题。除此之外还有可能碰到诸如 文档导出、接口分类规划、操作便利性等一系列问题。 基于此情况，因此本文接下来就来推荐几个常用的 API管理系统，帮助前后端分离开发模式下提升效率和可靠性，我想总有一个适合阁下吧☁️\nSwagger Swagger 是一种用于描述、构建和可视化 RESTful API 的开源工具集。它提供了一系列功能，包括 API 文档自动生成、API 调试和可视化等。下面是使用 Swagger 的一般步骤：\n定义 API 规范：使用 Swagger 规范（通常是 OpenAPI 规范）编写 API 的定义和描述。这些规范使用 YAML 或 JSON 格式表示，并描述了 API 的路径、参数、操作、响应等信息。 编写 Swagger 文档：根据 API 的定义和描述编写 Swagger 文档。您可以使用 Swagger 编辑器或其他文档工具来创建和编辑 Swagger 文档。 自动生成文档：使用 Swagger 工具和插件，将 Swagger 文档与代码（如后端服务代码）集成在一起，并生成 API 文档。这些工具可以根据 Swagger 文档自动生成可交互式的 API 文档和UI界面。 调试和测试：使用 Swagger 提供的内置功能，可以在 Swagger UI 中直接进行 API 调试和测试。通过 Swagger UI，您可以轻松地发送请求，查看响应并检查请求和响应的详细信息。 项目地址： 点我进入\nDemo地址： 点我进入\neolinker eolinker是一款用于API管理和接口测试的工具。它提供了一个用户友好的界面，可以帮助开发人员和团队更好地管理和测试他们的API接口。\neolinker的主要功能包括：\n接口文档管理：可以方便地创建、编辑和共享接口文档，包括接口的URL、请求方式、参数、返回结果等。 接口测试：可以对接口进行自动化测试，验证接口的正确性和性能。 接口联调：可以方便地进行接口联调，支持多人协作，提高开发效率。 接口监控：可以实时监控接口的状态和性能，及时发现和解决问题。 接口权限管理：可以对接口进行权限控制，保证接口的安全性和机密性。 项目地址：点我进入\nShowDoc 首先这是我最推荐的一个API管理工具，因为界面简单，风格清晰，个人觉得很好看的UI界面！！！\nShowDoc的主要功能包括：\n文档编写：提供了富文本编辑器和Markdown编辑器，可以方便地编写项目文档，支持插入图片、代码块等功能。 接口管理：可以创建和管理API接口文档，包括接口的URL、请求方式、参数、返回结果等信息，还可以进行接口测试。 团队协作：支持多人协作，可以邀请团队成员共同编辑和管理文档，方便团队间的沟通和合作。 版本控制：支持对文档的版本控制，可以查看历史版本，并进行比较和恢复。 权限管理：可以对文档进行权限控制，设置不同的用户角色和权限，保证文档的安全性和机密性。 导出和分享：支持将文档导出为HTML、PDF和Markdown格式，方便分享和发布。 项目地址：点我进入\nDemo：点我进入\n","date":"2023-08-10T00:00:00Z","image":"https://img11.360buyimg.com/ddimg/jfs/t1/120366/32/35885/27184/64a24181F7497ff67/58deb560821eb1b2.jpg","permalink":"http://localhost:1313/posts/api/","title":"推荐几款好用的API文档管理工具"},{"content":"OpenEBS存储使用 OpenEBS 是一种模拟了 AWS 的 EBS、阿里云的云盘等块存储实现的基于容器的存储开源软件。OpenEBS 是一种基于 CAS(Container Attached Storage) 理念的容器解决方案，其核心理念是存储和应用一样采用微服务架构，并通过 Kubernetes 来做资源编排。其架构实现上，每个卷的 Controller 都是一个单独的 Pod，且与应用 Pod 在同一个节点，卷的数据使用多个 Pod 进行管理。\nOpenEBS 有很多组件，可以分为以下几类：\n控制平面组件 - 管理 OpenEBS 卷容器，通常会用到容器编排软件的功能 数据平面组件 - 为应用程序提供数据存储，包含 Jiva 和 cStor 两个存储后端 节点磁盘管理器 - 发现、监控和管理连接到 Kubernetes 节点的媒体 与云原生工具的整合 - 与 Prometheus、Grafana、Fluentd 和 Jaeger 进行整合。 控制平面 OpenEBS 上下文中的控制平面是指部署在集群中的一组工具或组件，它们负责：\n管理 kubernetes 工作节点上可用的存储 配置和管理数据引擎 与 CSI 接口以管理卷的生命周期 与 CSI 和其他工具进行接口，执行快照、克隆、调整大小、备份、恢复等操作。 集成到其他工具中，如 Prometheus/Grafana 以进行遥测和监控 集成到其他工具中进行调试、故障排除或日志管理 OpenEBS 控制平面由一组微服务组成，这些微服务本身由 Kubernetes 管理，使 OpenEBS 真正成为 Kubernetes 原生的。由 OpenEBS 控制平面管理的配置被保存为 Kubernetes 自定义资源。控制平面的功能可以分解为以下各个阶段：\nOpenEBS 提供了一个动态供应器，它是标准的 Kubernetes 外部存储插件。OpenEBS PV 供应器的主要任务是向应用 Pod 发起卷供应，并实现Kubernetes 的 PV 规范。\nm-apiserver 暴露了存储 REST API，并承担了大部分的卷策略处理和管理。\n控制平面和数据平面之间的连接采用 Kubernetes sidecar 模式。有如下几个场景，控制平面需要与数据平面进行通信。\n对于 IOPS、吞吐量、延迟等卷统计 - 通过 volume-exporter sidecar实现 用于通过卷控制器 Pod 执行卷策略，以及通过卷复制 Pod 进行磁盘/池管理 - 通过卷管理 sidecar 实现。 OpenEBS Local Pv OpenEBS 为Kubernetes Local Volumes提供动态 PV 供应器。本地卷意味着存储只能从单个节点使用。本地卷表示已挂载的本地存储设备，例如磁盘、分区或目录。\n由于 Local Volume 只能从单个节点访问，因此本地卷受底层节点可用性的影响，并不适合所有应用程序。如果一个节点变得不健康，那么本地卷也将变得不可访问，使用它的 Pod 将无法运行。使用本地卷的应用程序必须能够容忍这种可用性降低以及潜在的数据丢失，具体取决于底层磁盘的耐用性特征。\n可以从本地卷中受益的良好工作负载示例包括：\n复制数据库，如 MongoDB、Cassandra 可以使用自己的高可用性配置（如 Elastic、MinIO）配置的有状态工作负载 通常在单个节点或单节点 Kubernetes 集群中运行的边缘工作负载。 OpenEBS 通过提供 Kubernetes 当前缺少的功能来帮助用户将本地卷投入生产，例如：\n本地卷的动态 PV Provisioner。 由 Ext3、XFS、LVM 或 ZFS 等文件系统上的主机路径支持的本地卷。 监控用于创建本地卷的底层设备或存储的健康状况。 容量管理功能，如过度配置和/或配额强制执行。 当本地卷由 ZFS 等高级文件系统支持时，可以使用快照、克隆、压缩等底层存储功能。 通过 Velero 进行备份和恢复。 通过 LUKS 或使用底层文件系统（如 ZFS）的内置加密支持来保护本地卷。 节点磁盘管理器(NDM) 节点磁盘管理器（NDM）是OpenEBS架构中的一个重要组件。NDM 将块设备视为需要监控和管理的资源，就像其他资源（如 CPU、内存和网络）一样。它是一个运行在每个节点上的守护进程，根据过滤器检测附加的块设备并将它们作为块设备自定义资源加载到 Kubernetes 中。这些自定义资源旨在通过提供以下功能来帮助超融合存储运营商：\n易于访问 Kubernetes 集群中可用的块设备清单。 预测磁盘故障以帮助采取预防措施。 允许动态地将磁盘附加/分离到存储 pod，而无需重新启动在磁盘附加/分离的节点上运行的相应 NDM pod。 Node Disk Manager在Kubernetes中是以DaemonSet的方式进行运行的 Node Disk Manager 尽管执行了上述所有操作，但 NDM 有助于整体简化持久卷的配置。\nNDM 在安装 OpenEBS 期间部署为守护进程。NDM daemonset 发现每个节点上的磁盘并创建称为块设备或 BD 的自定义资源。\n启用OpenEBS 由于 OpenEBS 通过 iSCSI 协议提供存储支持，因此，需要在所有 Kubernetes 节点上都安装 iSCSI 客户端（启动器）。\n比如我们这里使用的是Rocky的系统，执行下面的命令安装启动 iSCSI 启动器：\ndnf install iscsi-initiator-utils -y # 查看iSCSI状态是否正常 cat /etc/iscsi/initiatorname.iscsi # 启动iSCSI systemctl start iscsid.service systemctl status iscsid.service 安装OpenEBS 使用kubectl的方式进行安装 Helm部署也是可选的: 地址 [root@Online-Beijing-master1 ~]# kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml # 检查是否安装完成,正常应该都是Running即可 [root@Online-Beijing-master1 ~]# kubectl get pods -n openebs NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-846c6bdc56-vvvsv 1/1 Running 0 6m11s openebs-ndm-5sfdk 1/1 Running 0 6m11s openebs-ndm-cluster-exporter-b49987ffb-vjq87 1/1 Running 0 6m11s openebs-ndm-kthfj 1/1 Running 0 6m11s openebs-ndm-node-exporter-94zhh 1/1 Running 0 6m11s openebs-ndm-node-exporter-q9h5p 1/1 Running 0 6m11s openebs-ndm-node-exporter-x7z9t 1/1 Running 0 6m11s openebs-ndm-operator-6469f6bb4c-95kss 1/1 Running 0 6m11s openebs-ndm-wf9k2 1/1 Running 0 6m11s # 正常我们会有两个StorageClass [root@Online-Beijing-master1 ~]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-device openebs.io/local Delete WaitForFirstConsumer false 20m openebs-hostpath openebs.io/local Delete WaitForFirstConsumer false 20m 我们自己创建一个Pvc对象来给我们的Deployment来进行使用 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: local-hostpath-pvc spec: storageClassName: openebs-hostpath accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 创建一个Pod进行测试 apiVersion: v1 kind: Pod metadata: name: hello-local-hostpath-pod spec: volumes: - name: local-storage persistentVolumeClaim: claimName: local-hostpath-pvc containers: - name: hello-container image: busybox command: - sh - -c - \u0026#39;while true; do echo \u0026#34;`date` [`hostname`] Hello from OpenEBS Local PV.\u0026#34; \u0026gt;\u0026gt; /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done\u0026#39; volumeMounts: - mountPath: /data name: local-storage 注意：如果你想知道这个Pvc具体挂载位置可以使用kubectl describe pv pv名称,其中的Ptah: /var/openebs/local/pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188就是我们所在节点的路径\n[root@Online-Beijing-master1 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE example-local 20Gi RWO Delete Bound default/bound-tasknginx local-storage 26d pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 1Gi RWO Delete Bound default/local-hostpath-pvc openebs-hostpath 3m53s [root@Online-Beijing-master1 ~]# kubectl describe pv pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 Name: pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 Labels: openebs.io/cas-type=local-hostpath Annotations: pv.kubernetes.io/provisioned-by: openebs.io/local Finalizers: [kubernetes.io/pv-protection] StorageClass: openebs-hostpath Status: Bound Claim: default/local-hostpath-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 1Gi Node Affinity: Required Terms: Term 0: kubernetes.io/hostname in [online-beijing-node1] Message: Source: Type: LocalVolume (a persistent volume backed by local storage on a node) Path: /var/openebs/local/pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 Events: \u0026lt;none\u0026gt; 进入所在节点的目录进行验证 cd /var/openebs/local/pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 # 随便创建一个文件 touch openebs.txt # 进入容器内部 kubectl exec -it nginx-67fbcff654-glklg bash # 查看所挂载openebs的路径是否成功有openebs.txt root@nginx-67fbcff654-glklg:/# ls /data/ 1.txt openebs.txt 修改OpenEBS的HostPath默认存储 修改名字为openebs-hostpath的StorageClass当中的BasePath即可 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: openebs-hostpath annotations: openebs.io/cas-type: local cas.openebs.io/config: | #hostpath type will create a PV by # creating a sub-directory under the # BASEPATH provided below. - name: StorageType value: \u0026#34;hostpath\u0026#34; #Specify the location (directory) where # where PV(volume) data will be saved. # A sub-directory with pv-name will be # created. When the volume is deleted, # the PV sub-directory will be deleted. #Default value is /var/openebs/local - name: BasePath value: \u0026#34;/var/openebs/local/\u0026#34; ","date":"2023-05-14T00:00:00Z","image":"https://openebs.io/docs/assets/images/control-plane-overview-93c59878e3356a11f03029dd0fc1cd6b.svg","permalink":"http://localhost:1313/kubernetes/openebs/","title":"OpenEBS存储的使用"},{"content":"Traekfik是什么 Traefik 是一种开源 边缘路由器，它使您发布服务成为一种有趣而轻松的体验。它代表您的系统接收请求并找出哪些组件负责处理它们。\nTraefik 的与众不同之处在于，除了它的许多功能之外，它还可以自动为您的服务发现正确的配置。当 Traefik 检查您的基础架构时，奇迹就会发生，它会在其中找到相关信息并发现哪个服务服务于哪个请求。\nTraefik 原生兼容所有主要的集群技术，例如 Kubernetes、Docker、Docker Swarm、AWS、Mesos、Marathon，等等；并且可以同时处理很多。（它甚至适用于在裸机上运行的遗留软件。）\n使用 Traefik，无需维护和同步单独的配置文件：一切都自动实时发生（无需重启，无连接中断）。使用 Traefik，您可以花时间为系统开发和部署新功能，而不是配置和维护其工作状态。\n边缘路由器 Traefik 是一个Edge Router，这意味着它是您平台的大门，它拦截并路由每个传入请求：它知道确定哪些服务处理哪些请求的所有逻辑和每条规则（基于path，host，标头，等等…）。\n自动服务发现 传统上边缘路由器（或反向代理）需要一个配置文件，其中包含到您的服务的每条可能路径，Traefik 从服务本身获取它们。部署您的服务，您附加信息告诉 Traefik 服务可以处理的请求的特征。\n首先，当启动 Traefik 时，需要定义 entrypoints（入口点），然后，根据连接到这些 entrypoints 的路由来分析传入的请求，来查看他们是否与一组规则相匹配，如果匹配，则路由可能会将请求通过一系列中间件转换过后再转发到你的服务上去。在了解 Traefik 之前有几个核心概念我们必须要了解：\nProviders 用来自动发现平台上的服务，可以是编排工具、容器引擎或者 key-value 存储等，比如 Docker、Kubernetes、File Entrypoints 监听传入的流量（端口等…），是网络入口点，它们定义了接收请求的端口（HTTP 或者 TCP）。 Routers 分析请求（host, path, headers, SSL, …），负责将传入请求连接到可以处理这些请求的服务上去。 Services 将请求转发给你的应用（load balancing, …），负责配置如何获取最终将处理传入请求的实际服务。 Middlewares 中间件，用来修改请求或者根据请求来做出一些判断（authentication, rate limiting, headers, …），中间件被附件到路由上，是一种在请求发送到你的服务之前（或者在服务的响应发送到客户端之前）调整请求的一种方法。 部署Traefik Traefik的配置可以使用两种方式：静态配置和动态配置\n静态配置：在 Traefik 中定义静态配置选项有三种不同的、互斥的即你只能同时使用一种）方式。 在配置文件中 在命令行参数中 作为环境变量 动态配置：Traefik从提供者处获取其动态配置：无论是编排器、服务注册表还是普通的旧配置文件。 # 使用Helm的方式进行部署Traefik2.9.x [root@Online-Beijing-master1 ~]# helm repo add traefik https://traefik.github.io/charts [root@Online-Beijing-master1 ~]# helm repo update [root@Online-Beijing-master1 yaml]# helm fetch traefik/traefik [root@Online-Beijing-master1 yaml]# tar -zxf traefik-21.1.0.tgz 修改一下value.yaml中的部分内容，改动大概如下部分的内容 deployment: initContainers: # The \u0026#34;volume-permissions\u0026#34; init container is required if you run into permission issues. # Related issue: https://github.com/traefik/traefik/issues/6825 - name: volume-permissions image: busybox:1.35 command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;touch /data/acme.json \u0026amp;\u0026amp; chmod -Rv 600 /data/* \u0026amp;\u0026amp; chown 65532:65532 /data/acme.json\u0026#34;] volumeMounts: - name: data mountPath: /data websecure: port: 8443 hostPort: 443 expose: true exposedPort: 443 protocol: TCP web: port: 8000 hostPort: 80 expose: true exposedPort: 80 protocol: TCP service: enabled: false ingressRoute: dashboard: enabled: false nodeSelector: node.kubernetes.io/traefik-manager: \u0026#39;true\u0026#39; tolerations: - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 创建一个traefik-v2的名称空间 [root@Online-Beijing-master1 yaml]# kubectl create ns traefik-v2 部署Traefik [root@Online-Beijing-master1 yaml]# helm install traefik ./traefik -f ./traefik/values.yaml --namespace traefik-v2 [root@Online-Beijing-master1 yaml]# kubectl get pods traefik-67b8896675-4xdrx -n traefik-v2 -o yaml 其中 entryPoints 属性定义了 web 和 websecure 这两个入口点的，并开启 kubernetesingress 和 kubernetescrd 这两个 provider，也就是我们可以使用 Kubernetes 原本的 Ingress 资源对象，也可以使用 Traefik 自己扩展的 IngressRoute 这样的 CRD 资源对象\napiVersion: v1 kind: Pod metadata: ...... spec: containers: - args: - --global.checknewversion - --global.sendanonymoususage - --entrypoints.metrics.address=:9100/tcp - --entrypoints.traefik.address=:9000/tcp - --entrypoints.web.address=:8000/tcp - --entrypoints.websecure.address=:8443/tcp - --api.dashboard=true - --ping=true - --metrics.prometheus=true - --metrics.prometheus.entrypoint=metrics - --providers.kubernetescrd - --providers.kubernetesingress - --entrypoints.websecure.http.tls=true 创建用于 Dashboard 访问的 IngressRoute 资源 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-dashboard namespace: traefik-v2 spec: entryPoints: - web # 这里对应的是web的entryPoints 如果是https就需要使用websecure的entryPoints routes: - match: Host(`traefik.kube.com`) # 指定域名 kind: Rule services: - name: api@internal kind: TraefikService # 引用另外的 Traefik Service [root@Online-Beijing-master1 yaml]# kubectl apply -f traefik.yaml ingressroute.traefik.containo.us/traefik-dashboard created\tTraefik的基本使用 自定义一个IngressRoute 假设我们要访问一个简单地nginx服务,下面是traefik的匹配规则\n匹配规则 描述 HeadersRegexp(key, regexp) key检查标题中是否定义了键，其值与正则表达式匹配regexp Host(example.com, ...) 检查请求域（主机标头值）是否针对给定的domains. HostHeader(example.com, ...) 与 相同Host，仅因历史原因而存在。 HostRegexp(example.com, {subdomain:[a-z]+}.example.com, ...) 匹配请求域。请参阅下面的“正则表达式语法”。 Method(GET, ...) 检查请求方法是否为给定的methods( GET, POST, PUT, DELETE, PATCH, HEAD)之一 Path(/path, /articles/{cat:[a-z]+}/{id:[0-9]+}, ...) 匹配准确的请求路径。请参阅下面的“正则表达式语法”。 PathPrefix(/products/, /articles/{cat:[a-z]+}/{id:[0-9]+}) 匹配请求前缀路径。请参阅下面的“正则表达式语法”。 Query(foo=bar, bar=baz) 匹配查询字符串参数。它接受一系列键=值对。 ClientIP(10.0.0.0/16, ::1) 如果请求客户端 IP 是给定的 IP/CIDR 之一，则匹配。它接受 IPv4、IPv6 和 CIDR apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-http namespace: default spec: entryPoints: - web # 依旧对应web routes: - match: Host(`traefik.qingyang.com`) # 指定域名 kind: Rule services: - name: vue-demo # 这个地方对应kubernetes的svc port: 80 如果我们需要开启HTTPS来访问我们这个应用的话，就需要监听 websecure 这个入口点，也就是通过 443 端口来访问，同样用 HTTPS 访问应用必然就需要证书，这里我们用 openssl 来创建一个自签名的证书：\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=traefik.qingyang.com\u0026#34; 然后创建secret\n[root@Online-Beijing-master1 yaml]# kubectl create secret tls traefik-demo-tls --cert=tls.crt --key=tls.key 这个时候我们就可以创建一个 HTTPS 访问应用的 IngressRoute 对象了\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-https spec: entryPoints: - websecure routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: vue-demo port: 80 tls: secretName: traefik-demo-tls ACME自动签发 raefik 同样也支持使用 Let’s Encrypt 自动生成证书，要使用 Let’s Encrypt 来进行自动化 HTTPS，就需要首先开启 ACME，开启 ACME 需要通过静态配置的方式，也就是说可以通过环境变量、启动参数等方式来提供。\nACME 有多种校验方式 tlsChallenge、httpChallenge 和 dnsChallenge 三种验证方式，之前更常用的是 http 这种验证方式(可以百度一下)这几种的校验方式。要使用 tls 校验方式的话需要保证 Traefik 的 443 端口是可达的，dns 校验方式可以生成通配符的证书，只需要配置上 DNS 解析服务商的 API 访问密钥即可校验。我们这里用 DNS 校验的方式来为大家说明如何配置 ACME。\n重新修改一下刚刚Traefik的参数values.yaml additionalArguments: # 使用 dns 验证方式 - --certificatesResolvers.ali.acme.dnsChallenge.provider=alidns # 先使用staging环境进行验证，验证成功后再使用移除下面一行的配置 # - --certificatesResolvers.ali.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory # 邮箱配置 - --certificatesResolvers.ali.acme.email=ych_1024@163.com # 保存 ACME 证书的位置 - --certificatesResolvers.ali.acme.storage=/data/acme.json envFrom: - secretRef: name: traefik-alidns-secret # ALICLOUD_ACCESS_KEY # ALICLOUD_SECRET_KEY # ALICLOUD_REGION_ID persistence: enabled: true # 开启持久化 accessMode: ReadWriteOnce size: 128Mi path: /data # 由于上面持久化了ACME的数据，需要重新配置下面的安全上下文 securityContext: readOnlyRootFilesystem: false runAsGroup: 0 runAsUser: 0 runAsNonRoot: false 然后更新traefik\nhelm up traefik ./traefik -f ./traefik/values.yaml --namespace traefik-v2 这样我们可以通过设置 --certificatesresolvers.ali.acme.dnschallenge.provider=alidns 参数来指定指定阿里云的 DNS 校验，要使用阿里云的 DNS 校验我们还需要配置3个环境变量：ALICLOUD_ACCESS_KEY、ALICLOUD_SECRET_KEY、ALICLOUD_REGION_ID，分别对应我们平时开发阿里云应用的时候的密钥，可以登录阿里云后台获取，由于这是比较私密的信息，所以我们用 Secret 对象来创建 kubectl create secret generic traefik-alidns-secret --from-literal=ALICLOUD_ACCESS_KEY=\u0026lt;aliyun ak\u0026gt; --from-literal=ALICLOUD_SECRET_KEY=\u0026lt;aliyun sk\u0026gt; --from-literal=ALICLOUD_REGION_ID=cn-beijing -n traefik-v2 创建完成后将这个 Secret 通过环境变量配置到 Traefik 的应用中，还有一个值得注意的是验证通过的证书我们这里存到 /data/acme.json 文件中，我们一定要将这个文件持久化，否则每次 Traefik 重建后就需要重新认证，而 Let’s Encrypt 本身校验次数是有限制的。所以我们在 values 中重新开启了数据持久化，不过开启过后需要我们提供一个可用的 PV 存储，由于我们将 Traefik 固定到 master1 节点上的，所以我们可以创建一个 hostpath 类型的 PV apiVersion: v1 kind: PersistentVolume metadata: name: traefik spec: accessModes: - ReadWriteOnce capacity: storage: 128Mi hostPath: path: /data/k8s/traefik 更新IngressRoute apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-https spec: entryPoints: - websecure routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: vue-demo port: 80 tls: certResolver: ali domains: - main: \u0026#34;*.qingyang.com\u0026#34; 只需要将 tls 部分改成我们定义的 ali 这个证书解析器，如果我们想要生成一个通配符的域名证书的话可以定义 domains 参数来指定，然后更新 IngressRoute 对象，这个时候我们再去用 HTTPS 访问我们的应用（当然需要将域名在阿里云 DNS 上做解析）\n中间件 连接到路由器的中间件是一种在将请求发送到您的服务之前（或在将服务的答案发送到客户端之前）调整请求的方法。\nTraefik 中有几个可用的中间件，有的可以修改请求，headers，有的负责重定向，有的添加认证等等。\n使用相同协议的中间件可以组合成链以适应各种场景。\nHTTP中间件列表 TCP中间件列表 强制跳转Https Traefik 中也是可以配置强制跳转的，只是这个功能现在是通过中间件来提供的了。如下所示，我们使用 redirectScheme 中间件来创建提供强制跳转服务\n# Redirect to https apiVersion: traefik.containo.us/v1alpha1 kind: Middleware # 创建一个中间件 metadata: name: test-redirectscheme spec: redirectScheme: scheme: https permanent: true --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-http namespace: default spec: entryPoints: - web routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: vue-demo port: 80 middlewares: - name: test-redirectscheme # 指定添加中间件的名称 如果你有具体需求的话请前往官网文档查看更多中间件的适用方法。\nTraefik Pilot 虽然 Traefik 已经默认实现了很多中间件，可以满足大部分我们日常的需求，但是在实际工作中，用户仍然还是有自定义中间件的需求，这就 Traefik Pilot 的功能了。\nTraefik Pilot 是一个 SaaS 平台，和 Traefik 进行链接来扩展其功能，它提供了很多功能，通过一个全局控制面板和 Dashboard 来增强对 Traefik 的观测和控制：\nTraefik 代理和代理组的网络活动的指标 服务健康问题和安全漏洞警报 扩展 Traefik 功能的插件 在 Traefik 可以使用 Traefik Pilot 的功能之前，必须先连接它们，我们只需要对 Traefik 的静态配置进行少量更改即可。\n注意: Traefik 代理必须要能访问互联网才能连接到 Traefik Pilot，通过 HTTPS 在 443 端口上建立连接。 这个我就不演示了，我的虚拟机木有外网。\n灰度发布 跟Ingress-nginx是一样的就不多介绍灰度发布是啥了\n基于权重的轮询 这次使用的Deployment和上次Ingress-nginx一样，大家去Ingress那篇文章去找一下咯\n我们可以直接利用TraefikService这个对象来配置基于权重的轮询\napiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: auy-cat-wrr spec: weighted: services: - name: production weight: 3 # 定义权重 port: 80 kind: Service - name: canary-service weight: 1 port: 80 然后修改我们IngressRoute中使用的TraefikService\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-http namespace: default spec: entryPoints: - web routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: auy-cat-wrr kind: TraefikService # 使用声明的 TraefikService 服务，而不是 K8S 的 Service 流量复制 除了灰度发布之外，Traefik还引入了流量镜像服务，是一种可以将流入流量复制并同时将其发送给其他服务的方法，镜像服务可以获得给定百分比的请求同时也会忽略这部分请求的响应。\n假设我们刚刚的production为线上服务canary为预览服务,现在希望请求production的流量同时复制一份也请求到canary版本中\napiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: mirror-replication spec: mirroring: name: production port: 80 mirrors: - name: canary-service percent: 50 port: 80 --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-http namespace: default spec: entryPoints: - web routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: mirror-replication kind: TraefikService # 使用声明的 TraefikService 服务，而不是 K8S 的 Service 代理Tcp/Udp 另外 Traefik2.X 已经支持了 TCP 服务的，下面我们以 mongo 为例来了解下 Traefik 是如何支持 TCP 服务得。\napiVersion: apps/v1 kind: Deployment metadata: name: mongo-traefik labels: app: mongo-traefik spec: selector: matchLabels: app: mongo-traefik template: metadata: labels: app: mongo-traefik spec: containers: - name: mongo image: mongo ports: - containerPort: 27017 --- apiVersion: v1 kind: Service metadata: name: mongo-traefik spec: selector: app: mongo-traefik ports: - port: 27017 新增Traefik的入口点 ports: web: port: 8000 hostPort: 80 websecure: port: 8443 hostPort: 443 mongo: port: 27017 hostPort: 27017 这里给入口点添加 hostPort 是为了能够通过节点的端口访问到服务，关于 entryPoints 入口点的更多信息，可以查看文档 entrypoints 了解更多信息。\nhelm upgrade --install traefik ./traefik -f ./traefik/values.yaml -n traefik-v2 由于 Traefik 中使用 TCP 路由配置需要 SNI，而 SNI 又是依赖 TLS 的，所以我们需要配置证书才行，如果没有证书的话，我们可以使用通配符 * 进行配置，我们这里创建一个 IngressRouteTCP 类型的 CRD apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: mongo-traefik-tcp spec: entryPoints: - mongo routes: - match: HostSNI(`*`) services: - name: mongo-traefik port: 27017 我这里没有mongo的客户端,直接校验一下端口就行了\ntelnet traefik.qingyang.com 27017 使用特定的域名进行代理访问 假设我现在有一个mysql服务，想通过traefik.mysql.prod进行连接访问.\n我们在加一个proxy-mysql的entryPoints\nports: web: port: 8000 hostPort: 80 websecure: port: 8443 hostPort: 443 mongo: port: 27017 hostPort: 27017 proxy-mysql: port: 3306 hostPort: 3306 生成一个traefik.mysql.prod的自签名证书 装个Golangdnf install -y go git clone https://github.com/jsha/minica.git cd minica go build [root@Online-Beijing-master1 minica]# ./minica --domains \u0026#39;traefik.mysql.prod\u0026#39; [root@Online-Beijing-master1 minica]# cd traefik.mysql.prod/ 生成secret,请确保你处于当前的cert.key和key.pem的目录下 kubectl create secret tls tcp-demo-mysql --cert=cert.pem --key=key.pem 创建IngressRouteTCP对象 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: tcp-inner-mysql namespace: default spec: entryPoints: - proxy-mysql routes: - match: HostSNI(`traefik.mysql.prod`) services: - name: env-prod-mysql-svc port: 3306 tls: # 绑定Tls secretName: tcp-demo-mysql 代理一个Udp服务 部署一个UDP服务 kind: Deployment apiVersion: apps/v1 metadata: name: whoami labels: app: whoami spec: replicas: 1 selector: matchLabels: app: whoami template: metadata: labels: app: whoami spec: containers: - name: whoami image: containous/whoamiudp ports: - name: web containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: whoamiudp spec: ports: - protocol: UDP name: udp port: 8080 selector: app: whoami 在Traefik当中添加UDP的入口点，老样子修改values.yaml udpend: port: 18080 hostPort: 18080 protocol: UDP UDP 的入口点增加成功后，接下来我们可以创建一个 IngressRouteUDP 类型的资源对象，用来代理 UDP 请求： apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteUDP metadata: name: whoamiudp spec: entryPoints: - udpend routes: - services: - name: whoamiudp port: 80 ","date":"2023-04-07T00:00:00Z","image":"https://doc.traefik.io/traefik/assets/img/traefik-architecture.png","permalink":"http://localhost:1313/kubernetes/traefik/","title":"Traekfik基础使用指南"},{"content":"本地存储 前面我们有通过 hostPath 或者 emptyDir 的方式来持久化我们的数据，但是显然我们还需要更加可靠的存储来保存应用的持久化数据，这样容器在重建后，依然可以使用之前的数据。但是存储资源和 CPU 资源以及内存资源有很大不同，为了屏蔽底层的技术实现细节，让用户更加方便的使用，Kubernetes 便引入了 PV 和 PVC 两个重要的资源对象来实现对存储的管理。\nPersistentVolume PV 的全称是：PersistentVolume（持久化卷），是对底层共享存储的一种抽象，PV 由管理员进行创建和配置，它和具体的底层的共享存储技术的实现方式有关，比如 Ceph、GlusterFS、NFS、hostPath 等，都是通过插件机制完成与共享存储的对接。\nPersistentVolumeClaim PVC 的全称是：PersistentVolumeClaim（持久化卷声明），PVC 是用户存储的一种声明，PVC 和 Pod 比较类似，Pod 消耗的是节点，PVC 消耗的是 PV 资源，Pod 可以请求 CPU 和内存，而 PVC 可以请求特定的存储空间和访问模式。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。\n但是通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求，而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等，为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等，用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了，此外 StorageClass 还可以为我们自动生成 PV，免去了每次手动创建的麻烦。\nHostPath 我们上面提到了 PV 是对底层存储技术的一种抽象，PV 一般都是由管理员来创建和配置的，我们首先来创建一个 hostPath 类型的 PersistentVolume。Kubernetes 支持 hostPath 类型的 PersistentVolume 使用节点上的文件或目录来模拟附带网络的存储，但是需要注意的是在生产集群中，我们不会使用 hostPath，集群管理员会提供网络存储资源，比如 NFS 共享卷或 Ceph 存储卷，集群管理员还可以使用 StorageClasses 来设置动态提供存储。因为 Pod 并不是始终固定在某个节点上面的，所以要使用 hostPath 的话我们就需要将 Pod 固定在某个节点上，这样显然就大大降低了应用的容错性。\n当然了，生产环境中用的还是相对较少因为有较少的需求需要将Pod来固定到某些节点上。\n创建PersistentVolume 假设我们现在在节点1上新建一个/data/hostPath/index.html [root@Online-Beijing-node1 ~]# echo \u0026#34;Hello This is new hostPath message.\u0026#34; \u0026gt;\u0026gt; /data/hostPath/index.html 接下来创建一个Pv对象 apiVersion: v1 kind: PersistentVolume metadata: name: demo-hostpath labels: type: local spec: capacity: # 定义该Pv的容量为10Gb storage: 10Gi accessModes: # 定义该Pv的访问模式 - ReadWriteOnce hostPath: path: \u0026#34;/data/hostPath\u0026#34; storageClassName: type-ssd-sc Capacity（存储能力）：一般来说，一个 PV 对象都要指定一个存储能力，通过 PV 的 capacity 属性来设置的，目前只支持存储空间的设置，就是我们这里的 storage=10Gi，不过未来可能会加入 IOPS、吞吐量等指标的配置。 AccessModes（访问模式）：用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式： ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 创建完成后查看 PersistentVolume 的信息，输出结果显示该 PersistentVolume 的状态（STATUS） 为 Available。 这意味着它还没有被绑定给 PersistentVolumeClaim\n[root@Online-Beijing-master1 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE demo-hostpath 10Gi RWO Retain Available type-ssd-sc 13m 其中有一项 RECLAIM POLICY 的配置，同样我们可以通过 PV 的 persistentVolumeReclaimPolicy（回收策略）属性来进行配置，目前 PV 支持的策略有三种：\nRetain（保留）：回收策略 Retain 使得用户可以手动回收资源。当 PersistentVolumeClaim 对象被删除时，PersistentVolume 卷仍然存在，对应的数据卷被视为\u0026quot;已释放（released）\u0026quot;。 由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。 管理员可以通过下面的步骤来手动回收该卷： 删除 PersistentVolume 对象。与之相关的、位于外部基础设施中的存储资产 （例如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）在 PV 删除之后仍然存在。 根据情况，手动清除所关联的存储资产上的数据。 手动删除所关联的存储资产。 Recycle（回收）：回收策略 Recycle 已被废弃。取而代之的建议方案是使用动态制备。如果下层的卷插件支持，回收策略 Recycle 会在卷上执行一些基本的擦除 （rm -rf /thevolume/*）操作，之后允许该卷用于新的 PVC 申领。 Delete（删除）：对于支持 Delete 回收策略的卷插件，删除动作会将 PersistentVolume 对象从 Kubernetes 中移除，同时也会从外部基础设施（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）中移除所关联的存储资产。 目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。\n不过需要注意的是，目前只有 NFS 和 HostPath 两种类型支持回收策略，当然一般来说还是设置为 Retain 这种策略保险一点。\n关于 PV 的状态，实际上描述的是 PV 的生命周期的某个阶段，一个 PV 的生命周期中，可能会处于4种不同的阶段：\nAvailable（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PVC 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 创建PersistentVolumeClaim 如果我们需要使用这个 PV 的话，就需要创建一个对应的 PVC 来和他进行绑定了，就类似于我们的服务是通过 Pod 来运行的，而不是 Node，只是 Pod 跑在 Node 上而已。\n让我们申请一个使用3G空间的PersistentVolumeClaim\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi storageClassName: type-ssd-sc 创建 PersistentVolumeClaim 之后，Kubernetes 控制平面将查找满足申领要求的 PersistentVolume。 如果控制平面找到具有相同 StorageClass 的适当的 PersistentVolume， 则将 PersistentVolumeClaim 绑定到该 PersistentVolume 上。所以再次kubectl get pv的PersistentVolume状态应该属于Bound状态。\n[root@Online-Beijing-master1 yaml]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE demo-hostpath 10Gi RWO Retain Bound default/task-pv-claim type-ssd-sc 47m [root@Online-Beijing-master1 yaml]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound demo-hostpath 10Gi RWO type-ssd-sc 18m 可以看到已经绑定到了一个Volume叫做demo-hostpath的PersistentVolume\n需要注意的是目前PersistentVolume和PersistentVolumeClaim之间是一对一绑定的关系，也就是说一个PersistentVolume只能被一个PersistentVolumeClaim绑定。\n创建一个Deployment 创建一个deployment然后绑定PersistentVolumeClaim紧接着固定节点到online-beijing-node1\n--- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s.kuboard.cn/name: task-nginx-demo name: task-nginx-demo namespace: default spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s.kuboard.cn/name: task-nginx-demo strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s.kuboard.cn/name: task-nginx-demo nodeSelector: kubernetes.io/hostname: online-beijing-node1 spec: containers: - image: \u0026#39;nginx:latest\u0026#39; imagePullPolicy: Always name: task-nginx-demo ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: task-hostpath-volume dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: task-hostpath-volume persistentVolumeClaim: claimName: task-pv-claim 当这个deployment创建完成以后我们就可以通过访问service测试一下.\n正常情况下你可以看到Hello This is new hostPath message.这条信息\n[root@Online-Beijing-master1 yaml]# curl -v 10.10.56.102 * Rebuilt URL to: 10.10.56.102/ * Trying 10.10.56.102... * TCP_NODELAY set * Connected to 10.10.56.102 (10.10.56.102) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: 10.10.56.102 \u0026gt; User-Agent: curl/7.61.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.23.3 \u0026lt; Date: Wed, 22 Mar 2023 09:54:49 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 36 \u0026lt; Last-Modified: Wed, 22 Mar 2023 07:53:15 GMT \u0026lt; Connection: keep-alive \u0026lt; ETag: \u0026#34;641ab3eb-24\u0026#34; \u0026lt; Accept-Ranges: bytes Hello This is new hostPath message. 这个就是我们一个很简单的基于hostPath来持久化数据使用PersistentVolume和PersistentVolumeClaim简单教学。\nLocal PersistentVolume 上面我们创建了后端是 hostPath 类型的 PV 资源对象,那么个人认为hostPath的缺点在于\nPod不能进行随时随地的节点更换,如果更换则会出现丢失数据的现象。\n需要每次都搭配nodeSelector进行使用。\n其优点也是相对于比较明显\n因为hostPath使用的是本地磁盘,可以充分的利用磁盘的读写性能。 所以在 hostPath 的基础上，Kubernetes 依靠 PV、PVC 实现了一个新的特性，这个特性的名字叫作：Local Persistent Volume，也就是我们说的 Local PV。\nlocal 卷只能用作静态创建的持久卷。不支持动态配置。\n然而，local 卷仍然取决于底层节点的可用性，并不适合所有应用程序。 如果节点变得不健康，那么 local 卷也将变得不可被 Pod 访问。使用它的 Pod 将不能运行。 使用 local 卷的应用程序必须能够容忍这种可用性的降低，以及因底层磁盘的耐用性特征而带来的潜在的数据丢失风险。\n它与HostPath有何不同？ 为了更好地理解本地持久卷的优势，将其与HostPath 卷进行比较很有用。HostPath 卷将主机节点文件系统中的文件或目录挂载到 Pod 中。类似地，Local Persistent Volume 将本地磁盘或分区挂载到 Pod 中\n最大的区别是 Kubernetes 调度程序了解本地持久卷属于哪个节点。对于 HostPath 卷，引用 HostPath 卷的 pod 可能会被调度程序移动到不同的节点，从而导致数据丢失。但是对于 Local Persistent Volumes，Kubernetes 调度器确保使用 Local Persistent Volume 的 pod 总是被调度到同一个节点。\n虽然 HostPath 卷可以通过 Persistent Volume Claim (PVC) 引用或直接内嵌在 pod 定义中，但 Local Persistent Volumes 只能通过 PVC 引用。这提供了额外的安全优势，因为 Persistent Volume 对象由管理员管理，防止 Pod 能够访问主机上的任何路径。\n所以，一般来说 Local PV 对应的存储介质是一块额外挂载在宿主机的磁盘或者块设备。\n创建一个Local持久卷实例 下面是一个使用 local 卷和 nodeAffinity 的持久卷示例：\napiVersion: v1 kind: PersistentVolume metadata: name: example-local spec: capacity: storage: 20Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/ssd1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - online-beijing-node1 使用 local 卷时，你需要设置 PersistentVolume 对象的 nodeAffinity 字段。 Kubernetes 调度器使用 PersistentVolume 的 nodeAffinity 信息来将使用 local 卷的 Pod 调度到正确的节点。\n当然了,这也就意味着如果你的Pod想使用这个PV的话,那么就只能运行在online-beijing-node1这个节点上。这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。\n绑定PersistentVolumeClaim\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: bound-tasknginx spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local-storage 接下来创建一个Pod来绑定这个Pvc,然后可以通过访问Pod的IP地址进行验证。\napiVersion: v1 kind: Pod metadata: name: pv-local-pod spec: volumes: - name: example-pv-local persistentVolumeClaim: claimName: bound-tasknginx containers: - name: example-pv-local image: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /usr/share/nginx/html name: example-pv-local [root@Online-Beijing-master1 yaml]# curl -v 10.10.38.225 * Rebuilt URL to: 10.10.38.225/ * Trying 10.10.38.225... * TCP_NODELAY set * Connected to 10.10.38.225 (10.10.38.225) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: 10.10.38.225 \u0026gt; User-Agent: curl/7.61.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.23.3 \u0026lt; Date: Thu, 23 Mar 2023 08:45:18 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 25 \u0026lt; Last-Modified: Thu, 23 Mar 2023 08:43:41 GMT \u0026lt; Connection: keep-alive \u0026lt; ETag: \u0026#34;641c113d-19\u0026#34; \u0026lt; Accept-Ranges: bytes \u0026lt; Date: 2023-03-23 LocalPv * Connection #0 to host 10.10.38.225 left intact 当然了你也可以进入到Pod当中查看是否成功\n[root@Online-Beijing-master1 yaml]# kubectl exec -it pv-local-pod /bin/bash root@pv-local-pod:/usr/share/nginx/html# cd /usr/share/nginx/html/ root@pv-local-pod:/usr/share/nginx/html# cat index.html Date: 2023-03-23 LocalPv 删除静态管理的持久化存储 需要注意的是，我们上面手动创建PersistentVolume的方式，即静态的PersistentVolume管理方式，在删除PersistentVolume时需要按如下流程执行操作。\n删除使用这个PersistentVolume的 Pod 从宿主机移除本地磁盘 删除PersistentVolumeClaim 删除PersistentVolume ","date":"2023-03-22T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/localstorage/","title":"Kubernetes-本地存储"},{"content":" ","date":"2023-03-10T00:00:00Z","image":"https://img11.360buyimg.com/ddimg/jfs/t1/170852/25/36092/117643/640ad741Fe576192d/3f596893cbcf93a4.jpg","permalink":"http://localhost:1313/shoot/yiheyuan/","title":"摄影日记-颐和园"},{"content":"什么是Ingress Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。\nIngress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。\nIngress 公开从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。\n下面是一个将所有流量都发送到同一 Service 的简单 Ingress 示例：\nIngress 其实就是从 Kuberenets 集群外部访问集群的一个入口，将外部的请求转发到集群内不同的 Service 上，其实就相当于 nginx、haproxy 等负载均衡代理服务器，可能你会觉得我们直接使用 nginx 就实现了，但是只使用 nginx 这种方式有很大缺陷，每次有新服务加入的时候怎么改 Nginx 配置？不可能让我们去手动更改或者滚动更新前端的 Nginx Pod 吧？那我们再加上一个服务发现的工具比如 consul 如何？貌似是可以，对吧？Ingress 实际上就是这样实现的，只是服务发现的功能自己实现了，不需要使用第三方的服务了，然后再加上一个域名规则定义，路由信息的刷新依靠 Ingress Controller 来提供。\nIngress Controller 可以理解为一个监听器，通过不断地监听 kube-apiserver，实时的感知后端 Service、Pod 的变化，当得到这些信息变化后，Ingress Controller 再结合 Ingress 的配置，更新反向代理负载均衡器，达到服务发现的作用。其实这点和服务发现工具 consul、 consul-template 非常类似。\n现在可以供大家使用的 Ingress Controller 有很多，比如 traefik、nginx-controller、Kubernetes Ingress Controller for Kong、HAProxy Ingress controller，当然你也可以自己实现一个 Ingress Controller，现在普遍用得较多的是 traefik 和 nginx-controller，traefik 的性能较 nginx-controller 差，但是配置使用要简单许多，我们这里会重点给大家介绍 nginx-controller 以及 traefik 的使用。\n安装NGINX Ingress Controller 官方文档：NGINX Ingress Controller NGINX Ingress Controller 是使用 Kubernetes Ingress 资源对象构建的，用 ConfigMap 来存储 Nginx 配置的一种 Ingress Controller 实现。\n由于 nginx-ingress 所在的节点需要能够访问外网，这样域名可以解析到这些节点上直接使用，所以需要让 nginx-ingress 绑定节点的 80 和 443 端口，所以可以使用 hostPort 来进行访问。\n查看当前Ingress-Nginx适用的kubernetes版本\nIngress-NGINX version k8s supported version Alpine Version Nginx Version v1.6.4 1.26, 1.25, 1.24, 1.23 3.17.0 1.21.6 v1.5.1 1.25, 1.24, 1.23 3.16.2 1.21.6 v1.4.0 1.25, 1.24, 1.23, 1.22 3.16.2 1.19.10† v1.3.1 1.24, 1.23, 1.22, 1.21, 1.20 3.16.2 1.19.10† v1.3.0 1.24, 1.23, 1.22, 1.21, 1.20 3.16.0 1.19.10† v1.2.1 1.23, 1.22, 1.21, 1.20, 1.19 3.14.6 1.19.10† v1.1.3 1.23, 1.22, 1.21, 1.20, 1.19 3.14.4 1.19.10† v1.1.2 1.23, 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9† v1.1.1 1.23, 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9† v1.1.0 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9† v1.0.5 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9† v1.0.4 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9† v1.0.3 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9† v1.0.2 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9† v1.0.1 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9† v1.0.0 1.22, 1.21, 1.20, 1.19 3.13.5 1.20.1 使用Helm进行部署nginx-ingress-controller helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm fetch ingress-nginx/ingress-nginx tar -xvf ingress-nginx-4.5.2.tgz 新建一个value-test.yaml的配置文件 dnsPolicy：因为处于hostNetwork:true的状态下,Pod默认使用宿主机的DNS解析,这样会导致如果你使用ServiceName的方式来访问Pod的话会出现无法解析的情况。所以修改为ClusterFirstWithHostNet 请将webhook的镜像修改为registry.cn-beijing.aliyuncs.com/polymerization/kube-webhook-certgen:v20220916-gd32f8c343 controller: name: controller image: repository: registry.cn-beijing.aliyuncs.com/polymerization/nginx-controller tag: \u0026#34;v1.6.4\u0026#34; digest: sha256:e727015a639975f4fc0808b91f9e88a83c60938b640ee6c2f5606ddd779c858d dnsPolicy: ClusterFirstWithHostNet hostNetwork: true publishService: # hostNetwork 模式下设置为false，通过节点IP地址上报ingress status数据 enabled: false kind: DaemonSet tolerations: # 注意,如果你的Kubernetes集群中存在多个Taint需要全部进行容忍。 - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; nodeSelector: # 固定节点-\u0026gt;请给3台master全部打上这个标签-\u0026gt;个人建议将ingress-manager边缘化 node.kubernetes.io/ingress-manager: \u0026#39;true\u0026#39; service: # HostNetwork 模式不需要创建service enabled: false admissionWebhooks: enable: true patch: enable: true image: registry: registry.cn-beijing.aliyuncs.com image: polymerization/kube-webhook-certgen tag: v20220916-gd32f8c343 digest: sha256:c0e3bef270e179a5e4ab373f8ba6d57f596f3683d9d40c33ea900b19ec182ba2 pullPolicy: IfNotPresent defaultBackend: enabled: false 部署ingress-controller # 安装 helm install --namespace ingress-nginx ingress-nginx ./ingress-nginx -f value-test.yaml # 卸载 helm uninstall ingress-nginx --namespace ingress-nginx Ingress的基本使用 创建一个ingress资源对象 一个最小的 Ingress 资源示例\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: simple-qingyang-ingress namespace: out-apps spec: ingressClassName: nginx rules: - host: nginx.qingyang.com # 将域名映射到后端服务 http: paths: - path: / pathType: Prefix backend: service: name: os-qingyang port: number: 80 Ingress 需要指定 apiVersion、kind、 metadata和 spec 字段。 Ingress 对象的命名必须是合法的 DNS 子域名名称。 关于如何使用配置文件，请参见部署应用、 配置容器、 管理资源。 Ingress 经常使用注解（annotations）来配置一些选项，具体取决于 Ingress 控制器，例如重写目标注解。 不同的 Ingress 控制器支持不同的注解。 查看你所选的 Ingress 控制器的文档，以了解其支持哪些注解。\n如果 ingressClassName 被省略，那么你应该定义一个默认 Ingress 类,否则无法转发服务\n创建一个默认的ingressClass\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller name: default-nginx annotations: ingressclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; spec: controller: k8s.io/ingress-nginx 有一些 Ingress 控制器不需要定义默认的 IngressClass。比如：Ingress-NGINX 控制器可以通过参数 --watch-ingress-without-class 来配置。 不过仍然推荐创建默认的ingressClass\n可以看一下简单地ingress-controller请求流程\n客户端首先对 ngdemo.qikqiak.com 执行 DNS 解析，得到 Ingress Controller 所在节点的 IP 后客户端向 Ingress Controller 发送 HTTP 请求 根据 Ingress 对象里面的描述匹配域名，找到对应的 Service 对象，并获取关联的 Endpoints 列表，将客户端的请求转发给其中一个 Pod 创建Todo-app测试(暂时废弃) 首先部署MongoDB apiVersion: apps/v1 kind: Deployment metadata: name: mongo spec: selector: matchLabels: app: mongo template: metadata: labels: app: mongo spec: volumes: - name: data emptyDir: {} - name: resolv-conf configMap: name: cache-dns items: - key: resolv.conf path: resolv.conf containers: - name: mongo image: mongo ports: - containerPort: 27017 volumeMounts: - name: data mountPath: /data/db - name: resolv-conf mountPath: /etc/resolv.conf subPath: resolv.conf --- apiVersion: v1 kind: Service metadata: name: mongo spec: selector: app: mongo type: ClusterIP ports: - name: db port: 27017 targetPort: 27017 创建Todo apiVersion: apps/v1 kind: Deployment metadata: name: todo spec: selector: matchLabels: app: todo template: metadata: labels: app: todo spec: containers: - name: web image: cnych/todo:v1.1 env: - name: \u0026#34;DBHOST\u0026#34; value: \u0026#34;mongodb://mongo.default.svc.cluster.local:27017\u0026#34; ports: - containerPort: 3000 --- apiVersion: v1 kind: Service metadata: name: todo spec: selector: app: todo type: ClusterIP ports: - name: web port: 3000 targetPort: 3000 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: todo spec: ingressClassName: nginx rules: - host: nginx.qingyang.com # 将域名映射到后端服务 http: paths: - path: / pathType: Prefix backend: service: name: todo port: number: 3000 URL Rewrite Rewrite的Ingress注解\nnginx.ingress.kubernetes.io/rewrite-target Target URI where the traffic must be redirected string nginx.ingress.kubernetes.io/ssl-redirect Indicates if the location section is only accessible via SSL (defaults to True when Ingress contains a Certificate) bool nginx.ingress.kubernetes.io/force-ssl-redirect Forces the redirection to HTTPS even if the Ingress is not TLS Enabled bool nginx.ingress.kubernetes.io/app-root Defines the Application Root that the Controller must redirect if it’s in / context string nginx.ingress.kubernetes.io/use-regex Indicates if the paths defined on an Ingress use regular expressions bool 现在我们需要对访问的 URL 路径做一个 Rewrite，比如在 PATH 中添加一个 app 的前缀，关于 Rewrite 的操作在 ingress-nginx 官方文档中也给出对应的说明。\nnginx.ingress.kubernetes.io/rewrite-target: 流量必须重定向的目标URI(Target URI where the traffic must be redirected) apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: todo annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 spec: ingressClassName: nginx rules: - host: nginx.qingyang.com http: paths: - path: /something(/|$)(.*) # 匹配/something和/something/* pathType: Prefix backend: service: name: todo port: number: 3000 在此入口定义中，捕获的任何字符都(.*)将分配给占位符$2，然后将其用作注释中的参数rewrite-target。\n例如，上面的入口定义将导致以下重写：\nnginx.qingyang.com/something改写为nginx.qingyang.com/ nginx.qingyang.com/something/改写为nginx.qingyang.com/ nginx.qingyang.com/something/new改写为nginx.qingyang.com/new 使用此方法可能会导致部分css、js等内容无法找到,可以使用以下方法实现\n通过configuration-snippet注解 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: todo annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 nginx.ingress.kubernetes.io/configuration-snippet: | rewrite ^/css/(.*)$ /something/css/$1 redirect; # 为css样式添加/something前缀 rewrite ^/js/(.*)$ /something/js/$1 Basic Auth 们还可以在 Ingress Controller 上面配置一些基本的 Auth 认证，比如 Basic Auth，可以用 htpasswd 生成一个密码文件来验证身份验证。\n[root@Online-Beijing-master1 ~]# htpasswd -c auth admin 创建一个secret\n[root@Online-Beijing-master1 ~]# kubectl create secret generic basic-auth --from-file=authBasic Auth 的 Ingress 对象： 创建一个具有 Basic Auth 的 Ingress 对象\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: basic-auth-demo annotations: nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-realm: \u0026#39;Authentication Required - admin\u0026#39; spec: ingressClassName: nginx rules: - host: nginx.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: os-vue-comment port: number: 80 正常会弹出来认证窗口，进行认证就行。\n灰度应用 在日常工作中我们经常需要对服务进行版本更新升级，所以我们经常会使用到滚动升级、蓝绿发布、灰度发布等不同的发布操作。而 ingress-nginx 支持通过 Annotations 配置来实现不同场景下的灰度发布和测试，可以满足金丝雀发布、蓝绿部署与 A/B 测试等业务场景。\n在某些情况下，您可能希望通过向与生产服务不同的服务发送少量请求来金丝雀一组新的更改。Canary 注释使 Ingress 规范可以充当请求路由到的替代服务，具体取决于应用的规则。\ningress-nginx 的 Annotations 支持以下 4 种 Canary 规则：\nnginx.ingress.kubernetes.io/canary-by-header: 基于 Request Header 的流量切分，适用于灰度发布以及 A/B 测试。当 Request Header 设置为 always 时，请求将会被一直发送到 Canary 版本；当 Request Header 设置为 never时，请求不会被发送到 Canary 入口；对于任何其他 Header 值，将忽略 Header，并通过优先级将请求与其他金丝雀规则进行优先级的比较。 nginx.ingress.kubernetes.io/canary-by-header-value: 要匹配的 Request Header 的值，用于通知 Ingress 将请求路由到 Canary Ingress 中指定的服务。当 Request Header 设置为此值时，它将被路由到 Canary 入口。该规则允许用户自定义 Request Header 的值。此注释必须与 一起使用nginx.ingress.kubernetes.io/canary-by-header。如果未定义canary-by-header,那么该注解没有任何效果。 nginx.ingress.kubernetes.io/canary-weight: 基于服务权重的流量切分，适用于蓝绿部署，权重范围 0 - 100 按百分比将请求路由到 Canary Ingress 中指定的服务。权重为 0 意味着该金丝雀规则不会向 Canary 入口的服务发送任何请求，权重为 100 意味着所有请求都将被发送到 Canary 入口。 nginx.ingress.kubernetes.io/canary-by-cookie: 基于 cookie 的流量切分，适用于灰度发布与 A/B 测试。用于通知 Ingress 将请求路由到 Canary Ingress 中指定的服务的cookie。当 cookie 值设置为 always 时，它将被路由到 Canary 入口；当 cookie 值设置为 never 时，请求不会被发送到 Canary 入口；对于任何其他值，将忽略 cookie 并将请求与其他金丝雀规则进行优先级的比较。 Canary 规则按优先顺序进行评估。优先级如下：canary-by-header -\u0026gt; canary-by-cookie -\u0026gt; canary-weight\n把以上的四个 annotation 规则可以总体划分为以下两类：\n基于权重的的Canary规则 基于用户请求的Canary规则 灰度验证 首先我们先创建一个基于Producation版本的应用 apiVersion: apps/v1 kind: Deployment metadata: name: production-app labels: version: production spec: replicas: 1 selector: matchLabels: version: production template: metadata: labels: version: production spec: containers: - name: production-demo image: mirrorgooglecontainers/echoserver:1.10 ports: - containerPort: 8080 env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP --- apiVersion: v1 kind: Service metadata: name: production-service labels: version: production spec: ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: version: production 创建Production 版本的应用路由 (Ingress)。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: production-ingress spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: production-service port: number: 80 创建Canary版本的应用上线 apiVersion: apps/v1 kind: Deployment metadata: name: canary-demo labels: app: canary-app version: canary spec: replicas: 1 selector: matchLabels: app: canary-app template: metadata: labels: app: canary-app spec: containers: - name: canary-demo image: mirrorgooglecontainers/echoserver:1.10 ports: - containerPort: 8080 env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP --- apiVersion: v1 kind: Service metadata: name: canary-service labels: version: canary spec: ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: app: canary-app 基于权重的Canary规则 基于权重的流量切分的典型应用场景就是蓝绿部署，可通过将权重设置为 0 或 100 来实现。例如，可将 Green 版本设置为主要部分，并将 Blue 版本的入口配置为 Canary。最初，将权重设置为 0，因此不会将流量代理到 Blue 版本。一旦新版本测试和验证都成功后，即可将 Blue 版本的权重设置为 100，即所有流量从 Green 版本转向 Blue。\n以下 Ingress 示例的 Canary 版本使用了基于权重进行流量切分的 annotation 规则，将分配 30% 的流量请求发送至 Canary 版本。\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # 开启canary机制 nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # 切分30的流量到canary版本中 spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: canary-service port: number: 80 应用的 Canary 版本基于权重 (30%) 进行流量切分后，访问到 Canary 版本的概率接近 30%，流量比例可能会有小范围的浮动。\n基于 Request Header 基于 Request Header 进行流量切分的典型应用场景即灰度发布或 A/B 测试场景。参考以下截图，在 KubeSphere 给 Canary 版本的应用路由 (Ingress) 新增一条 annotation nginx.ingress.kubernetes.io/canary-by-header: canary(这里的 annotation 的 value 可以是任意值)，使当前的 Ingress 实现基于 Request Header 进行流量切分。\n金丝雀规则按优先顺序 canary-by-header - \u0026gt; canary-by-cookie - \u0026gt; canary-weight进行如下排序，因此以下情况将忽略原有 canary-weight 的规则。\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary-by-header: \u0026#34;canary-header\u0026#34; # 添加header nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # 开启canary机制 nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # 切分30的流量到canary版本中 spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: canary-service port: number: 80 我们尝试访问一下\n[root@Online-Beijing-master1 ~]# for i in $(seq 1 10); do curl -s prod.qingyang.com | grep \u0026#34;Hostname\u0026#34;; done Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: canary-demo-54dfb9bd-n2zlg Hostname: production-app-678488687f-s4sbd 尝试加入请求头访问\n如果你的canary-header的值为never则表示请求永远不会请求到当前版本,如果你的canary-header的值设置为always的话则表示永远请求当前版本 [root@Online-Beijing-master1 ~]# for i in $(seq 1 10); do curl -s -H \u0026#34;canary-header: never\u0026#34; prod.qingyang.com | grep \u0026#34;Hostname\u0026#34;; done Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd ------------------------------------------------ [root@Online-Beijing-master1 ~]# for i in $(seq 1 10); do curl -s -H \u0026#34;canary-header: always\u0026#34; prod.qingyang.com | grep \u0026#34;Hostname\u0026#34;; done Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg 如果你想让用户请求到指定的服务上可以添加ginx.ingress.kubernetes.io/canary-by-header-value: user-value,当请求访问携带canary-header: user-value的时候,那么该请求会被转发到canary版本。\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary-by-header-value: \u0026#34;user-value\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: \u0026#34;canary-header\u0026#34; # 添加header nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # 开启canary机制 nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # 切分30的流量到canary版本中 spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: canary-service port: number: 80 基于 Cookie的canary 与基于 Request Header 的 annotation 用法规则类似。例如在 A/B 测试场景下，需要让地域为北京的用户访问 Canary 版本。那么当 cookie 的 annotation 设置为 nginx.ingress.kubernetes.io/canary-by-cookie: \u0026quot;users_from_beijing\u0026quot;，此时后台可对登录的用户请求进行检查，如果该用户访问源来自北京则设置 cookie users_from_beijing的值为 always，这样就可以确保北京的用户仅访问 Canary 版本\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary-by-cookie: \u0026#34;user_from_beijing\u0026#34; # 添加cookie nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # 开启canary机制 nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # 切分30的流量到canary版本中 spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: canary-service port: number: 80 自签HTTPS 如果我们需要用 HTTPS 来访问我们这个应用的话，就需要监听 443 端口了，同样用 HTTPS 访问应用必然就需要证书，这里我们用 openssl 来创建一个自签名的证书：\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=prod.qingyang.com\u0026#34; 创建tls类型的secret\nkubectl create secret tls self-sign-nginx --cert=tls.crt --key=tls.key 创建带tls的ingress\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # 开启canary机制 nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # 切分30的流量到canary版本中 spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: nginx-demo port: number: 80 tls: - hosts: - prod.qingyang.com secretName: self-sign-nginx CertManager 自动 HTTPS cert-manager 将证书和证书颁发者作为资源类型添加到 Kubernetes 集群中，并简化了这些证书的获取、更新和使用过程。\n它可以从各种受支持的来源颁发证书，包括 Let’s Encrypt、HashiCorp Vault和Venafi以及私有 PKI。\n它将确保证书有效且最新，并尝试在到期前的配置时间更新证书。\n它大致基于 kube-lego的工作，并借鉴了其他类似项目（例如 kube-cert-manager）的一些智慧。\nIssuers: 代表的是证书颁发者，可以定义各种提供者的证书颁发者，当前支持基于 Let's Encrypt/HashiCorp/Vault 和 CA 的证书颁发者，还可以定义不同环境下的证书颁发者。 Certificates: 代表的是生成证书的请求. 部署cert-manager 好像这个quay.io能拉下来了…\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml 正常部署完成可以看到Pod正在运行\n[root@Online-Beijing-master1 ~]# kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-6499989f7-m6vdj 1/1 Running 0 2m30s cert-manager-cainjector-645b688547-xjcb8 1/1 Running 0 2m30s cert-manager-webhook-6b7f49999f-mcnf7 1/1 Running 0 2m30s 我们可以通过下面的测试来验证下是否可以签发基本的证书类型，创建一个 Issuer 资源对象来测试 webhook 工作是否正常(在开始签发证书之前，必须在群集中至少配置一个 Issuer 或 ClusterIssuer 资源)\napiVersion: v1 kind: Namespace metadata: name: cert-manager-test --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} # 配置自签名的证书机构类型 --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-cert namespace: cert-manager-test spec: dnsNames: - example.com secretName: selfsigned-cert-tls issuerRef: name: test-selfsigned 自动HTTPS Let's Encrypt 使用 ACME 协议来校验域名是否真的属于你，校验成功后就可以自动颁发免费证书，证书有效期只有 90 天，在到期前需要再校验一次来实现续期，而 cert-manager 是可以自动续期的，所以事实上并不用担心证书过期的问题。目前主要有 HTTP 和 DNS 两种校验方式。\nHTTP-01 校验 HTTP-01 的校验是通过给你域名指向的 HTTP 服务增加一个临时 location，在校验的时候 Let's Encrypt 会发送 http 请求到 http://\u0026lt;YOUR_DOMAIN\u0026gt;/.well-known/acme-challenge/\u0026lt;TOKEN\u0026gt;，其中 YOUR_DOMAIN 就是被校验的域名，TOKEN 是 cert-manager 生成的一个路径，它通过修改 Ingress 规则来增加这个临时校验路径并指向提供 TOKEN 的服务。Let's Encrypt 会对比 TOKEN 是否符合预期，校验成功后就会颁发证书了，不过这种方法不支持泛域名证书。\n使用 HTTP 校验这种方式，首先需要将域名解析配置好，也就是需要保证 ACME 服务端可以正常访问到你的 HTTP 服务。这里我们以上面的 TODO 应用为例，我们已经将 demo.qingyang.com 域名做好了正确的解析。\n由于 Let’s Encrypt 的生产环境有着严格的接口调用限制，所以一般我们需要先在 staging 环境测试通过后，再切换到生产环境。首先我们创建一个全局范围 staging 环境使用的 HTTP-01 校验方式的证书颁发机构：\napiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # ACME服务端地址 server: https://acme-staging-v02.api.letsencrypt.org/directory # 注册ACME的邮箱 email: ailunbolinkenasi@gmail.com # 用于存放ACME帐号privateKey的secret privateKeySecretRef: name: example-issuer-account-key solvers: - http01: # ACME的类型 ingress: class: nginx # 指定ingress的名称 接下来我们就可以生成免费证书了，cert-manager 给我们提供了 Certificate 这个用于生成证书的自定义资源对象，不过这个对象需要在一个具体的命名空间下使用，证书最终会在这个命名空间下以 Secret 的资源对象存储。我们这里是要结合 ingress-nginx 一起使用，实际上我们只需要修改 Ingress 对象，添加上 cert-manager 的相关注解即可，不需要手动创建 Certificate 对象了。\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: https-ingress annotations: cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-staging\u0026#34; # 使用哪个issuer spec: ingressClassName: nginx tls: - hosts: - demo.qingyang.com secretName: demo-qingyang-com-tls # 用于存储证书的Secret对象名字 rules: - host: demo.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: vue-demo port: number: 80 创建完成后会多出一个ingress对象,主要是为了让acme可以访问到当前的的token\n[root@Online-Beijing-master1 ~]# kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE cm-acme-http-solver-xpxm7 \u0026lt;none\u0026gt; demo.qingyang.com 10.1.6.24,10.1.6.45,10.1.6.48 80 7m51s https-ingress nginx demo.qingyang.com 10.1.6.24,10.1.6.45,10.1.6.48 80, 443 7m55s 可以查看当前的acme认证,其中/.well-known/acme-challenge/pVY-ihomZPdlWDMt44cV9qZUwMQScjHvd3Zkf_FDLRI是被验证对象\n[root@Online-Beijing-master1 ~]# kubectl describe ingress cm-acme-http-solver-xpxm7 Name: cm-acme-http-solver-xpxm7 Labels: acme.cert-manager.io/http-domain=1002178207 acme.cert-manager.io/http-token=1266355919 acme.cert-manager.io/http01-solver=true Namespace: default Address: 10.1.6.24,10.1.6.45,10.1.6.48 Ingress Class: \u0026lt;none\u0026gt; Default backend: \u0026lt;default\u0026gt; Rules: Host Path Backends ---- ---- -------- demo.qingyang.com /.well-known/acme-challenge/pVY-ihomZPdlWDMt44cV9qZUwMQScjHvd3Zkf_FDLRI cm-acme-http-solver-f4ntj:8089 (10.10.180.124:8089) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/whitelist-source-range: 0.0.0.0/0,::/0 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 7m46s (x2 over 8m13s) nginx-ingress-controller Scheduled for sync Normal Sync 7m46s (x2 over 8m13s) nginx-ingress-controller Scheduled for sync Normal Sync 7m46s (x2 over 8m13s) nginx-ingress-controller Scheduled for sync 你可以尝试访问https://demo.qingyang.com/.well-known/acme-challenge/pVY-ihomZPdlWDMt44cV9qZUwMQScjHvd3Zkf_FDLRI。正常会出现具体的验证密钥即成功.\n由于我的是本地自己搭建的kubernetes集群,没有外部解析的访问权限所以这个地方就没办法给大家演示了。\nDNS-01 校验 NS-01 的校验是通过 DNS 提供商的 API 拿到你的 DNS 控制权限， 在 Let's Encrypt 为 cert-manager 提供 TOKEN 后，cert-manager 将创建从该 TOKEN 和你的帐户密钥派生的 TXT 记录，并将该记录放在 _acme-challenge.\u0026lt;YOUR_DOMAIN\u0026gt;。然后 Let's Encrypt 将向 DNS 系统查询该记录，如果找到匹配项，就可以颁发证书，这种方法是支持泛域名证书的。\nDNS-01 支持多种不同的服务提供商，直接在 Issuer 或者 ClusterIssuer 中可以直接配置，对于一些不支持的 DNS 服务提供商可以使用外部 webhook 来提供支持，比如阿里云的 DNS 解析默认情况下是不支持的，我们可以使用阿里云这个 webhook 来提供支持。\nalidns-webhook 安装alidns-webhook kubectl apply -f https://raw.githubusercontent.com/pragkent/alidns-webhook/master/deploy/bundle.yaml 接着创建一个包含访问阿里云 DNS 认证密钥信息的 Secret 对象，对应的 accessk-key 和 secret-key kubectl create secret generic alidns-secret --from-literal=access-key=YOUR_ACCESS_KEY --from-literal=secret-key=YOUR_SECRET_KEY -n cert-manager 接下来同样首先创建一个 staging 环境的 DNS 类型的证书机构资源对象 apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns01 spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: beilanzhisen@163.com privateKeySecretRef: name: letsencrypt-staging-dns01 solvers: - dns01: # ACME DNS-01 类型 webhook: groupName: acme.yourcompany.com solverName: alidns config: region: \u0026#34;\u0026#34; accessKeySecretRef: # 引用 ak name: alidns-secret key: access-key secretKeySecretRef: # 引用 sk name: alidns-secret key: secret-key 接下来我们就可以使用上面的 ClusterIssuer 对象来或者证书数据了，创建如下所示的 Certificate 资源对象\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: qingyang-com-cert spec: secretName: qingyang-com-tls commonName: \u0026#34;*.qingyang.com\u0026#34; dnsNames: - qingyang.com - \u0026#34;*.qingyang.com\u0026#34; issuerRef: name: letsencrypt-staging-dns01 kind: ClusterIssuer 后我们就可以直接在 Ingress 资源对象中使用上面的 Secret 对象了\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: https-ingress annotations: cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-staging\u0026#34; # 使用哪个issuer spec: ingressClassName: nginx tls: - hosts: - \u0026#34;*.qingyang.com\u0026#34; secretName: qingyang-com-tls # 用于存储证书的Secret对象名字 rules: - host: demo.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: vue-demo port: number: 80 ","date":"2023-03-08T00:00:00Z","image":"https://d33wubrfki0l68.cloudfront.net/4f01eaec32889ff16ee255e97822b6d165b633f0/a54b4/zh-cn/docs/images/ingress.svg","permalink":"http://localhost:1313/kubernetes/ingress/","title":"Ingress的简单使用"},{"content":"如果在集群规模较大并发较高的情况下我们仍然需要对 DNS 进行优化，典型的就是大家比较熟悉的 CoreDNS 会出现超时5s的情况。\n超时原因 在 iptables 模式下（默认情况下），每个服务的 kube-proxy 在主机网络名称空间的 nat 表中创建一些 iptables 规则。 比如在集群中具有两个 DNS 服务器实例的 kube-dns 服务，其相关规则大致如下所示：\n(1) -A PREROUTING -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES \u0026lt;...\u0026gt; (2) -A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment \u0026#34;kube-system/kube-dns:dns cluster IP\u0026#34; -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU \u0026lt;...\u0026gt; (3) -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment \u0026#34;kube-system/kube-dns:dns\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-LLLB6FGXBLX6PZF7 (4) -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment \u0026#34;kube-system/kube-dns:dns\u0026#34; -j KUBE-SEP-LRVEW52VMYCOUSMZ \u0026lt;...\u0026gt; (5) -A KUBE-SEP-LLLB6FGXBLX6PZF7 -p udp -m comment --comment \u0026#34;kube-system/kube-dns:dns\u0026#34; -m udp -j DNAT --to-destination 10.32.0.6:53 \u0026lt;...\u0026gt; (6) -A KUBE-SEP-LRVEW52VMYCOUSMZ -p udp -m comment --comment \u0026#34;kube-system/kube-dns:dns\u0026#34; -m udp -j DNAT --to-destination 10.32.0.7:53 我们知道每个 Pod 的 /etc/resolv.conf 文件中都有填充的 nameserver 10.96.0.10 这个条目。所以来自 Pod 的 DNS 查找请求将发送到 10.96.0.10，这是 kube-dns 服务的 ClusterIP 地址。 由于 (1) 请求进入 KUBE-SERVICE 链，然后匹配规则 (2)，最后根据 (3) 的 random 随机模式，跳转到 (5) 或 (6) 条目，将请求 UDP 数据包的目标 IP 地址修改为 DNS 服务器的实际 IP 地址，这是通过 DNAT 完成的。其中 10.32.0.6 和 10.32.0.7 是我们集群中 CoreDNS 的两个 Pod 副本的 IP 地址。\n内核中的DNAT DNAT 的主要职责是同时更改传出数据包的目的地，响应数据包的源，并确保对所有后续数据包进行相同的修改。后者严重依赖于连接跟踪机制，也称为 conntrack，它被实现为内核模块。conntrack 会跟踪系统中正在进行的网络连接。\nconntrack 中的每个连接都由两个元组表示，一个元组用于原始请求（IP_CT_DIR_ORIGINAL），另一个元组用于答复（IP_CT_DIR_REPLY）。对于 UDP，每个元组都由源 IP 地址，源端口以及目标 IP 地址和目标端口组成，答复元组包含存储在src 字段中的目标的真实地址。\n例如，如果 IP 地址为 10.40.0.17 的 Pod 向 kube-dns 的 ClusterIP 发送一个请求，该请求被转换为 10.32.0.6，则将创建以下元组：\n原始：src = 10.40.0.17 dst = 10.96.0.10 sport = 53378 dport = 53 回复：src = 10.32.0.6 dst = 10.40.0.17 sport = 53 dport = 53378 通过这些条目内核可以相应地修改任何相关数据包的目的地和源地址，而无需再次遍历 DNAT 规则，此外，它将知道如何修改回复以及应将回复发送给谁。创建 conntrack 条目后，将首先对其进行确认，然后如果没有已确认的 conntrack 条目具有相同的原始元组或回复元组，则内核将尝试确认该条目。\n具体原因可以参考 weave works 总结的文章 Racy conntrack and DNS lookup timeouts\n只有多个线程或进程，并发从同一个 socket 发送相同五元组的 UDP 报文时，才有一定概率会发生 glibc、musl（alpine linux 的 libc 库）都使用 parallel query, 就是并发发出多个查询请求，因此很容易碰到这样的冲突，造成查询请求被丢弃 由于 ipvs 也使用了 conntrack, 使用 kube-proxy 的 ipvs 模式，并不能避免这个问题 解决方法 要彻底解决这个问题最好当然是内核上去 FIX 掉这个 BUG，除了这种方法之外我们还可以使用其他方法来进行规避，我们可以避免相同五元组 DNS请求的并发。\n在 resolv.conf 中就有两个相关的参数可以进行配置：\nsingle-request-reopen：发送 A 类型请求和 AAAA 类型请求使用不同的源端口，这样两个请求在 conntrack 表中不占用同一个表项，从而避免冲突。 single-request：避免并发，改为串行发送 A 类型和 AAAA 类型请求。没有了并发，从而也避免了冲突。 Pod 的 postStart hook 中添加 lifecycle: postStart: exec: command: - /bin/sh - -c - \u0026#34;/bin/echo \u0026#39;options single-request-reopen\u0026#39; \u0026gt;\u0026gt; /etc/resolv.conf\u0026#34; 使用 template.spec.dnsConfig 配置 template: spec: dnsConfig: options: - name: single-request-reopen\t使用 ConfigMap 覆盖 Pod 里面的 /etc/resolv.conf # configmap apiVersion: v1 data: resolv.conf: | nameserver 1.2.3.4 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 single-request-reopen timeout:1 kind: ConfigMap metadata: name: resolvconf --- # Pod Spec spec: volumeMounts: - name: resolv-conf mountPath: /etc/resolv.conf subPath: resolv.conf # 在某个目录下面挂载一个文件（保证不覆盖当前目录）需要使用subPath -\u0026gt; 不支持热更新 ... volumes: - name: resolv-conf configMap: name: resolvconf items: - key: resolv.conf path: resolv.conf NodeLocal DNSCache NodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 DNS 缓存代理来提高集群 DNS 性能。 在当今的体系结构中，运行在 ClusterFirst DNS 模式下的 Pod 可以连接到 kube-dns serviceIP 进行 DNS 查询。 通过 kube-proxy 添加的 iptables 规则将其转换为 kube-dns/CoreDNS 端点。 借助这种新架构，Pod 将可以访问在同一节点上运行的 DNS 缓存代理，从而避免 iptables DNAT 规则和连接跟踪。 本地缓存代理将查询 kube-dns 服务以获取集群主机名的缓存缺失（默认为 “cluster.local” 后缀）。\n动机 使用当前的 DNS 体系结构，如果没有本地 kube-dns/CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须延伸到另一个节点。 在这种场景下，拥有本地缓存将有助于改善延迟。 跳过 iptables DNAT 和连接跟踪将有助于减少 conntrack 竞争并避免 UDP DNS 条目填满 conntrack 表。 从本地缓存代理到 kube-dns 服务的连接可以升级为 TCP。 TCP conntrack 条目将在连接关闭时被删除，相反 UDP 条目必须超时 （默认 nf_conntrack_udp_timeout 是 30 秒）。 将 DNS 查询从 UDP 升级到 TCP 将减少由于被丢弃的 UDP 包和 DNS 超时而带来的尾部等待时间； 这类延时通常长达 30 秒（3 次重试 + 10 秒超时）。 由于 nodelocal 缓存监听 UDP DNS 查询，应用不需要变更。 在节点级别对 DNS 请求的度量和可见性。 可以重新启用负缓存，从而减少对 kube-dns 服务的查询数量。 工作原理如下\n此图显示了 NodeLocal DNSCache 如何处理 DNS 查询\n安装NodeLocalDNS 直接从官方的资源清单当中获取即可\nimage：默认镜像国内是下载不了的请更换地址 wget https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # 下载完成请更换image地址： registry.cn-beijing.aliyuncs.com/custom_img/k8s-dns-node-cache:1.22.18 注意资源清单中的几个变量信息\n__PILLAR__DNS__SERVER__：表示 kube-dns 这个 Service 的 ClusterIP。 __PILLAR__LOCAL__DNS__: 表示 DNSCache 本地的 IP，默认为 169.254.20.10 __PILLAR__DNS__DOMAIN__: 表示集群域，默认就是 cluster.local # 通过以下命令进行获取 kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP} # 修改部分变量信息 sed -i \u0026#39;s/__PILLAR__DNS__SERVER__/10.10.0.10/g s/__PILLAR__LOCAL__DNS__/169.254.20.10/g s/__PILLAR__DNS__DOMAIN__/cluster.local/g\u0026#39; nodelocaldns.yaml # 创建资源配置清单 kubectl apply -f nodelocaldns.yaml 如果 kube-proxy 运行在 IPVS 模式(因为我是ipvs的模式) sed -i \u0026#34;s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/$kubedns/g\u0026#34; nodelocaldns.yaml 在此模式下，node-local-dns Pod 只会侦听 \u0026lt;node-local-address\u0026gt; 的地址。 node-local-dns 接口不能绑定 kube-dns 的集群 IP 地址，因为 IPVS 负载均衡使用的接口已经占用了该地址。 node-local-dns Pod 会设置 __PILLAR__UPSTREAM__SERVERS__。\n查看Pod是否运行成功\n[root@Online-Beijing-master1 ~]# kubectl get pods -n kube-system | grep node-local-dns node-local-dns-578vf 1/1 Running 0 5m23s node-local-dns-5jhcl 1/1 Running 0 5m23s node-local-dns-8hz5j 1/1 Running 0 5m23s node-local-dns-ch44w 1/1 Running 0 5m23s node-local-dns-jbg2p 1/1 Running 0 5m23s node-local-dns-t92ww 1/1 Running 0 5m23s 如果 kube-proxy 组件使用的是 ipvs 模式的话我们还需要修改 kubelet 的 --cluster-dns 参数，将其指向 169.254.20.10，Daemonset 会在每个节点创建一个网卡来绑这个 IP，Pod 向本节点这个 IP 发 DNS 请求，缓存没有命中的时候才会再代理到上游集群 DNS 进行查询。\n如果担心线上环境修改 --cluster-dns 参数会产生影响，我们也可以直接在新部署的 Pod 中通过 dnsConfig 配置使用新的 localdns 的地址来进行解析。\n通过修改--cluster-dns实现 # 1. 首先查看当前的proxy模式 [root@Online-Beijing-master1 ~]# kubectl get cm kube-proxy -n kube-system -o yaml | grep mode mode: \u0026#34;ipvs\u0026#34; sed -i \u0026#39;s/10.10.0.10/169.254.20.10/g\u0026#39; /var/lib/kubelet/config.yaml systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet Pod中通过dnsConfig配置使用localdns dnsConfig: nameservers: - 169.254.20.10 searches: - default.svc.cluster.local - svc.cluster.local - cluster.local options: - name: ndots value: \u0026#39;3\u0026#39; dnsPolicy: None 由于指定nameservers属于append操作，如果需要忽略原来的dns地址请使用dnsPolicy: None\nCoreDns的性能优化 合理控制CoreDNS的副本数量 kubectl -n kube-system scale --replicas=10 deployment/coredns 为 coredns 定义 HPA 自动扩缩容。 安装 cluster-proportional-autoscaler 以实现更精确的扩缩容(推荐)。 禁用IPv6 如果 K8S 节点没有禁用 IPV6 的话，容器内进程请求 coredns 时的默认行为是同时发起 IPV4 和 IPV6 解析，而通常我们只需要用到 IPV4，当容器请求某个域名时，coredns 解析不到 IPV6 记录，就会 forward 到 upstream 去解析，如果到 upstream 需要经过较长时间(比如跨公网，跨机房专线)，就会拖慢整个解析流程的速度，业务层面就会感知 DNS 解析慢。\nkubectl edit cm coredns -n kube-system Corefile中添加禁用IPv6\napiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } # 添加此内容 template ANY AAAA { rcode NXDOMAIN } ... } 优化ndots 默认情况下，Kubernetes 集群中的域名解析往往需要经过多次请求才能解析到。查看 pod 内 的 /etc/resolv.conf 可以知道 ndots 选项默认为 5\nroot@nginxv1-56f77cbc67-4v4fp:/# cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local nameserver 10.10.0.10 options ndots:5 意思是: 如果域名中 . 的数量小于 5，就依次遍历 search 中的后缀并拼接上进行 DNS 查询。\n举个例子，在 debug 命名空间查询 kubernetes.default.svc.cluster.local 这个 service:\n域名中有4个.小于5尝试拼接上第一个 search 进行查询,也就是查询即kubernetes.default.svc.cluster.local.debug.svc.cluster.local查不到该域名。 继续尝试 kubernetes.default.svc.cluster.local.svc.cluster.local，查不到该域名。 继续尝试 kubernetes.default.svc.cluster.local.cluster.local，仍然查不到该域名。 尝试不加后缀，即 kubernetes.default.svc.cluster.local，查询成功，返回响应的 ClusterIP。 可以看到一个简单的 service 域名解析需要经过 4 轮解析才能成功，集群中充斥着大量无用的 DNS 请求。\n我们可以设置较小的 ndots，在 Pod 的 dnsConfig 中可以设置\nspec: containers: - name: nginxv1 image: nginx:latest resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: Always securityContext: privileged: false # 加入dnsConfig进行设置 dnsConfig: options: - name: ndots value: \u0026#34;2\u0026#34; 然后业务发请求时尽量将 service 域名拼完整，这样就不会经过 search 拼接造成大量多余的 DNS 请求。\n不过这样会比较麻烦，有没有更好的办法呢？有的！请看下面的 autopath 方式。\n启用autopath 启用 CoreDNS 的 autopath 插件可以避免每次域名解析经过多次请求才能解析到，原理是 CoreDNS 智能识别拼接过 search 的 DNS 解析，直接响应 CNAME 并附上相应的 ClusterIP，一步到位，可以极大减少集群内 DNS 请求数量。\nkubectl -n kube-system edit configmap coredns { \u0026#34;Corefile\u0026#34;: \u0026#34;.:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure # 修改为 pods verified fallthrough in-addr.arpa ip6.arpa ttl 30 } autopath @kubernetes # 添加autopath @kubernetes prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } template ANY AAAA { rcode NXDOMAIN } cache 30 loop reload loadbalance } } 需要注意的是，启用 autopath 后，由于 coredns 需要 watch 所有的 pod，会增加 coredns 的内存消耗，根据情况适当调节 coredns 的 memory request 和 limit。\n有兴趣的可以去看看这篇文章：详解DNS和CoreDNS ","date":"2023-02-26T00:00:00Z","image":"https://d33wubrfki0l68.cloudfront.net/bf8e5eaac697bac89c5b36a0edb8855c860bfb45/6944f/images/docs/nodelocaldns.svg","permalink":"http://localhost:1313/kubernetes/nodelocaldns/","title":"CacheDNS和DNS缓存"},{"content":"使用Kubeadm创建一个高可用的Etcd集群 默认情况下，kubeadm 在每个控制平面节点上运行一个本地 etcd 实例。也可以使用外部的 etcd 集群，并在不同的主机上提供 etcd 实例。 这两种方法的区别在 高可用拓扑的选项 页面中阐述。\n这个任务将指导你创建一个由三个成员组成的高可用外部 etcd 集群，该集群在创建过程中可被 kubeadm 使用。\n准备开始 三个可以通过 2379 和 2380 端口相互通信的主机。本文档使用这些作为默认端口。不过，它们可以通过 kubeadm 的配置文件进行自定义。 每个主机必须安装 systemd 和 bash 兼容的 shell。 每台主机必须安装有容器运行时、kubelet 和 kubeadm 每个主机都应该能够访问 Kubernetes 容器镜像仓库 (registry.k8s.io)， 或者使用 kubeadm config images list/pull 列出/拉取所需的 etcd 镜像。 本指南将把 etcd 实例设置为由 kubelet 管理的静态 Pod。 一些可以用来在主机间复制文件的基础设施。例如 ssh 和 scp 就可以满足需求。 本次容器运行时采用Containerd作为Runtime\n将Kubelet配置为Etcd的服务启动管理器 你必须在要运行 etcd 的所有主机上执行此操作。\ncat \u0026lt;\u0026lt; EOF \u0026gt; /usr/lib/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock Restart=always EOF 启动kubelet\nsystemctl daemon-reload systemctl restart kubelet 注意: 请执行完毕后务必确保kubelet处于running状态。\n为Kubeadm创建配置文件 # 使用你的主机 IP 替换 HOST0、HOST1 和 HOST2 的 IP 地址 export HOST0=10.1.6.48 export HOST1=10.1.6.24 export HOST2=10.1.6.45 # 使用你的主机名更新 NAME0、NAME1 和 NAME2 export NAME0=\u0026#34;containerd-master1\u0026#34; export NAME1=\u0026#34;containerd-master2\u0026#34; export NAME2=\u0026#34;containerd-master3\u0026#34; # 创建临时目录来存储将被分发到其它主机上的文件 mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ HOSTS=(${HOST0} ${HOST1} ${HOST2}) NAMES=(${NAME0} ${NAME1} ${NAME2}) for i in \u0026#34;${!HOSTS[@]}\u0026#34;; do HOST=${HOSTS[$i]} NAME=${NAMES[$i]} cat \u0026lt;\u0026lt; EOF \u0026gt; /tmp/${HOST}/kubeadmcfg.yaml --- apiVersion: \u0026#34;kubeadm.k8s.io/v1beta3\u0026#34; kind: InitConfiguration nodeRegistration: name: ${NAME} localAPIEndpoint: advertiseAddress: ${HOST} --- apiVersion: \u0026#34;kubeadm.k8s.io/v1beta3\u0026#34; kind: ClusterConfiguration etcd: local: dataDir: /var/lib/etcds serverCertSANs: - \u0026#34;${HOST}\u0026#34; peerCertSANs: - \u0026#34;${HOST}\u0026#34; extraArgs: initial-cluster: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 imageRepository: registry.aliyuncs.com/google_containers EOF done 生成证书颁发机构 如果你还没有 CA，则在 $HOST0（你为 kubeadm 生成配置文件的位置）上运行此命令。\nkubeadm init phase certs etcd-ca 这一操作将会生成 /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key 为每个成员创建证书 kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST2}/ # 清理不可重复使用的证书 find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST1}/ find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete # HOST0不需要进行移动 kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml 复制证书和 kubeadm 配置 scp -r /tmp/${HOST1}/* root@${HOST1}: scp -r /tmp/${HOST2}/* root@${HOST2}: chown -R root:root pki/ mv pki /etc/kubernetes/ 请检查证书文件是否都存在 检查$HOST0\n[root@containerd-master1 ~]# tree /etc/kubernetes/pki/ /etc/kubernetes/pki/ ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── ca.key ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key 1 directory, 10 files 检查$HOST1\n[root@containerd-master2 ~]# tree /etc/kubernetes/pki/ /etc/kubernetes/pki/ ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key 1 directory, 9 files 检查$HOST2\n[root@containerd-master3 ~]# tree /etc/kubernetes/pki/ /etc/kubernetes/pki/ ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key 1 directory, 9 files 创建Etcd的Pod清单 请在$HOST0进行执行\nkubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml 请在$HOST1进行执行\nkubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml 请在$HOST2进行执行\nkubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml 检查Etcd的Pod是否运行 三台Etcd主机全部使用crictl ps -a 进行查看EtcdPod是否处于running状态 [root@containerd-master1 ~]# crictl ps -a CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID POD 0a183925d2542 0048118155842 52 seconds ago Running ","date":"2023-02-26T00:00:00Z","image":"https://img.linux.net.cn/data/attachment/album/201501/29/141718izklanww82qm888k.png","permalink":"http://localhost:1313/kubernetes/InstallEtcdHA/","title":"使用Kubeadm创建一个高可用的ETCD集群"},{"content":"ConfigMap ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。\nConfigMap 将你的环境配置信息和 容器镜像 解耦，便于应用配置的修改。 ConfigMap 在设计上不是用来保存大量数据的。在 ConfigMap 中保存的数据不可超过1MiB(这其实是ETCD的要求哈哈哈)。如果你需要保存超出此尺寸限制的数据，你可能希望考虑挂载存储卷 或者使用独立的数据库或者文件服务。\n这是一个 ConfigMap 的示例，它的一些键只有一个值，其他键的值看起来像是 配置的片段格式。\n通过Key和Value这种键值对来进行写入数据 apiVersion: v1 kind: ConfigMap metadata: name: game-demo data: # 类属性键；每一个键都映射到一个简单的值 player_initial_lives: \u0026#34;3\u0026#34; ui_properties_file_name: \u0026#34;user-interface.properties\u0026#34; # 类文件键,一般用来保存一个文件到指定目录 game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true 你可以使用四种方式来使用 ConfigMap 配置 Pod 中的容器：\n在容器命令和参数内 容器的环境变量 在只读卷里面添加一个文件，让应用来读取 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap 通过环境变量的方式使用ConfigMap 首先我们创建一个Deployment然后通过Env环境变量的方式进行使用ConfigMap\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-web-beijing namespace: default labels: k8s-app: nginx-web zone: beijing spec: replicas: 1 selector: matchLabels: k8s-app: nginx-web template: metadata: name: nginx-web-beijing labels: k8s-app: nginx-web spec: containers: - name: vue-shop-beijing image: nginx:latest resources: requests: memory: 100Mi cpu: 10m # 通过环境变量的方式进行挂载 env: - name: PLAYER_INITIAL_LIVES valueFrom: configMapKeyRef: name: game-demo # 表示当前键来自game-demo这个ConfigMap key: player_initial_lives # 表示取player_initial_lives这个键的内容 然后我们可以进入到Pod内部进行echo挂载的变量名查看是否有输出\nroot@nginx-web-beijing-d6d994854-d6tjk:/# echo $PLAYER_INITIAL_LIVES 3 将ConfigMap当做文件使用 创建一个 ConfigMap 对象或者使用现有的 ConfigMap 对象。多个 Pod 可以引用同一个 ConfigMap。 修改 Pod 定义，在 spec.volumes[] 下添加一个卷。 为该卷设置任意名称，之后将 spec.volumes[].configMap.name 字段设置为对你的 ConfigMap 对象的引用。 为每个需要该 ConfigMap 的容器添加一个 .spec.containers[].volumeMounts[]。 设置 .spec.containers[].volumeMounts[].readOnly=true 并将 .spec.containers[].volumeMounts[].mountPath 设置为一个未使用的目录名， ConfigMap 的内容将出现在该目录中。 更改你的镜像或者命令行，以便程序能够从该目录中查找文件。ConfigMap 中的每个 data 键会变成 mountPath 下面的一个文件名。 创建一个挂载ConfigMap的Deployment\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-web-beijing namespace: default labels: k8s-app: nginx-web zone: beijing spec: replicas: 1 selector: matchLabels: k8s-app: nginx-web template: metadata: name: nginx-web-beijing labels: k8s-app: nginx-web spec: containers: - name: vue-shop-beijing image: nginx:latest resources: requests: memory: 100Mi cpu: 10m volumeMounts: - name: vue-config mountPath: \u0026#34;/etc/vue-config\u0026#34; readOnly: true volumes: - name: vue-config configMap: name: game-demo 进入容器中查看/etc/vue-config目录下是否有配置文件\nroot@nginx-web-beijing-5649b8f646-pzmm4:/etc/vue-config# ls service-interface.properties root@nginx-web-beijing-5649b8f646-pzmm4:/etc/vue-config# cat service-interface.properties port: 4000 如果 Pod 中有多个容器，则每个容器都需要自己的 volumeMounts 块，但针对每个 ConfigMap，你只需要设置一个 spec.volumes 块。\n被挂载的ConfigMap内容会被自动更新 当卷中使用的 ConfigMap 被更新时，所投射的键最终也会被更新。 kubelet 组件会在每次周期性同步时检查所挂载的 ConfigMap 是否为最新。 不过，kubelet 使用的是其本地的高速缓存来获得 ConfigMap 的当前值。 高速缓存的类型可以通过 KubeletConfiguration 结构. 的 ConfigMapAndSecretChangeDetectionStrategy 字段来配置。\nConfigMap 既可以通过 watch 操作实现内容传播（默认形式），也可实现基于 TTL 的缓存，还可以直接经过所有请求重定向到 API 服务器。 因此，从 ConfigMap 被更新的那一刻算起，到新的主键被投射到 Pod 中去， 这一时间跨度可能与 kubelet 的同步周期加上高速缓存的传播延迟相等。 这里的传播延迟取决于所选的高速缓存类型 （分别对应 watch 操作的传播延迟、高速缓存的 TTL 时长或者 0）。\n以环境变量方式使用的 ConfigMap 数据不会被自动更新。 更新这些数据需要重新启动 Pod。\nSecret Secret 是一种包含少量敏感信息例如密码、令牌或密钥的对象。 这样的信息可能会被放在 Pod 规约中或者镜像中。 使用 Secret 意味着你不需要在应用程序代码中包含机密数据。\n由于创建 Secret 可以独立于使用它们的 Pod， 因此在创建、查看和编辑 Pod 的工作流程中暴露 Secret（及其数据）的风险较小。 Kubernetes 和在集群中运行的应用程序也可以对 Secret 采取额外的预防措施， 例如避免将机密数据写入非易失性存储。\nSecret 类似于 ConfigMap 但专门用于保存机密数据。\n注意: 默认情况下，Kubernetes Secret 未加密地存储在 API 服务器的底层数据存储（etcd）中。 任何拥有 API 访问权限的人都可以检索或修改 Secret，任何有权访问 etcd 的人也可以。 此外，任何有权限在命名空间中创建 Pod 的人都可以使用该访问权限读取该命名空间中的任何 Secret； 这包括间接访问，例如创建 Deployment 的能力。\n为了安全地使用 Secret，请至少执行以下步骤：\n为 Secret 启用静态加密。 以最小特权访问 Secret 并启用或配置 RBAC 规则。 限制 Secret 对特定容器的访问。 考虑使用外部 Secret 存储驱动。 Secret的使用 Pod 可以用三种方式之一来使用 Secret：\n作为挂载到一个或多个容器上的卷 中的文件。 作为容器的环境变量。 由 kubelet 在为 Pod 拉取镜像时使用。 Kubernetes控制面也使用 Secret； 例如，引导令牌 Secret 是一种帮助自动化节点注册的机制。\nSecret 主要使用的有以下三种类型：\nOpaque：base64 编码格式的 Secret，用来存储密码、密钥等；但数据也可以通过base64 –decode解码得到原始数据，所有加密性很弱。 kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。 kubernetes.io/service-account-token：用于 ServiceAccount, ServiceAccount 创建时 Kubernetes 会默认创建一个对应的 Secret 对象，Pod 如果使用了 ServiceAccount，对应的 Secret 会自动挂载到 Pod 目录 /run/secrets/kubernetes.io/serviceaccount 中。 bootstrap.kubernetes.io/token：用于节点接入集群的校验的 Secret Opaque Secret的使用 Opaque 类型的数据是一个 map 类型，要求 value 必须是 base64 编码格式，比如我们来创建一个用户名为 admin，密码为 admin321 的 Secret 对象，首先我们需要先把用户名和密码做 base64 编码：\n[root@Online-Beijing-master1 ~]# echo -n \u0026#34;admin321\u0026#34; | base64 YWRtaW4zMjE= 然后我们就可以利用上面编码过后的数据来编写一个 YAML 文件：(opaque-demo.yaml)\napiVersion: v1 kind: Secret metadata: name: base-user-info type: Opaque data: username: YWRtaW4= password: YWRtaW4zMjE= 创建好 Secret对象后，有两种方式来使用它：\n以环境变量的形式 以Volume的形式挂载 通过环境变量挂载Secret apiVersion: apps/v1 kind: Deployment metadata: name: nginx-web-beijing namespace: default labels: k8s-app: nginx-web zone: beijing spec: replicas: 1 selector: matchLabels: k8s-app: nginx-web template: metadata: name: nginx-web-beijing labels: k8s-app: nginx-web spec: containers: - name: vue-shop-beijing image: nginx:latest resources: requests: memory: 100Mi cpu: 10m env: - name: USERNAME valueFrom: secretKeyRef: name: base-user-info key: username - name: PASSWORD valueFrom: secretKeyRef: name: base-user-info key: password 通过Volume挂载 Secret 把两个 key 挂载成了两个对应的文件。当然如果想要挂载到指定的文件上面，是不是也可以使用上一节课的方法：在 secretName 下面添加 items 指定 key 和 path\napiVersion: v1 kind: Pod metadata: name: secret2-pod spec: containers: - name: secret2 image: busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ls /etc/secrets\u0026#34;] volumeMounts: - name: secrets mountPath: /etc/secrets volumes: - name: secrets secret: secretName: base-user-info 一般来说Pod默认的访问API Server的Token都会挂载到/var/run/secrets/kubernetes.io/serviceaccount当中,利用自带的token和ca.crt，默认情况下所有的Pod都会被注入当前namespace下的token和ca.crt这样以来就可以去访问APIServer了\nroot@nginx-web-beijing-87c9f478f-knrfp:/var/run/secrets/kubernetes.io/serviceaccount# ls ca.crt namespace token kubernetes.io/dockerconfigjson 除了上面的 Opaque 这种类型外，我们还可以来创建用户 docker registry 认证的 Secret，直接使用kubectl create 命令创建即可，如下：\nkubectl create secret docker-registry beijing-harbor --docker-server=DOCKER_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL kubectl create secret docker-registry beijing-harbor --docker-server=127.0.0.1 --docker-username=admin --docker-password=123123 --docker-email=beilanzhisen@163.com 除了上面这种方法之外，我们也可以通过指定文件的方式来创建镜像仓库认证信息，需要注意对应的 KEY 和 TYPE：\nkubectl create secret generic beijing-harbor --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson 如果我们需要拉取私有仓库中的 Docker 镜像的话就需要使用到上面的 myregistry 这个 Secret：\npiVersion: apps/v1 kind: Deployment metadata: name: foo spec: containers: - name: foo image: 192.168.1.100:5000/test:v1 imagePullSecrets: - name: beijing-harbor ImagePullSecrets 与 Secrets 不同，因为 Secrets 可以挂载到 Pod 中，但是 ImagePullSecrets 只能由 Kubelet 访问。\nServiceAccount ServiceAccount 主要是用于解决 Pod 在集群中的身份认证问题的。认证使用的授权信息其实就是利用前面我们讲到的一个类型为 kubernetes.io/service-account-token 进行管理的。\nServiceAccount 是命名空间级别的，每一个命名空间创建的时候就会自动创建一个名为 default 的 ServiceAccount 对象:\nkubectl create ns kube-test kubectl get secret -n kube-test NAME TYPE DATA AGE default-token-vn4tr kubernetes.io/service-account-token 3 2m27s 实现原理 apiVersion: v1 data: ca.crt: LS0tLS... namespace: a3ViZS10ZXN0 token: ZXlKaG... kind: Secret metadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 75b3314b-e949-4f7b-9450-9bcd89c8c972 creationTimestamp: \u0026#34;2019-11-23T04:19:47Z\u0026#34; name: default-token-vn4tr namespace: kube-test resourceVersion: \u0026#34;4297521\u0026#34; selfLink: /api/v1/namespaces/kube-test/secrets/default-token-vn4tr uid: e3e60f95-f255-471b-a6c0-600a3c0ee53a type: kubernetes.io/service-account-token 在 data 区域我们可以看到有3个信息：\nca.crt：用于校验服务端的证书信息 namespace：表示当前管理的命名空间 token：用于 Pod 身份认证的 Token 前面我们也提到了默认情况下当前 namespace 下面的 Pod 会默认使用 default 这个 ServiceAccount，对应的 Secret 会自动挂载到 Pod 的 /var/run/secrets/kubernetes.io/serviceaccount/ 目录中，这样我们就可以在 Pod 里面获取到用于身份认证的信息了。\n实际上这个自动挂载过程是在 Pod 创建的时候通过 Admisson Controller（准入控制器） 来实现的，关于准入控制器的详细信息我们会在后面的安全章节中和大家继续学习。\nAdmission Controller（准入控制）是 Kubernetes API Server 用于拦截请求的一种手段。Admission 可以做到对请求的资源对象进行校验，修改，Pod 创建时 Admission Controller 会根据指定的的 ServiceAccount（默认的 default）把对应的 Secret 挂载到容器中的固定目录下 /var/run/secrets/kubernetes.io/serviceaccount/。\n","date":"2023-02-14T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/configmaporservice/","title":"ConfigMap和Secret的使用"},{"content":"HorizontalPodAutoscaler HPA官方文档 在Kubernetes 中HorizontalPodAutoscaler自动更新工作负载资源 （例如 Deployment 或者 StatefulSet）， 目的是自动扩缩工作负载以满足需求。\n水平扩缩意味着对增加的负载的响应是部署更多的 Pod。 这与垂直(Vertical)扩缩不同，对于 Kubernetes， 垂直扩缩意味着将更多资源（例如：内存或 CPU）分配给已经为工作负载运行的 Pod。\n如果负载减少，并且Pod的数量高于配置的最小值，HorizontalPodAutoscaler 会指示工作负载资源（Deployment、StatefulSet 或其他类似资源）缩减。\n水平Pod自动扩缩不适用于无法扩缩的对象: 例如DemonSet这种\n我们可以简单的通过 kubectl autoscale 命令来创建一个 HPA 资源对象，HPA Controller默认30s轮询一次（可通过 kube-controller-manager 的--horizontal-pod-autoscaler-sync-period 参数进行设置），查询指定的资源中的 Pod 资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。\nHorizontalPodAutoscaler 是如何工作的 Kubernetes 将水平 Pod 自动扩缩实现为一个间歇运行的控制回路（它不是一个连续的过程）。间隔由 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 参数设置（默认间隔为 15 秒）。\n在每个时间段内，控制器管理器都会根据每个 HorizontalPodAutoscaler 定义中指定的指标查询资源利用率。 控制器管理器找到由 scaleTargetRef 定义的目标资源，然后根据目标资源的 .spec.selector 标签选择 Pod， 并从资源指标 API（针对每个 Pod 的资源指标）或自定义指标获取指标 API（适用于所有其他指标）\n对于按 Pod 统计的资源指标（如 CPU），控制器从资源指标 API 中获取每一个 HorizontalPodAutoscaler 指定的 Pod 的度量值，如果设置了目标使用率，控制器获取每个 Pod 中的容器资源使用情况， 并计算资源使用率。如果设置了 target 值，将直接使用原始数据（不再计算百分比）。 接下来，控制器根据平均的资源使用率或原始值计算出扩缩的比例，进而计算出目标副本数。 如果 Pod 使用自定义指示，控制器机制与资源指标类似，区别在于自定义指标只使用原始值，而不是使用率。 如果 Pod 使用对象指标和外部指标（每个指标描述一个对象信息）。 这个指标将直接根据目标设定值相比较，并生成一个上面提到的扩缩比例。 在 autoscaling/v2 版本 API 中，这个指标也可以根据 Pod 数量平分后再计算。 HorizontalPodAutoscaler的常见用途是将其配置为从聚合 API （metrics.k8s.io、custom.metrics.k8s.io 或 external.metrics.k8s.io）获取指标。 metrics.k8s.io API 通常由名为Metrics Server的插件提供，需要单独启动。有关资源指标的更多信息， 请参阅 Metrics Server。\nMetrics-Server 在 HPA 的第一个版本中，我们需要 Heapster 提供 CPU 和内存指标，在 HPA v2 过后就需要安装 Metrcis Server 了，Metrics Server 可以通过标准的 Kubernetes API 把监控数据暴露出来，有了 Metrics Server 之后，我们就完全可以通过标准的 Kubernetes API 来访问我们想要获取的监控数据了：\nhttps://api.k8s.io:8443/metrics.k8s.io/v1beta1/namespaces/\u0026lt;namespace-name\u0026gt;/pods/\u0026lt;pod-name\u0026gt; 比如当我们访问上面的 API 的时候，我们就可以获取到该 Pod 的资源数据，这些数据其实是来自于 kubelet 的 Summary API 采集而来的。不过需要说明的是我们这里可以通过标准的 API 来获取资源监控数据，并不是因为 Metrics Server 就是 APIServer 的一部分，而是通过 Kubernetes 提供的 Aggregator 汇聚插件来实现的，是独立于 APIServer 之外运行的。\n聚合 API Aggregator 允许开发人员编写一个自己的服务，把这个服务注册到 Kubernetes 的 APIServer 里面去，这样我们就可以像原生的 APIServer 提供的 API 使用自己的 API 了，我们把自己的服务运行在 Kubernetes 集群里面，然后 Kubernetes 的 Aggregator 通过 Service 名称就可以转发到我们自己写的 Service 里面去了。这样这个聚合层就带来了很多好处：\n增加了 API 的扩展性，开发人员可以编写自己的 API 服务来暴露他们想要的 API。 丰富了 API，核心 kubernetes 团队阻止了很多新的 API 提案，通过允许开发人员将他们的 API 作为单独的服务公开，这样就无须社区繁杂的审查了。 开发分阶段实验性 API，新的 API 可以在单独的聚合服务中开发，当它稳定之后，在合并会 APIServer 就很容易了。 确保新 API 遵循 Kubernetes 约定，如果没有这里提出的机制，社区成员可能会被迫推出自己的东西，这样很可能造成社区成员和社区约定不一致。 部署HPA 我们要使用 HPA，就需要在集群中安装 Metrics Server 服务，要安装 Metrics Server 就需要开启 Aggregator，因为 Metrics Server 就是通过该代理进行扩展的，不过我们集群是通过 Kubeadm 搭建的，默认已经开启了，如果是二进制方式安装的集群，需要单独配置 kube-apsierver 添加如下所示的参数：\n--requestheader-client-ca-file=\u0026lt;path to aggregator CA cert\u0026gt; --requestheader-allowed-names=aggregator --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --proxy-client-cert-file=\u0026lt;path to aggregator proxy cert\u0026gt; --proxy-client-key-file=\u0026lt;path to aggregator proxy key\u0026gt; Aggregator 聚合层启动完成后，就可以来安装 Metrics Server 了，我们可以获取该仓库的官方安装资源清单：\n官方仓库地址：https://github.com/kubernetes-sigs/metrics-server # 请修改镜像为: registry.aliyuncs.com/google_containers/metrics-server:v0.6.2 wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.2/components.yaml 如果出现x509: cannot validate certificate for 10.151.30.22 because it doesn’t contain any IP SANs这种错误,因为部署集群的时候，CA 证书并没有把各个节点的 IP 签上去，所以这里 Metrics Server 通过 IP 去请求时，提示签的证书没有对应的IP所导致的,我们可以添加一个--kubelet-insecure-tls参数跳过证书校验：\n- args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls # 修改完成后记得部署 kubectl apply -f components.yaml 验证HPA是否安装成功,现在我们可以通过 kubectl top 命令来获取到资源数据了，证明 Metrics Server 已经安装成功了。\n[root@Online-Beijing-master1 ~]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% online-beijing-master1 82m 1% 1970Mi 12% online-beijing-master2 59m 0% 1379Mi 8% online-beijing-master3 61m 0% 1389Mi 8% online-beijing-node1 35m 0% 1957Mi 12% online-beijing-node2 33m 0% 1875Mi 11% online-beijing-node3 35m 0% 1045Mi 6% 首先我们先创建一个deployment，准备对他进行HPA\napiVersion: apps/v1 kind: Deployment metadata: name: hpa-demo-nginx namespace: default labels: k8s-app: hpa-demo-nginx spec: replicas: 1 selector: matchLabels: k8s-app: hpa-demo-nginx template: metadata: name: hpa-demo-nginx labels: k8s-app: hpa-demo-nginx spec: containers: - name: hpa-demo-nginx image: nginx:latest resources: requests: cpu: 10m memory: 100Mi securityContext: privileged: false 创建基于CPU的自动扩容 我们这次只针对CPU进行操作,后续我们会根据更多的自定义资源来进行扩缩容。\n现在我们来创建一个HPA，可以使用kubectl autoscale命令来创建：\nkubectl autoscale deployment hpa-demo-nginx --cpu-percent=10 --min=1 --max=6 此命令创建了一个关联资源hpa-demo-nginx 的HPA，最小的 pod 副本数为3，最大为6。HPA会根据设定的 cpu使用率（10%）动态的增加或者减少pod数量。\n[root@Online-Beijing-master1 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo-nginx Deployment/hpa-demo-nginx \u0026lt;unknown\u0026gt;/10% 1 6 0 8s 接下来对Pod进行压力测试,不断的去请求当前hpa-demo-nginxPod的IP\nkubectl run -i --tty load-generator --image=busybox /bin/sh while true; do wget -q -O- http://10.10.180.71; done 正常可以看到HPA已经正常工作了，Pod的副本数量已经分配到了我们当时指定的6个\n[root@Online-Beijing-master1 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo-nginx Deployment/hpa-demo-nginx 78%/10% 1 6 6 4m20s 从kubernetesv1.12版本开始,我们可以通过设置kube-controller-manager的--horizontal-pod-autoscaler-downscale-stabilization参数来设置一个持续时间,指的是用于当前扩容操作完成后,多久以后才进行一次缩放操作。默认为5分钟,也就是五分钟后才会进行缩放。\n创建一个基于内存的自动扩容 跟CPU是一样的,都是基于metrics-server获取指标然后进行扩容。\napiVersion: apps/v1 kind: Deployment metadata: name: hpa-mem-demo namespace: default labels: k8s-app: hpa-mem-demo spec: replicas: 1 selector: matchLabels: k8s-app: hpa-mem-demo template: metadata: name: hpa-mem-demo labels: k8s-app: hpa-mem-demo spec: containers: - name: hpa-mem-demo image: nginx:latest resources: requests: memory: 20Mi cpu: 10m securityContext: privileged: true volumeMounts: - name: mount-configmap mountPath: /etc/script volumes: - name: mount-configmap configMap: name: increase-mem-config 这里和前面普通的应用有一些区别，我们将一个名为 increase-mem-config 的 ConfigMap 资源对象挂载到了容器中，该配置文件是用于后面增加容器内存占用的脚本，配置文件如下所示：（increase-mem-cm.yaml）\napiVersion: v1 kind: ConfigMap metadata: name: increase-mem-config data: increase-mem.sh: | #!/bin/bash mkdir /tmp/memory mount -t tmpfs -o size=40M tmpfs /tmp/memory dd if=/dev/zero of=/tmp/memory/block sleep 60 rm /tmp/memory/block umount /tmp/memory rmdir /tmp/memory 由于这里增加内存的脚本需要使用到 mount 命令，这需要声明为特权模式，所以我们添加了 securityContext.privileged=true 这个配置。现在我们直接创建上面的资源对象即可\nkubectl apply -f hpa-demo-mem.yaml kubectl apply -f increase-mem-config.yaml ","date":"2023-02-14T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/HorizontalPodAutoscaler/","title":"HorizontalPodAutoscaler"},{"content":"访问控制概览 Kubernetes API的每个请求都会经过多阶段的访问控制之后才会被接受,这一阶段包括认证、授权、以及准入控制(Admission Control)等\n认证插件 x509证书：使用x509证书只需要API Server启动的时候配置 --client-ca-file=SOMEFILE。在证书认证的时候,其CN域名做用户名,而组织机构用作group名。 静态Token文件：使用静态Token文件认证只需要在API Server启动的时候配置 --token-auth-file=SOMEFILE。该文件为csv格式,每行至少包括三列token、username、user id 引导Token 为了支持平滑的启动和引导新的集群,kubernetes包含了一种动态管理的持有令牌类型,称作启动引导令牌(Bootstrap Token) 这些令牌以Secret的形式保存在kube-system的名称空间中,可以动态的管理和创建。 控制器管理器包含的TokenCleaner控制器能够在启动引导令牌过期时将其删除。 在使用kubeadm部署kubernetes的时候,可以通过kubeadm token list进行查询。 ServiceAccount：是kubernetes自动生成的,并且会自动挂载到容器的/run/secrets/kubernetes.io/serviceaccount目录当中 Webhook令牌身份认证 --authentication-token-webhook-config-file：指向一个配置文件,其中描述如何访问远程的Webhook服务 --authentication-token-webhook-cache-ttl：用来设定身份认证决定的缓存时间。默认为2分钟。 静态Token用法 新建一个存放静态Token的目录 mkdir -p /etc/kubernetes/auth 将Token内容写入到文件当中 注意：该文件格式为CSV格式，其实你也可以随便写:happy:\n描述： Token值 用户名称 用户ID 可选组名 kube-token,kubeadminer,1000,\u0026#34;group1,group2,group3\u0026#34; 假设这是我们请求名称空间的请求: curl -k -v -XGET -H \u0026quot;Authrization: Bearer kube-token\u0026quot; https://api.k8s.version.cn:6443/api/v1/namespaces/default\n正常请求会返回，因为我没有创建这个kube-token\n{ \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;namespaces \\\u0026#34;default\\\u0026#34; is forbidden: User \\\u0026#34;system:anonymous\\\u0026#34; cannot get resource \\\u0026#34;namespaces\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; in the namespace \\\u0026#34;default\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;namespaces\u0026#34; }, \u0026#34;code\u0026#34;: 403 设置API Server 注意： 操作的时候请备份你的API Server文件，这是一个好习惯.\n# vim /etc/kubernetes/manifests/kube-apiserver.yaml # 挂载本地的/etc/basic-auth路径 volumeMounts: - mountPath: /etc/basic-auth name: auth-files readOnly: true volumes: - hostPath: path: /etc/basic-auth type: DirectoryOrCreate name: auth-files # 修改API Server启动参数 - --token-auth-file=/etc/basic-auth/kube-token.csv 请您等待APIServer重启完成\n当我们再次通过kube-token进行请求API Server 如果能成功识别到我们的用户即可，也就知道我们当前的用户为kubeadminer\n{ \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;namespaces \\\u0026#34;default\\\u0026#34; is forbidden: User \\\u0026#34;kubeadminer\\\u0026#34; cannot get resource \\\u0026#34;namespaces\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; in the namespace \\\u0026#34;default\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;namespaces\u0026#34; }, \u0026#34;code\u0026#34;: 403 UserAccount和ServiceAccount的区别 UserAccount是与外部认证二次开发对接实现的,签发的Token是外部系统,每次APIServer需要带此Token请求外部认证服务器。 构建符合Kubernetes规范的认证服务 需要依照kubernetes规范，来构建认证服务进行认证\nURL：http://api.k8s.verbos/authenticate Method: POST(需要携带请求数据等等…) Input： Output： WebHook认证用法 新建一个webhook-config.json { \u0026#34;kind\u0026#34;: \u0026#34;Config\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;preferences\u0026#34;: {}, \u0026#34;clusters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;github-authn\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;https://192.168.1.100:8443/authenticate\u0026#34; } } ], \u0026#34;users\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;authn-apiserver\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;token\u0026#34;: \u0026#34;secret\u0026#34; } } ], \u0026#34;contexts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;webhook\u0026#34;, \u0026#34;context\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;github-authn\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;authn-apiserver\u0026#34; } } ], \u0026#34;current-context\u0026#34;: \u0026#34;webhook\u0026#34; } 授权 授权主要是用于对于集群资源的访问控制，通过检查请求包含的相关属性值，与相对应的访问策略进行比较，API请求必须满足某些策略才能被处理。跟认证类似，kubernetes也支持多种授权机制，并支持同时开启多个插件授权（只要有一个通过即可）。如果授权成功，则用户的请求会发送到准入控制模块进行下一步处理。\nkubernetes授权仅处理以下请求属性\nuser、group、extra API、请求方法、请求路径 请求资源和子资源 Namespace API Group kubernetes支持以下授权插件\nABAC RBAC WebHook Node RBAC和ABAC ABAC(Attribute Based Access Control)本来是不错的概念，但是在kubernetes中实现的比较难于管理和理解，而且需要对master所在节点进行SSH和文件系统权限，要使得对授权的变更生效还需要重启API Server\n而RBAC的策略可以利用kubelet或者kubernetes API进行配置。RBAC可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。RBAC在kubernetes中被映射为API资源和操作。\nRole和ClusterRole Role(角色)是一系列特定角色权限的集合，例如一个角色可以包含读取Pod的权限和列出Pod的权限。\nRole只能用来给某个特定的namespace中的资源作鉴权，对多namespace和集群级的资源活着是非资源类的API(如/healthz)使用ClusterRole\n下面是一个简单的Role配置\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: reader-pods rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] 下面是一个简单地ClusterRole配置\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: reader-pods rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] Binding 如果你想使全局生效可以使用kind: ClusterRoleBingding并且不写namespace\nkind: RoleBingding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: reader-pods # 当前用户只有development的读取权限 namespace: development subjects: - kind: User name: guest01 apiGroup: rbac.authorization.k8s.io/v1 roleRef: kind: ClusterRole name: reader-pods apiGroup: rbac.authorization.k8s.io/v1 账户/组的管理 角色绑定(Role Bingding)是将角色中定义的权限赋予一个或者一组用户。 它包含主体(用户、组或服务账户)的列表和对这些主体所获得的角色的引用 组的概念\n当外部系统认证对接的时候,用户信息可包含组信息，授权可以针对用户群组进行 当对Service Account授权的时候,Group代表某个Namespace下的所有Service Account 如果针对群组授权 kind: ClusterRoleBingding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: reader-pods-global subjects: - kind: Group name: guest01 apiGroup: rbac.authorization.k8s.io/v1 roleRef: kind: ClusterRole name: reader-pods apiGroup: rbac.authorization.k8s.io/v1 name: system:serviceaccounts表示当前是一个serviceaccount的格式进行命名的,并且授权给development这个Namespace里面所有的ServiceAccount\nkind: ClusterRoleBingding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: reader-pods-global subjects: subjects: - kind: Group name: system:serviceaccounts:development apiGroup: rbac.authorization.k8s.io/v1 roleRef: kind: ClusterRole name: reader-pods apiGroup: rbac.authorization.k8s.io/v1 规划系统角色 User:\n管理员：管理所有资源的权限 普通用户 是否有该用户创建的namespace下的所有object的操作权限？ 对其他用户的namespace资源是否可读、是否可写 SystemAccount System是开发者创建应用后，应用于apiserver通讯需要的身份 用户可以创建自定的ServiceAccount，kubernetes也为每个namespace创建default ServiceAccount Default ServiceAccount通常需要给定权限以后才能对API Server进行写操作 与权限相关的其他最佳实践 ClusterRole是非namespace绑定的,针对整个集群生效。 通常需要创建一个管理员角色，并且绑定给开发运营团队的成员 ThirdPartyResource和CustomResourceDefinition是全局资源，普通用户创建以后需要管理员授权才能真正操作该对象。\nssh到master节点通过insecure port访问apiserver可以绕过鉴权,当需要做管理操作又没有权限的时候可以使用(不推荐)\n准入 准入控制 准入控制(Admission Control)在授权后对请求做进一步的验证或添加默认参数。不同于授权和认证只关心请求的用户和操作，准入控制还处理请求的内容，并且仅对创建、更新、删除或连接等有效，而对读操作无效。 准入控制支持同时开启多个插件，它们依次调用，只有全部插件都通过请求才可以进入系统。 配额管理\n原因：资源有限，如何限定某个用户有多少资源 方案 预定义每个Namespace的ResourceQuota,并且把spec保存为ConfigMap 用户可以创建多少个Pod? BestEffortPod QuSPod 创建ResourceQuota Controller: 一般用于监控namespace创建事件，当namespace创建的时候，在该namespace创建对应的ResourceQuota对象 # 限制default namespace只能创建3个configmap apiVersion: v1 kind: ResourceQuota metadata: name: default-counts namespace: default spec: hard: configmaps: \u0026#34;3\u0026#34; 准入控制插件 AlwaysAdmit: 接受所有请求 AlwaysPullImages: 总是拉新的镜像。一般用多租户场景 DenyEscalatingExec: 禁止特权容器的exec和attach操作 ImagePolicyWebhook: 通过webhook决定image策略,需要同时配置--admission-control-config-file ServiceAccount: 自动创建默认Service Account，并确保Pods引用的ServiceAccount已经存在 SecurityContextDeny: 拒绝包含非法SecurityContext配置的容器 ResourceQuota：限制Pod的请求不会超过配额，需要在namespace中创建ResourceQuota对象 MutatingWebhookConfiguration: 变形插件，支持对准入对象的修改 ValidatingWebhoookConfiguration: 校验插件，只能对准入对象合法性进行校验 太多了 我就不一个一个写了…除了默认的准入控制插件以外，kubernetes预留了准入控制插件的扩展点，用户可以自定义准入控制插件\n准入控制插件的演示 MutatingWebhookConfiguration: 说白了就是对准入控制的对象内容进行修改 确保启用 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 控制器。 这里 是一组推荐的 admission 控制器，通常可以启用。 确保启用了admissionregistration.k8s.io/v1的API git clone https://github.com/cncamp/admission-controller-webhook-demo.git 这个暂且先不演示了…我也在研究哈哈\n限流 漏斗算法 漏洞算法也很容易理解，请求进来以后首先进入漏斗里面，然后漏斗以恒定的速率将请求流出进行处理，从而起到平滑流量的作用。 当请求量过大，漏斗达到最大容量的时候会溢出，此时请求被丢弃 在系统看来，请求永远是以平滑的速率过来，从而起到了保护系统的作用\n令牌桶算法 令牌桶算法是对漏斗算法的一种改进，除了能够起到限流的作用外，还允许一定程度的流量突发 在令牌桶算法中，存在一个令牌桶，算法中存在一种机制以恒定的速率向令牌桶中放入令牌。 令牌桶也有一定的容量，如果满了的话令牌也无法放进去 当有请求进入的时候，会首先到令牌桶中去那令牌，则该请求会被处理，并消耗掉拿到的令牌，如果令牌桶为空，则请求会被丢弃。 API Server的限流 max-requests-inflight: 在给定的时间内的最大的non-mutating请求数 max-mutating-requests-inflight: 在给定时间内的最大mutating请求数，调整apiserver的流控qos 反应出来的问题\n粒度粗: 无法为不同用户，不同场景设置不同的限流 单队列：共享限流的窗口/桶，一个坏用户可能将整个系统堵塞 不公平：正常用户的请求会被排到队尾，无法及时处理请求而饿死 无优先级: 重要的系统指令一并被限流，系统故障难以恢复 默认值 节点数1000-3000 节点数\u0026gt;3000 max-requests-inflight 400 1500 3000 max-mutating-requests-inflight 200 500 1000 API Priority and Fairness APF以更细颗粒度的方式对请求进行分类和隔离。 它还引入了空间有限的排队机制，因此在非常短暂的突发情况下，API服务器不会拒绝任何请求 通过使用公平排队技术从队列中分发请求，这样一个行为不佳的控制器就不会饿死其他控制器 APF的核心 多等级 多队列 APF 的实现依赖两个非常重要的资源 FlowSchema, PriorityLevelConfiguration APF 对请求进行更细粒度的分类，每一个请求分类对应一个 FlowSchema (FS) FS 内的请求又会根据 distinguisher 进一步划分为不同的 Flow. FS 会设置一个优先级 (Priority Level, PL)，不同优先级的并发资源是隔离的。所以不同优先级的资源不会相互排挤。特定优先级的请求可以被高优处理。 一个 PL 可以对应多个 FS，PL 中维护了一个 QueueSet，用于缓存不能及时处理的请求，请求不会因为超出 PL 的并发限制而被丢弃。 FS 中的每个 Flow 通过 shuffle sharding 算法从 QueueSet 选取特定的 queues 缓存请求。 每次从 QueueSet 中取请求执行时，会先应用 fair queuing 算法从 QueueSet 中选中一个 queue，然后从这个 queue 中取出 oldest 请求执行。所以即使是同一个 PL 内的请求，也不会出现一个 Flow 内的请求一直占用资源的不公平现象。 通过 kubectl get flowschema查看当前的flow\nFlow Schema FlowSchema会匹配一些入站请求,并将他们分配给优先级 每个入站请求都会有对应的FlowSchema测试是否匹配，首先从matchingPrecedence数值最低的匹配开始(我们认为这是逻辑上匹配最高)，然后依次进行匹配，直到首个匹配出现.\napiVersion: flowcontrol.apiserver.k8s.io/v1beta1 kind: FlowSchema metadata: name: kube-scheduler # FlowSchema名称 spec: distinguisherMethod: type: ByNamespace # Distinguisher 区分器 matchingPrecedence: 800 # 规则优先级,数字越小级别越高 priorityLevelConfiguration: # 对应的优先级队列 name: workload-high # 优先级队列名称 rules: - resourceRules: - resources: - \u0026#39;*\u0026#39; # 对应的资源和请求类型 verbs: - \u0026#39;*\u0026#39; subjects: - kind: User user: name: system:kube-scheduler PriorityLevelConfiguration(优先级队列) 一个PriorityLevelConfiguration表示单个隔离类型。 每个PriorityLevelConfiguration对未完成的请求数有各自的限制,对排队中的请求数也有限制。\nPriorityLevelConfiguration是可以被多个FlowSchema进行复用的\napiVersion: flowcontrol.apiserver.k8s.io/v1beta1 kind: PriorityLevelConfiguration metadata: name: global-default spec: limited: assuredConcurrencyShares: 20 # 允许的并发请求 limitResponse: queuing: handSize: 6 # shuffle sharding配置,每个flowschmea+distinguisher的请求会被enqueue到多少个队列 queueLengthLimit: 50 # 每个队列中的对象数量 queues: 128 # 当前PriorityLevel的队列总数 type: Queue type: Limited 可以通过kubectl get PriorityLevelConfiguration查看当前kubernetes中的优先级队列\nsystem: 用于 system:nodes 组（即 kubelet）的请求； kubelet 必须能连上 API 服务器，以便工作负载能够调度到其上。 leader-election: 用于内置控制器的领导选举的请求 （特别是来自 kube-system 名称空间中 system:kube- controller-manager 和 system:kube-scheduler 用户和服务账号，针对 endpoints、 configmaps 或 leases 的请求）。 将这些请求与其他流量相隔离非常重要，因为领导者选举失败会导致控制器发生故障并重新启动，这反过来会导致新启动的控制器在同步信息时，流量开销更大。 workload-high: 优先级用于内置控制器的请求 workload-low: 优先级适用于来自任何服务帐户的请求，通常包括来自 Pods 中运行的控制器的所有请求。 global-default: 优先级可处理所有其他流量，例如：非特权用户运行的交互式 kubectl 命令。 exempt: 优先级的请求完全不受流控限制：它们总是立刻被分发。 特殊的 exempt FlowSchema把 system:masters 组的所有请求都归入该优先级组。 catch-all: 优先级与特殊的 catch-all FlowSchema 结合使用，以确保每个请求都分类。 一般不应该依赖于 catch-all 的配置，而应适当地创建自己的 catch-all FlowSchema 和PriorityLevelConfigurations（或使用默认安装的 global-default 配置）。 为了帮助捕获部分请求未分类的配置错误，强制要求 catch-all 优先级仅允许5个并发份额，并且不对请求进行排队，使得仅与 catch-all FlowSchema 匹配的流量被拒绝的可能性更高，并显示 HTTP 429 错误。 [root@Online-Beijing-master1 ~]# kubectl get PriorityLevelConfiguration NAME TYPE ASSUREDCONCURRENCYSHARES QUEUES HANDSIZE QUEUELENGTHLIMIT AGE catch-all Limited 5 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 13h exempt Exempt \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 13h global-default Limited 20 128 6 50 13h leader-election Limited 10 16 4 50 13h node-high Limited 40 64 6 50 13h system Limited 30 64 6 50 13h workload-high Limited 40 128 6 50 13h workload-low Limited 100 128 6 50 13h 详细说一下分流策略\n根据service-accounts的flow进行限制,distinguisherMethod根据不同的用户进行限流。 这一类的flow应该通过priorityLevelConfiguration中定义的workload-low进行限流 通过workload-low中定义的assuredConcurrencyShares设置当前请求的最大并发数量 高可用APIServer 搭建多租户的kubernetes集群 授信\n认证: 禁止匿名访问，只允许可信用户做操作。 授权：基于授信的操作，防止多用户之间互相影响，比如普通用户删除Kubernetes核心服务，或者A用户删除或修改B用户 的应用。 隔离\n可见行隔离：用户只关心自己的应用，无需看到其他用户的服务和部署。 资源隔离：有些关键项目对资源需求较高，需要有专业设备，不与其他人共享。 应用访问隔离：用户创建的服务，按照既定规则允许其他用户访问。 资源管理\nQuota管理: 谁能用多少资源 ","date":"2023-02-07T00:00:00Z","image":"https://bj.bcebos.com/baidu-rmb-video-cover-1/2b6495c8749e3f4e4369e28cb50eeb87.png","permalink":"http://localhost:1313/kubernetes/ApiServerRead/","title":"Kubernetes中Api-Server简单解读"},{"content":"官方WebUI部署 Dashboard 安装部署 从官方仓库部署\n[root@containerd-kube-master ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml 如果无法下载请新建dashboard.yaml复制以下内容进行应用\n# Copyright 2017 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. apiVersion: v1 kind: Namespace metadata: name: kubernetes-dashboard --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kubernetes-dashboard type: Opaque --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kubernetes-dashboard type: Opaque data: csrf: \u0026#34;\u0026#34; --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kubernetes-dashboard type: Opaque --- kind: ConfigMap apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kubernetes-dashboard --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard rules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] resourceNames: [\u0026#34;kubernetes-dashboard-key-holder\u0026#34;, \u0026#34;kubernetes-dashboard-certs\u0026#34;, \u0026#34;kubernetes-dashboard-csrf\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;delete\u0026#34;] # Allow Dashboard to get and update \u0026#39;kubernetes-dashboard-settings\u0026#39; config map. - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;] resourceNames: [\u0026#34;kubernetes-dashboard-settings\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;update\u0026#34;] # Allow Dashboard to get metrics. - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;] resourceNames: [\u0026#34;heapster\u0026#34;, \u0026#34;dashboard-metrics-scraper\u0026#34;] verbs: [\u0026#34;proxy\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services/proxy\u0026#34;] resourceNames: [\u0026#34;heapster\u0026#34;, \u0026#34;http:heapster:\u0026#34;, \u0026#34;https:heapster:\u0026#34;, \u0026#34;dashboard-metrics-scraper\u0026#34;, \u0026#34;http:dashboard-metrics-scraper\u0026#34;] verbs: [\u0026#34;get\u0026#34;] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\u0026#34;metrics.k8s.io\u0026#34;] resources: [\u0026#34;pods\u0026#34;, \u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: securityContext: seccompProfile: type: RuntimeDefault containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.6.1 imagePullPolicy: Always ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kubernetes-dashboard # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- kind: Service apiVersion: v1 metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboard spec: ports: - port: 8000 targetPort: 8000 selector: k8s-app: dashboard-metrics-scraper --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboard spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: dashboard-metrics-scraper template: metadata: labels: k8s-app: dashboard-metrics-scraper spec: securityContext: seccompProfile: type: RuntimeDefault containers: - name: dashboard-metrics-scraper image: kubernetesui/metrics-scraper:v1.0.8 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - mountPath: /tmp name: tmp-volume securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 serviceAccountName: kubernetes-dashboard nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: tmp-volume emptyDir: {} 创建应用\nkubectl apply -f dashboard.yaml 查看Dashboard的Pod是否运行\n[root@containerd-kube-master ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kubernetes-dashboard dashboard-metrics-scraper-8c47d4b5d-t92v9 1/1 Running 0 26m kubernetes-dashboard kubernetes-dashboard-6c75475678-zlqn7 1/1 Running 0 26m 修改Dashboard的Services\n[root@containerd-kube-master ~]# kubectl edit services kubernetes-dashboard -n kubernetes-dashboard # Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;k8s-app\u0026#34;:\u0026#34;kubernetes-dashboard\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;kubernetes-dashboard\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kubernetes-dashboard\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;ports\u0026#34;:[{\u0026#34;port\u0026#34;:443,\u0026#34;targetPort\u0026#34;:8443}],\u0026#34;selector\u0026#34;:{\u0026#34;k8s-app\u0026#34;:\u0026#34;kubernetes-dashboard\u0026#34;}}} creationTimestamp: \u0026#34;2022-09-06T02:58:06Z\u0026#34; labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard resourceVersion: \u0026#34;5359\u0026#34; uid: 31019fac-9b16-46e5-9172-2cc3f63f2c86 spec: clusterIP: 10.101.114.92 clusterIPs: - 10.101.114.92 internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - port: 443 protocol: TCP targetPort: 8443 nodePort: 30000 # 添加一行nodePort selector: k8s-app: kubernetes-dashboard sessionAffinity: None type: ClusterIP # 修改为NodePort status: loadBalancer: {} 改完以后一定要看一眼生效无生效啊\n查看到TYPE如果是NodePort 查看PORT对应443映射到了30000端口 [root@containerd-kube-master ~]# kubectl get services --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 98m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 98m kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.103.193.200 \u0026lt;none\u0026gt; 8000/TCP 43m kubernetes-dashboard kubernetes-dashboard NodePort 10.101.114.92 \u0026lt;none\u0026gt; 443:30000/TCP 43m 访问页面 访问https://IP:PORT 例如: https://10.1.6.45:30000/#/login 新版本中默认不存储secret的加密数据了,所以如果你想使用Token进行登录的话,你需要新建一个用户并且绑定角色\n新建一个user.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard [root@containerd-kube-master ~]# kubectl apply -f user.yaml 基于admin-user创建一个token,然后把输出的token粘贴到Dashboard\n[root@Online-Beijing-master1 ~]# kubectl create token admin-user -n kubernetes-dashboard eyJhbGciOiJSUzI1NiIsImtpZCI6InN2amtNUDdoaTRoU3VUVUw3aC1UTkVCRXlBRkhnQmlPTWMzWWZmbUhTd0kifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjc3NTk5NjEwLCJpYXQiOjE2Nzc1OTYwMTAsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiZGU3ZDYxNWItMDk5MC00ZjI3LTlkOGQtNzkwMzQyNjZjNjRlIn19LCJuYmYiOjE2Nzc1OTYwMTAsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.uvntmSuffTF8HGIZNacfmDJr8DdMTY4SR_H6Gyxck6Te_p2J4u5mNNmId2F5uWaoeySCYam-CUkkAO2F5Uh2WjxfD0wb2YI8-FRKic8n40n30TgKhdDeK-wlliDK5kVlf9yvy5uDEc8TWZhkXz-e9WLCMqta4J6_vc6t5PYJf8u1I_SYVOiC5fgP46z3W9bZMYeGV7xmKQGl9OeofQokC1TJCojtrt0AZJ0MW_2FQjZG7ONdiywaOYnST-Ii5rK12hJFoAjZem6aTzXikOJJ03zw7Kw4WqVdMAqnbs81Clkefi-ZPxReED2wi65RbLf8KVXFVWMn9EccX5IQC5Y49Q 推荐几个其他的WebUi KubeSphere KubeGems Kuboard\n","date":"2023-02-03T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/KubeDashboard/","title":"kubernetes-dashboard"},{"content":"2022终将逝去 2022的最后一天,和朋友一起坐车去了天津。 早上五点起床开始收拾东西了,忙前忙后的发现坐公交已经赶不上了,算了还是打车吧。上高铁的时候真的是太困了…以至于一路上没拍到什么风景。\n其实有一个能和你说走就走的朋友真的很好\n说一下此行景点 Day1: 和朋友直接坐到滨海-\u0026gt;老码头(实则没去)-\u0026gt;国家海洋博物馆(学了很多)-\u0026gt;东堤公园(看日出🌄) Day2：和朋友从滨海坐到天津-\u0026gt;五大道-\u0026gt;意大利风情区-\u0026gt;市区 看海 看海是我觉得人生中最快乐的事情(对海有一种爱不释手的感觉),每次一看到海就能感觉到让自己的全身放空,真的太好看了。把烂事丢在2022! 我在天津很想你 这个应该是店家自己买的,来拍照的人不算很多,但是也是一个标志性的柱子了(哈哈). 里面是一个川小馆,由于我朋友不能在外面吃饭(无缘品尝了) 城市夜景 不得不说,我觉得天津真的比北京有烟火气的多。 我在解放桥海河上看到了天津的烟花,足足地放了2个多小时…是一个适合生活的城市,路上的行人也非常非常的多(不知道是不是因为节假日的原因)。路上顺带拍了几张天津的城市夜景。\n自己给它取个名字(格林威治大钟) \u0026ndash;可以听听五月天的《步步》 格林威治大钟前 归零超载的伤悲 背着我和我的诺言 一起计划的路线对照孤单的旅店 街头随便拍的-回来搞了个德国街头的调色 ","date":"2023-01-01T00:00:00Z","image":"https://img12.360buyimg.com/ddimg/jfs/t1/52521/8/22190/1015219/63b4f0deFd2378228/7dd2825c978f8f8c.jpg","permalink":"http://localhost:1313/tourists/tianjin/","title":"旅行日记-跨年•天津"},{"content":"kubernetes1.22.10部署 准备工作 兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及那些没有包管理器的发行版提供了通用说明。 每台机器 2 GB 或更多 RAM（任何更少都会为您的应用程序留下很小的空间）。 2 个 CPU 或更多。 集群中所有机器之间的完整网络连接（公共或专用网络都可以）。 每个节点的唯一主机名、MAC 地址和 product_uuid。有关更多详细信息，请参见此处。 您的机器上的某些端口是开放的。有关更多详细信息，请参见此处。 交换Swap分区。必须禁用Swap才能使 kubelet 正常工作。 我的服务器配置列表 没有必要按照我的这个配置去操作个人建议实验环境：正常演示环境2核2G就够了\n需要开放的端口 虽然 etcd 端口包含在控制平面部分，但您也可以在外部或自定义端口上托管自己的 etcd 集群。 可以覆盖所有默认端口号。当使用自定义端口时，这些端口需要打开而不是此处提到的默认值。 一个常见的例子是 API 服务器端口，有时会切换到 443。或者，默认端口保持原样，API 服务器放在负载均衡器后面，该负载均衡器监听 443 并将请求路由到默认端口上的 API 服务器。\n准备主机地址 修改每一台主机的/etc/hosts配置 # vim /etc/hosts 10.1.6.45 containerd-kube-master 10.1.6.46 containerd-kube-work1 10.1.6.47 containerd-kube-work2 关闭swap分区以及防火墙 进入fstab后找到你挂载的swap分区注释即可.\n[root@bogon ~]# swapoff -a [root@localhost ~]# echo \u0026#34;vm.swappiness = 0\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf [root@bogon ~]# vim /etc/fstab # /dev/mapper/rl-swap none swap defaults 0 0 [root@localhost ~]# systemctl stop firewalld \u0026amp;\u0026amp; systemctl disable firewalld # 关闭并且禁用防火墙 所有内容准备完成后重启三台服务器!\nContainerd的基础安装和操作 本文档后续基于Containerd+RockyLinux+Kubeadmin部署Kubernetes1.24版本的环境。\ncontainerd Docker CRI-O 需要注意的是,根据Kubernetes官方给出的公告。Kubernetes 1.20x版本将会废弃对Docker的支持\n参考连接 通过阿里云镜像源安装 官方镜像站 三台主机全部执行此操作 [root@containerd-kube-master ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 [root@containerd-kube-master ~]# yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo [root@containerd-kube-master ~]# yum -y install containerd.io 查看一下containerd的版本\n[root@containerd-kube-master ~]# containerd -v containerd containerd.io 1.6.8 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 生成containerd的配置文件 三台主机全部执行此操作 默认情况下在/etc/containerd/config.toml已经有这个文件了,但是里面是一些简短的配置. [root@containerd-kube-master containerd]# mkdir - /etc/containerd/ [root@containerd-kube-master containerd]# containerd config default | tee /etc/containerd/config.toml # 生成contained的默认配置 修改sandbox_img pause： 此镜像是kubernetes的基础容器 三台主机全部执行此操作 由于部分用户无法进入k8s.gcr.io资源地址,需要对此地址进行替换. [root@containerd-kube-master containerd]# vim /etc/containerd/config.toml sandbox_image = \u0026#34;k8s.gcr.io/pause:3.6\u0026#34; # 找到此选项并且修改为: registry.aliyuncs.com/google_containers/pause:3.6 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.default_runtime] runtime_type = \u0026#34;io.containerd.runtime.v1.linux\u0026#34;# 修改为io.containerd.runtime.v1.linux 修改Systemd Cgroup驱动 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true 然后重启containerd\nsystemctl restart containerd 设置Crictl 查找的容器运行时 crictl config runtime-endpoint unix:///var/run/containerd/containerd.sock (额外) 设置Containerd的私有仓库 如果你想用自己的私有仓库的话，可以通过如下方式进行设定\n[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.image_decryption] key_model = \u0026#34;node\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.auths] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs.\u0026#34;10.1.6.15\u0026#34;.auth] username = \u0026#34;\u0026#34; # 私有仓库账号 password = \u0026#34;\u0026#34; # 私有仓库密码 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.headers] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;10.1.6.15\u0026#34;] endpoint = [\u0026#34;\u0026#34;] # 私有仓库地址 启动containerd 三台主机全部执行此操作 保证Active: active(running)的状态即可 [root@containerd-kube-master containerd]# systemctl restart containerd [root@containerd-kube-master containerd]# systemctl enable containerd [root@containerd-kube-master containerd]# systemctl status containerd ● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; disabled; vendor preset: disabled) Active: active (running) since Mon 2022-09-05 02:53:02 EDT; 6s ago Docs: \u0026lt;https://containerd.io\u0026gt; Process: 8465 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 8467 (containerd) Tasks: 12 Memory: 25.2M CGroup: /system.slice/containerd.service └─8467 /usr/bin/containerd Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159619104-04:00\u0026#34; level=info msg=\u0026#34;Start subscribing containerd event\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159662811-04:00\u0026#34; level=info msg=\u0026#34;Start recovering state\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159718042-04:00\u0026#34; level=info msg=\u0026#34;Start event monitor\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159737174-04:00\u0026#34; level=info msg=\u0026#34;Start snapshots syncer\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159750064-04:00\u0026#34; level=info msg=\u0026#34;Start cni network conf syncer for default\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159756351-04:00\u0026#34; level=info msg=\u0026#34;Start streaming server\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159868546-04:00\u0026#34; level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159906215-04:00\u0026#34; level=info msg=serving... address=/run/containerd/containerd.sock Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.160196660-04:00\u0026#34; level=info msg=\u0026#34;containerd successfully booted in 0.021144s\u0026#34; Sep 05 02:53:02 containerd-kube-master systemd[1]: Started containerd container runtime. 配置IPV4转发 三台全部执行 cat \u0026lt;\u0026lt;EOF | tee /etc/modules-load.d/kubernetes1.24.conf overlay br_netfilter EOF cat \u0026lt;\u0026lt;EOF | tee /etc/sysctl.d/kubernetes1.24-forsys.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF [root@containerd-kube-master containerd]# modprobe br_netfilter [root@containerd-kube-master ~]# sysctl --system kubernetes安装 kubernetes的安装依旧是基于aliyun\n通过阿里云镜像源安装 三台全部安装 由于官网未开放同步方式, 可能会有索引gpg检查失败的情况, 这时请用 yum install -y --nogpgcheck kubelet kubeadm kubectl 安装 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg EOF [root@containerd-kube-work1 ~]# yum install -y kubelet-1.22.10 kubeadm-1.22.10 kubectl-1.22.10 可以通过yum --showduplicates list kubelet查看当前仓库中可用的版本\n安装命令提示 安装后可以使用tab进行快捷提示\n[root@containerd-kube-master ~]# yum -y install bash-completion [root@containerd-kube-master ~]# source \u0026lt;(kubeadm completion bash) \u0026amp;\u0026amp; source \u0026lt;(kubectl completion bash) 如果你想要永久的使其生效,请把他们加入到.bashrc当中\ncd ~ [root@containerd-kube-master ~]# vim .bashrc # .bashrc # User specific aliases and functions alias rm=\u0026#39;rm -i\u0026#39; alias cp=\u0026#39;cp -i\u0026#39; alias mv=\u0026#39;mv -i\u0026#39; # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi source \u0026lt;(kubeadm completion bash) source \u0026lt;(kubectl completion bash) source \u0026lt;(crictl completion bash) 启动kubelet [root@containerd-kube-master containerd]# systemctl enable kubelet 初始化集群配置信息 [root@containerd-kube-master containerd]# kubeadm config print init-defaults \u0026gt; init.yaml apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.1.6.45 # 初始化的第一台master主机的地址 bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock imagePullPolicy: IfNotPresent name: kubernetes-master # 定义主机的唯一名称 taints: null --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 修改为阿里云地址 kind: ClusterConfiguration kubernetesVersion: 1.22.10 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 # Pod使用的子网地址 scheduler: {} [root@containerd-kube-master containerd]# kubeadm init --config=init.yaml # 等待镜像Pull完成 [init] Using Kubernetes version: v1.24.0 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; 创建admin配置目录 [root@containerd-kube-master containerd]# mkdir -p $HOME/.kube [root@containerd-kube-master containerd]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@containerd-kube-master containerd]# sudo chown $(id -u):$(id -g) $HOME/.kube/config 创建集群网络 因为flannel不支持网络隔离,所以不想用了!\n不再基于flannel,而是基于calico [root@containerd-kube-master .kube]# curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O # --\u0026gt;这个地址好像是用不了了,现在404了... curl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O 编辑calico.yaml\n- name: CALICO_IPV4POOL_CIDR # 修改CIDR为Kubernetes的子网地址,即初始化集群的serviceSubnet value: \u0026#34;10.96.0.0/12\u0026#34; 创建calico网络\n[root@containerd-kube-master ~]# kubectl apply -f calico.yaml 加入集群 如果初始化成功会出现Your Kubernetes control-plane has initialized successfully!\n在node节点执行 [root@containerd-kube-work1 containerd]# kubeadm join 10.1.6.45:6443 --token abcdef.0123456789abcdef \\\\ --discovery-token-ca-cert-hash sha256:417d4385cc4f0cc572b106a6a13bf59fd865421f12a401f3660afa6ca19e500d 验证集群 查看master节点的Pod是否全部启动\n[root@containerd-kube-master ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6799f5f4b4-pf8w8 1/1 Running 0 2m16s kube-system calico-node-lzcjk 1/1 Running 0 54s kube-system calico-node-mrqkd 1/1 Running 0 2m16s kube-system calico-node-r45bc 1/1 Running 0 71s kube-system coredns-74586cf9b6-gkmbl 1/1 Running 0 2m37s kube-system coredns-74586cf9b6-tgh6f 1/1 Running 0 2m37s kube-system etcd-kubernetes-master 1/1 Running 2 2m42s kube-system kube-apiserver-kubernetes-master 1/1 Running 2 2m43s kube-system kube-controller-manager-kubernetes-master 1/1 Running 2 2m43s kube-system kube-proxy-mx4bg 1/1 Running 0 54s kube-system kube-proxy-ssw98 1/1 Running 0 71s kube-system kube-proxy-tpgfj 1/1 Running 0 2m38s kube-system kube-scheduler-kubernetes-master 1/1 Running 2 2m43s 从master上查看节点是否已经全部Ready\n[root@containerd-kube-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION containerd-kube-work1 Ready \u0026lt;none\u0026gt; 55s v1.24.2 containerd-kube-work2 Ready \u0026lt;none\u0026gt; 38s v1.24.2 containerd-kube-master Ready control-plane 2m29s v1.24.2 到此为止,1.24的kubernetes已经安装完毕\n提一个小问题 看一下你们的coredns是否在同一个节点上,如果在同一个节点上,建议重新分配一下coredns保证其高可用性\n[root@containerd-kube-master ~]# kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-74586cf9b6-gkmbl 1/1 Running 0 32m 10.105.56.1 kubernetes-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-74586cf9b6-tgh6f 1/1 Running 0 32m 10.105.56.3 kubernetes-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 重新分配coredns\n[root@containerd-kube-master ~]# kubectl --namespace kube-system rollout restart deployment coredns 问题解决 使用crictl image出现WARN[0000] image connect using default endpoints 出现该问题的原因是由于crictl不知道使用那个sock导致的 [root@containerd-kube-master ~]# crictl image WARN[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. 解决方法如下\n默认情况下containerd的sock存放于/run/containerd/containerd.sock # 重新设置一下使用的runtime-endpoint crictl config runtime-endpoint unix:///run/containerd/containerd.sock 默认生成的crictl存放在/etc/crictl.yaml\n[root@containerd-kube-master containerd]# vim /etc/crictl.yaml 编辑配置文件\nruntime-endpoint: \u0026#34;unix:///run/containerd/containerd.sock\u0026#34; image-endpoint: \u0026#34;unix:///run/containerd/containerd.sock\u0026#34; # 新版本增加了image-endpoint需要手动更改 timeout: 10 debug: false pull-image-on-create: false disable-pull-on-run: false [root@containerd-kube-master containerd]# systemctl daemon-reload [root@containerd-kube-master containerd]# crictl image IMAGE TAG IMAGE ID SIZE Master主集群加入token过期如何处理 默认情况下,该token只有24小时,如果token值过期的话需要重新生成 --discovery-token-ca-cert-hash sha256:793106d3b4305808d55c3cdb211f89a768bec86ecef64264b131dc8f2548da16 查看当前master集群的token列表 [root@containerd-kube-master .kube]# kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS abcdef.0123456789abcdef 8h 2022-09-06T10:10:05Z authentication,signing \u0026lt;none\u0026gt; system:bootstrappers:kubeadm:default-node-token 重新生成一份token [root@containerd-kube-master .kube]# kubeadm token create 通过证书hashtokne [root@containerd-kube-master .kube]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; ","date":"2022-12-29T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/InstallKubernetes1.22.10","title":"kubernetes1.22.0单节点集群部署"},{"content":"利用Kubeadm创建高可用集群 使用具有堆叠的控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。 使用外部 etcd 集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。 在下一步之前，你应该仔细考虑哪种方法更好地满足你的应用程序和环境的需求。 高可用拓扑选项 讲述了每种方法的优缺点。 如何安装Kubectl和Kubeadm 如何安装外部的Etcd集群 参与主机列表 IP CPU 内存 硬盘 角色 10.1.6.48 8 16 100 control-plane1 10.1.6.24 8 16 100 control-plane2 10.1.6.45 8 16 100 control-plane3 10.1.6.46 8 16 100 work1 10.1.6.43 8 16 100 work2 10.1.6.47 8 16 100 work3 10.1.6.213 4 4 20 HA+KP1 10.1.6.214 4 4 20 HA+KP2 10.1.6.215 Load_Balancer_IP 10.1.6.51 8 16 100 Etcd1 10.1.6.52 8 16 100 Etcd2 10.1.6.53 8 16 100 Etcd3 为Kube-apiserver创建负载均衡器 Keepalived 提供 VRRP 实现，并允许您配置 Linux 机器使负载均衡，预防单点故障。HAProxy 提供可靠、高性能的负载均衡，能与 Keepalived 完美配合。\n由于 lb1 和 lb2 上安装了 Keepalived 和 HAproxy，如果其中一个节点故障，虚拟 IP 地址（即浮动 IP 地址）将自动与另一个节点关联，使集群仍然可以正常运行，从而实现高可用。若有需要，也可以此为目的，添加更多安装 Keepalived 和 HAproxy 的节点。\n先运行以下命令安装 Keepalived 和 HAproxy。\nyum install keepalived haproxy psmisc -y dnf install keepalived haproxy psmisc -y 在两台用于负载均衡的机器上运行以下命令以配置 Proxy（两台机器的 Proxy 配置相同）：\nvi /etc/haproxy/haproxy.cfg global log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:8443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server kube-apiserver-1 10.1.6.48:6443 check # Replace the IP address with your own. server kube-apiserver-2 10.1.6.24:6443 check # Replace the IP address with your own. server kube-apiserver-3 10.1.6.45:6443 check # Replace the IP address with your own. 启动Haproxy\n请确保你的LB2也已经进行如上配置\nsystemctl restart haproxy systemctl enable haproxy 配置Keepalived KP1配置如下\nvim /etc/keepalived/keepalived.conf global_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 100 interface ens192 # 网卡名称 virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip 10.1.6.213 # The IP address of this machine unicast_peer { 10.1.6.214 # The IP address of peer machines } virtual_ipaddress { 10.1.6.215/24 # 虚拟IP地址 } track_script { chk_haproxy } } KP2配置如下\nvim /etc/keepalived/keepalived.conf global_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 90 interface ens192 # Network card virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip 10.1.6.214 # The IP address of this machine unicast_peer { 10.1.6.214 # The IP address of peer machines } virtual_ipaddress { 10.1.6.215/24 # The VIP address } track_script { chk_haproxy } } 请确保你的KP1和KP2都完成了如上配置\nsystemctl start keepalived systemctl enable keepalived 配置Master主机节点的Hosts文件 所有的Master主机都需要进行配置,防止后续解析不到api.k8s.verbos.com\n10.1.6.48 containerd-master1 10.1.6.24 containerd-master2 10.1.6.45 containerd-master3 10.1.6.46 containerd-work1 10.1.6.43 containerd-work2 10.1.6.47 containerd-work3 10.1.6.51 etcd1 10.1.6.52 etcd2 10.1.6.53 etcd3 10.1.6.215 api.k8s.verbos.com 设置Etcd集群证书 如果你使用的是工作在Work节点的Etcd或者其他单独的Etcd集群,请将Etcd的ca证书进行拷贝到Master节点当中.\nexport CONTROL_PLANE=\u0026#34;root@10.1.6.48\u0026#34; scp /etc/kubernetes/pki/etcd/ca.crt \u0026#34;${CONTROL_PLANE}\u0026#34;: scp /etc/kubernetes/pki/apiserver-etcd-client.crt \u0026#34;${CONTROL_PLANE}\u0026#34;: scp /etc/kubernetes/pki/apiserver-etcd-client.key \u0026#34;${CONTROL_PLANE}\u0026#34;: 设置第一个控制平面节点 用以下内容创建一个名为 kubeadm-config.yaml 的文件\napiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: v1.22.10 controlPlaneEndpoint: \u0026#34;api.k8s.verbos.com:8443\u0026#34; apiServer: certSANs: - 10.1.6.48 - 10.1.6.24 - 10.1.6.45 etcd: external: endpoints: - https://10.1.6.51:2379 # 适当地更改 ETCD_0_IP - https://10.1.6.52:2379 # 适当地更改 ETCD_1_IP - https://10.1.6.53:2379 # 适当地更改 ETCD_2_IP caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key imageRepository: registry.aliyuncs.com/google_containers networking: podSubnet: 10.244.0.0/16 serviceSubnet: 10.10.0.0/16 在节点上运行如下命令\nkubeadm init --config kubeadm-config.yaml --upload-certs --v=5 注意：如果你的集群初始化成功你将会看到如下信息.\n请保存好kubeadm join的内容,默认2小时后过期,过期后重新生成即可 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join api.k8s.verbos.com:8443 --token 9oerz0.fw6s8ft9xa44i077 \\ --discovery-token-ca-cert-hash sha256:be3c70562ae6bf8cfcfbbfa3bb8124fe63af3b1a0671e806a4ccf1bc243d5c6b \\ --control-plane --certificate-key cdf0f280f9e59e18e5f60d98b624008d828ba00ef096a2e38fd9b6b1463be152 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \u0026#34;kubeadm init phase upload-certs --upload-certs\u0026#34; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join api.k8s.verbos.com:8443 --token 9oerz0.fw6s8ft9xa44i077 \\ --discovery-token-ca-cert-hash sha256:be3c70562ae6bf8cfcfbbfa3bb8124fe63af3b1a0671e806a4ccf1bc243d5c6b 拷贝集群配置文件\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 运行网络CNI插件(Calico) 注意：此时你应当有一个CNI插件来提供网络服务,如果不安装CNI插件,那么集群将会处于不可用状态.\n如果你可以正常访问github\n[root@containerd-kube-master .kube]# curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O 国内用户\ncurl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O 修改Calico.yaml配置 修改CIDR为Kubernetes的子网地址,该地址为serviceSubnet的地址,也就是Pod运行的地址。\n- name: CALICO_IPV4POOL_CIDR value: \u0026#34;10.10.0.0/16\u0026#34; 执行calico.yaml\nkubectl apply -f calico.yaml 将其他控制平面节点加入集群 当你已经确保你的containerd-master1完成初始化的时候,你可以将其他的master节点加入到此集群当中.\nkubeadm join api.k8s.verbos.com:8443 --token 9oerz0.fw6s8ft9xa44i077 \\ --discovery-token-ca-cert-hash sha256:be3c70562ae6bf8cfcfbbfa3bb8124fe63af3b1a0671e806a4ccf1bc243d5c6b \\ --control-plane --certificate-key cdf0f280f9e59e18e5f60d98b624008d828ba00ef096a2e38fd9b6b1463be152 加入集群成功后,请拷贝集群配置文件否则将影响kubelet的工作\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 将工作节点加入集群 kubeadm join api.k8s.verbos.com:8443 --token mck9bg.renw4t689mmx7oe1 \\ --discovery-token-ca-cert-hash sha256:ed51fca8615acf5f437f63f66a9709e43ef942caf19aea4c595cb905e4a9a00f (可选项)修改Kubelet的数据存储目录 如果你需要修改kubelet的数据存储目录,请按照如下方式进行操作\nvim /etc/sysconfig/kubelet 设置你的kubelet的数据存储目录(建议单独的为kubelet挂载一块数据盘)\nKUBELET_EXTRA_ARGS=\u0026#34;--root-dir=/data/k8s/kubelet\u0026#34; 重启kubelet\nsystemctl daemon-reloa systemctl restart kubelet umount $(df -HT | grep \u0026#39;/var/lib/kubelet/pods\u0026#39; | awk \u0026#39;{print $7}\u0026#39;) 查看新的数据目录是否有kubelet的数据\n[root@containerd-master2 kubelet]# ls /data/k8s/kubelet/ cpu_manager_state memory_manager_state plugins plugins_registry pod-resources pods 如何剔除一个Master 先说一个问题,当我们正常如果一个Master要进行某种升级的时候,如何正确的安全的来进行删除该Master。 首先我们要将该Master设置为不可调度的状态,并且驱逐在其上面的Pod # 设置Master为不可调度的状态 kubectl cordon online-beijing-master1 # 将Master剔除该集群 kubectl drain online-beijing-master1 --ignore-daemonsets # 然后将其节点从集群中删除 kubectl delete node online-beijing-master1 这里提一个小问题: 当你使用kubectl drain的时候,如果你当前的Master已经有挂载的emptyDir需要使用--delete-emptydir-data 进行删除。 TODO: 删除前请注意备份您的数据。\n","date":"2022-12-29T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/BaseKubeadmHA/","title":"利用Kubeadm进行多Master高可用部署"},{"content":"Kubernetes中Cgroup泄漏问题 Cgorup文档: https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\n绝大多数的kubernetes集群都有这个隐患。只不过一般情况下，泄漏得比较慢，还没有表现出来而已。\n一个pod可能泄漏两个memory cgroup数量配额。即使pod百分之百发生泄漏， 那也需要一个节点销毁过三万多个pod之后，才会造成后续pod创建失败。\n一旦表现出来，这个节点就彻底不可用了，必须重启才能恢复。\n故障表现 该内容的故障信息已经提交给Github: https://github.com/kubernetes/kubernetes/issues/112940 我在服务器中更新Pod出现如下错误 cannot allocate memory\nunable to ensure pod container exists: failed to create container for [kubepods burstable podd5dafc96-2bcd-40db-90fd-c75758746a7a] : mkdir /sys/fs/cgroup/memory/kubepods/burstable/podd5dafc96-2bcd-40db-90fd-c75758746a7a: cannot allocate memory 使用dmesg查看系统日志的错误内容信息\nSLUB: Unable to allocate memory on node -1 服务器配置信息 操作系统: CentOS Linux release 7.9.2009 (Core) 系统内核: 3.10.0-1160.el7.x86_64 Kubernetes: 1.17.9 dockerVersion: 20.10.7 问题原因1 Kubernetes在1.9版本开启了对kmem的支持,因此 1.9以后的所有版本都有该问题，但必须搭配3.x内核的机器才会出问题。一旦出现会导致新 pod 无法创建，已有 pod不受影响，但pod 漂移到有问题的节点就会失败，直接影响业务稳定性。因为是内存泄露，直接重启机器可以暂时解决，但还会再次出现。 cgroup的kmem account特性在3.x 内核上有内存泄露问题，如果开启了kmem account特性会导致可分配内存越来越少，直到无法创建新 pod 或节点异常。\nkmem account 是cgroup 的一个扩展，全称CONFIG_MEMCG_KMEM，属于机器默认配置，本身没啥问题，只是该特性在 3.10 的内核上存在漏洞有内存泄露问题，4.x的内核修复了这个问题。 因为 kmem account 是 cgroup 的扩展能力，因此runc、docker、k8s 层面也进行了该功能的支持，即默认都打开了kmem 属性。 因为3.10 的内核已经明确提示 kmem 是实验性质，我们仍然使用该特性，所以这其实不算内核的问题，是 k8s 兼容问题。 问题原因2 memcg是 Linux 内核中用于管理 cgroup 内存的模块，整个生命周期应该是跟随 cgroup 的，但是在低版本内核中(已知3.10)，一旦给某个 memory cgroup 开启 kmem accounting 中的 memory.kmem.limit_in_bytes 就可能会导致不能彻底删除 memcg 和对应的 cssid，也就是说应用即使已经删除了 cgroup (/sys/fs/cgroup/memory 下对应的 cgroup 目录已经删除), 但在内核中没有释放 cssid，导致内核认为的 cgroup 的数量实际数量不一致，我们也无法得知内核认为的 cgroup 数量是多少。 这个问题可能会导致创建容器失败，因为创建容器为其需要创建 cgroup 来做隔离，而低版本内核有个限制：允许创建的 cgroup 最大数量写死为 65535，如果节点上经常创建和销毁大量容器导致创建很多 cgroup，删除容器但没有彻底删除 cgroup 造成泄露(真实数量我们无法得知)，到达 65535 后再创建容器就会报创建 cgroup 失败并报错 no space left on device，使用 kubernetes 最直观的感受就是 pod 创建之后无法启动成功。\n解决方案 目前官方给出的解决方案如下:\nkernel upgrade to 4.0+: Update kernel rebuild the kubelet with nokmem args. See nokmem Set cgroup.memory=nokmem in grub: see grub 解决方案一 感谢提供的解决方案: https://cloud.tencent.com/developer/article/1739289 https://github.com/torvalds/linux/commit/d6e0b7fa11862433773d986b5f995ffdf47ce672 https://support.mesosphere.com/s/article/Critical-Issue-KMEM-MSPH-2018-0006 这种方式的缺点是：\n1、要升级所有节点，节点重启的话已有 pod 肯定要漂移，如果节点规模很大，这个升级操作会很繁琐，业务部门也会有意见，要事先沟通。 2、这个问题归根结底是软件兼容问题，3.x 自己都说了不成熟，不建议你使用该特性，k8s、docker却 还要开启这个属性，那就不是内核的责任，因为我们是云上机器，想替换4.x 内核需要虚机团队做足够的测试和评审，因此这是个长期方案，不能立刻解决问题。 3、已有业务在 3.x 运行正常，不代表可以在 4.x 也运行正常，即全量升级内核之前需要做足够的测试，尤其是有些业务需求对os做过定制。 解决方案2 修改虚机启动的引导项 grub 中的cgroup.memory=nokmem，让机器启动时直接禁用 cgroup的 kmem 属性\nvim /etc/default/grub GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\u0026#34;$(sed \u0026#39;s, release .*$,,g\u0026#39; /etc/system-release)\u0026#34; GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\u0026#34;console\u0026#34; GRUB_CMDLINE_LINUX=\u0026#34;crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet cgroup.memory=nokmem\u0026#34; GRUB_DISABLE_RECOVERY=\u0026#34;true\u0026#34; 更改完成后你需要生成一下新的cgroup配置.\n/usr/sbin/grub2-mkconfig -o /boot/grub2/grub.cfg reboot # 重启服务器 解决方案3 如果你想在Kubernetes中禁用该属性。issue 中一般建议修改 kubelet代码并重新编译。\n对于v1.13及其之前版本的kubelet，需要手动替换以下两个函数。\nvendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go func EnableKernelMemoryAccounting(path string) error { return nil } func setKernelMemory(path string, kernelMemoryLimit int64) error { return nil } 重新编译并替换 kubelet\nmake WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS=\u0026#34;-N -l\u0026#34; 对于v1.14及其之后版本的kubelet,通过添加BUILDTAGS来禁止 kmem accounting.\nmake BUILDTAGS=\u0026#34;nokmem\u0026#34; WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS=\u0026#34;-N -l\u0026#34; 遇到1.16 版本的BUILDTAGS=”nokmem“编译出来的 let 还是有问题，还是通过修改代码的方式使其生效\nvendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/kmem.go package fs import ( \u0026#34;errors\u0026#34; ) func EnableKernelMemoryAccounting(path string) error { return nil } func setKernelMemory(path string, kernelMemoryLimit int64) error { return errors.New(\u0026#34;kernel memory accounting disabled in this runc build\u0026#34;) } 编译前，可以编辑下文件 hack/lib/version.sh，将 KUBE_GIT_TREE_STATE=\u0026quot;dirty\u0026quot; 改为 KUBE_GIT_TREE_STATE=\u0026quot;clean\u0026quot;，确保版本号干净。\n影响范围 k8s在1.9版本开启了对kmem的支持，因此1.9以后的所有版本都有该问题,但必须搭配 3.x内核的机器才会出问题。一旦出现会导致新pod无法创建,已有 pod不受影响，但pod 漂移到有问题的节点就会失败，直接影响业务稳定性。因为是内存泄露，直接重启机器可以暂时解决，但还会再次出现。\n大概得原理理解 keme是什么? kmem是Cgroup的一个扩展，全称CONFIG_MEMCG_KMEM，属于机器默认配置。\n内核内存与用户内存：\n内核内存：专用于Linux内核系统服务使用，是不可swap的，因而这部分内存非常宝贵的。但现实中存在很多针对内核内存资源的攻击，如不断地fork新进程从而耗尽系统资源，即所谓的“fork bomb”。\n为了防止这种攻击，社区中提议通过linux内核限制 cgroup中的kmem 容量，从而限制恶意进程的行为，即kernel memory accounting机制。\n使用如下命令查看KMEM是否打开：\ncat /boot/config-`uname -r`|grep CONFIG_MEMCG CONFIG_MEMCG=y CONFIG_MEMCG_SWAP=y CONFIG_MEMCG_SWAP_ENABLED=y CONFIG_MEMCG_KMEM=y cgroup与kmem机制 使用 cgroup 限制内存时，我们不但需要限制对用户内存的使用，也需要限制对内核内存的使用。kernel memory accounting 机制为 cgroup 的内存限制增加了 stack pages（例如新进程创建）、slab pages(SLAB/SLUB分配器使用的内存)、sockets memory pressure、tcp memory pressure等，以保证 kernel memory 不被滥用。\n当你开启了kmem 机制，具体体现在 memory.kmem.limit_in_bytes 这个文件上：\n/sys/fs/cgroup/memory/kubepods/pod632f736f-5ef2-11ea-ad9e-fa163e35f5d4/memory.kmem.limit_in_bytes 实际使用中，我们一般将 memory.kmem.limit_in_bytes 设置成大于 memory.limit_in_bytes，从而只限制应用的总内存使用。\ndocker与k8s使用kmem 以上描述都是cgroup层面即机器层面，但是 runc 和 docker 发现有这个属性之后，在后来的版本中也支持了 kmem ，k8s 发现 docker支持，也在 1.9 版本开始支持。\n1.9版本及之后，kubelet 才开启 kmem 属性\nkubelet 的这部分代码位于：\nhttps://github.com/kubernetes/kubernetes/blob/release-1.12/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go#L70-L106 对于k8s、docker 而言，kmem 属性属于正常迭代和优化，至于3.x的内核上存在 bug 不能兼容，不是k8s 关心的问题。但 issue 中不断有人反馈，因此在 k8s 1.14 版本的 kubelet 中，增加了一个编译选项 make BUILDTAGS=“nokmem”，就可以编译 kubelet 时就禁用 kmem，避免掉这个问题。而1.8 到1.14 中间的版本，只能选择更改 kubelet 的代码。\n","date":"2022-10-08T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/MemoryLeakageAnalysis/","title":"Kubernetes低版本中内存泄漏问题"},{"content":"Ansible-with_items 通过with_items进行循环 语法\n{{ item }}: 为读取with_items的固定写法 with_items: 是一个列表,下面可以有多个不同的内容 - hosts: test remote_user: root gather_facts: false vars_files: ./public_vars.yaml tasks: - name: Services Http start service: name={{ item }} state=started with_items: - httpd - firewalld 普通写法 - hosts: test remote_user: root gather_facts: false vars_files: ./public_vars.yaml tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Install httpd yum: name=\u0026#34;httpd\u0026#34; state=present - name: Services Http start service: name={{ item }} state=started with_items: - httpd - firewalld 使用变量的循环写法 - hosts: test remote_user: root gather_facts: true tasks: - name: Install httpd yum: name={{ packages }} state=present vars: packages: - httpd - pcre-devel 使用变量字典循环方式批量创建用户 - hosts: test remote_user: root gather_facts: false vars_files: ./public_vars.yaml tasks: - name: Add Users user: name={{ item.name }} groups={{ item.groups }} state=present with_items: - { name: \u0026#34;alex\u0026#34;,groups: \u0026#34;test\u0026#34;} - { name: \u0026#34;alex1\u0026#34;,groups: \u0026#34;test\u0026#34;} 使用变量字典循环拷贝文件 - hosts: test remote_user: root gather_facts: false tasks: - name: Add Users copy: src: \u0026#39;{{ item.src }}\u0026#39; dest: \u0026#39;{{ item.dest }}\u0026#39; mode: \u0026#39;{{ item.mode }}\u0026#39; with_items: - { src: \u0026#34;./1.txt\u0026#34;, dest: \u0026#34;/tmp\u0026#34;, mode: 0644} - { src: \u0026#34;./2.txt\u0026#34;, dest: \u0026#34;/tmp\u0026#34;, mode: 0644} Ansible-Handlers 通过notify进行监控-\u0026gt;通过handlers触发 关于Handler的一些小注意事项\n无论你拥有多少个notify通知相同的handlers,handlers仅仅会在所有tasks正常执行完成后运行一次\n只有tasks发生改变了才会通知handlers,没有改变则不会触发handlers\n不能使用handlers替代tasks,因为handlers是一个特殊的tasks\nnotify的名称要与handlers`的名称一致\n- hosts: test remote_user: root gather_facts: false tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Install httpd yum: name=\u0026#34;httpd\u0026#34; state=present notify: Debug Message register: install_info handlers: - name: Debug Message debug: msg: \u0026#39;{{install_info}}\u0026#39; Ansible-Tags 根据playbook中的指定标签的内容进行执行、调试等操作.\n对一个tasks指定一个tags标签 对一个tasks指定多个tags标签(真没啥意义,感觉不实用。) 对多个tasks指定一个标签 执行指定Tags标签内容 tags: 指定标签名称 - hosts: test remote_user: root gather_facts: false tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: - authorized_key_hosts -t: 通过-t选项参数进行选择指定标签进行运行 ansible-playbook 1.yaml -t authorized_key_hosts -i hosts 跳过指定标签执行其他内容 跳过指定的标签内容,执行标签内容外的其他内容 ansible-playbook 1.yaml --skip-tags \u0026#34;authorized_key_hosts\u0026#34; -i hosts Ansible-Include 一个可以将playbook简单的进行复用的一个功能!\n简单应用 编写一个重启http服务的配置\n- name: Start HTTP service: name=httpd state=restarted PlayBook中的应用\ninclude： 查找的文件目录为你当前所在的目录,可以通过pwd命令进行查看。 - hosts: test remote_user: root gather_facts: true tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Install HTTP yum: name=\u0026#34;httpd\u0026#34; state=present - name: Restart HTTP include: starthttp.yaml # 包含你刚刚写的配置 # include_tasks: starthttpd.yaml # 两种写法都可以 多个playbook合成 如果你写的playbook存在多个文件,你只想执行一个playbook,那么可以使用import_playbook。\nimport_playbook: 引入你需要的playbook文件,必须是一个完整的playbook文件 - import_playbook: ./tasks1.yaml - import_playbook: ./tasks2.yaml Ansible-ignore_errors 在Ansible中进行错误的忽略\n- hosts: test remote_user: root gather_facts: true tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Ingoring command: /bin/false ignore_errors: yes 输出结果大概是这样的\nPLAY [test] TASK [Ingoring] ***************************************************************************************************************************************************************************************************************************fatal: [10.1.6.5]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: [\u0026#34;/bin/false\u0026#34;], \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.026834\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-08-29 02:17:00.089749\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;non-zero return code\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;start\u0026#34;: \u0026#34;2022-08-29 02:17:00.062915\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout_lines\u0026#34;: []} ...ignoring PLAY RECAP ********************************************************************************************************************************************************************************************************************************10.1.6.5 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=1 Ansible-changed_when 通常用于失败后所执行一些操作: 例如失败后强制调用handlers、失败后强制删除等.\n通常而言，如果任务失败并且play在该主机上中止，则收到play中早前任务通知的处理程序将不会运行。如果在play中设置force_handlers: yes关键字，则即使play因为后续任务失败而中止也会调用被通知的处理程序。\nforce_handlers - hosts: test remote_user: root gather_facts: true force_handlers: yes # 强制调用handlers tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Test False command: echo \u0026#34;This is Force \u0026#34; notify: Restart HTTP service - name: Install available yum: name=dbdbdb state=present handlers: - name: Restart HTTP service service: name=httpd state=restarted 虽然任务是失败的,但是依旧调用了最后执行的handlers\n[root@localhost ansible_linux]# ansible-playbook 1.yaml -i hosts TASK [Install available] ******************************************************************************************************************************************************************************************************************fatal: [10.1.6.57]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;failures\u0026#34;: [\u0026#34;No package dbdbdb available.\u0026#34;], \u0026#34;msg\u0026#34;: \u0026#34;Failed to install some of the specified packages\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;results\u0026#34;: []} RUNNING HANDLER [Restart HTTP service] ****************************************************************************************************************************************************************************************************changed: [10.1.6.57] changed_when 当前命令确保不会对被控端主机进行变更的时候,可以使用changer_when来进行忽略提示中的changed\n- hosts: test remote_user: root gather_facts: true force_handlers: yes # 强制调用handlers tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Check HTTP shell: ps -aux | grep httpd changed_when: false # 这样ps -aux | grep httpd 再也不会提示changed - name: Message debug: msg: \u0026#34;{{ ansible_distribution }}\u0026#34; changed_when还可以检查tasks任务返回的结果\n查找输出当中是否存在successfuly如果没有则不执行 ","date":"2022-08-29T00:00:00Z","permalink":"http://localhost:1313/ansible/AnsibleTaskControll/","title":"Ansible-任务控制"},{"content":"Ansible怎么定义变量 通过playbook中的play进行变量的定义 通过inventory主机清单进行变量定义 通过执行playbook的时候增加-e选项进行定义 通过Playbook中的vars定义变量 在Playbook中通过写入vars语法定义变量 通过{{变量名}}进行引用! - hosts: test remote_user: root vars: - httpd_package: httpd tasks: - name: Install DepencyEnvorment yum: name: {{httpd_package}} state: present update_cache: yes 通过定义变量文件进行使用 定义一个名字为public_vars.yaml的变量配置文件 depence: [\u0026#39;openssl-devel\u0026#39;,\u0026#39;pcre-devel\u0026#39;,\u0026#39;zlib-devel\u0026#39;] 注意: 当你引用了变量文件中的变量,请在读取变量的时候增加双引号\u0026quot;\u0026quot;\n- hosts: test remote_user: root vars_files: - ./public_vars.yaml - ./public_vars2.yaml # 如果是多个变量的话 tasks: - name: \u0026#34;Install De\u0026#34; yum: name: \u0026#34;{{depence}}\u0026#34; # 通过双引号去引入变量内容,不然会报错 state: present update_cache: no 通过编辑inventory主机清单进定义 这种方法一般用的很少 [test] 10.1.6.205 [test:vars] file_name=group_sys 官方推荐的方法: 在项目目录中创建两个变量目录host_vars和group_vars\ngroup_vars mkdir host_vars; mkdir group_vars 创建一个同名文件,用于写入变量内容\n必须与hosts清单中的组名保持一致,如果不同名会报错。但是如果你想要多个配置文件使用同一个组中的变量,只需要在group_vars/all新建一个all文件,所有组可用!\n[root@bogon ~]# cat group_vars/test file_name: group_sys host_vars 在host_vars中创建一个文件,文件名与inventory清单中的主机名称要保持完全一致,如果是IP地址,则创建相同IP地址的文件即可 vim host_vars/10.1.6.205 [root@bogon ~]# cat host_vars/10.1.6.205 file_name: group_sys ","date":"2022-08-15T00:00:00Z","permalink":"http://localhost:1313/ansible/AnsibleVarbles/","title":"Ansible变量相关内容"},{"content":"MySQL性能优化-优化思路 大概的优化思路分为以下几个内容\n硬件层面优化 系统层面优化 MySQL版本选择优化 MySQL三层结构及参数优化 MySQL开发规范 MySQL的索引优化 MySQL的事务以及锁优化 MySQL架构优化 MySQL安全优化 PS: 优化是有风险的,如果你要优化就要变更。\n硬件层面优化 这个地方就略过了就是一些加大硬件配置的需求.\n系统层面优化 id: 空闲状态,如果数值越大,表示空闲状态越多。如果可能达到0的情况下,表示当前CPU的核心处于满负荷状态。 us: 表示当前CPU核心数量的使用率。 sy: 表示CPU与内核交互的频率,内核与CPU处理请求的占用,如果此参数高,表示内核很忙。 wa: CPU从内存中刷数据到硬盘中的占用,可能会出现I/O的问题。 [root@mysql-master ~]# top top - 15:05:11 up 35 days, 5:54, 2 users, load average: 0.00, 0.01, 0.05 Tasks: 225 total, 2 running, 223 sleeping, 0 stopped, 0 zombie %Cpu0 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu1 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu2 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu3 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu4 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu5 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu6 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu7 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 24522416 total, 14931524 free, 3675344 used, 5915548 buff/cache KiB Swap: 12386300 total, 12386300 free, 0 used. 20450988 avail Mem 通过 top -Hp 10380 指定占用高的进程,可以看到具体是那些线程占用过高\n假设 1893线程占用过高,可以从数据库中查看performance_schema库中具体的信息\n定位操作系统线程-\u0026gt;从系统线程中定位数据库线程\n*************************** 38. row *************************** THREAD_ID: 128014 NAME: thread/sql/one_connection TYPE: FOREGROUND PROCESSLIST_ID: 127988 PROCESSLIST_USER: ooooo PROCESSLIST_HOST: 192.168.0.1 PROCESSLIST_DB: test PROCESSLIST_COMMAND: Sleep PROCESSLIST_TIME: 104 PROCESSLIST_STATE: NULL PROCESSLIST_INFO: ** PARENT_THREAD_ID: NULL ROLE: NULL INSTRUMENTED: YES HISTORY: YES CONNECTION_TYPE: SSL/TLS THREAD_OS_ID: 16165 *************************** 39. row *************************** 如果可能存在的是IO问题 查询MySQL中的sys库中存在记录IO的表 如果存在IO问题： 可以选择用内存换取时间的方法…\nmysql\u0026gt; use sys mysql\u0026gt; show tables; | x$io_by_thread_by_latency | | x$io_global_by_file_by_bytes | | x$io_global_by_file_by_latency | | x$io_global_by_wait_by_bytes | | x$io_global_by_wait_by_latency | MySQL版本选择优化 在这里…笔者非常推荐MySQL8.0x!!! 同样的机器,8.0比5.7快2.5倍左右吧\n选择稳定版,选择开源社区的稳定版和GA版本 选择MySQL数据库GA版本发布后6-12个月的GA双数版本 要选择开发兼容的MySQL版本 MySQL三层结构及参数优化 连接层优化 一切根据自己或者项目需要自由设置吧!\nmax_connections = 1000 max_connect_errors = 999999 wait_timeout = 600 interactive_wait_timeout = 3600 net_read_timeout = 120 new_write_timeout = 120 max_allowed_packet = 500M Server层优化 一切根据自己或者项目需要自由设置吧!\nsort_buffer_size = 8M sql_safe_updates = 1 slow_query_log = 1 long_query_time = 1 slow_query_log_file = /data/mysql/mysql-slow.log log_queries_not_using_indexes = 10 read_buffer_size = 2M read_rnd_buffer_size = 8M sort_buffer_size = 8M join_buffer_size = 8M key_buffer_size = 16M max_binlog_size = 500M max_execution_time = 28800 log_timestamps = SYSTEM init_connect = \u0026#34;set names utf8mb4\u0026#34; binlog_format = ROW event_scheduler = OFF lock_wait_timeout = sync_binlog = 1 Engine层优化 一切根据自己或者项目需要自由设置吧!\ntransaction-isolation = \u0026#34;READ-COMMITIED\u0026#34; innodb_data_home_dir = /xxx innodb_log_group_home_dir = /xxx innodb_log_file_size = 2048M innodb_log_files_in_group = 3 innodb_flush_log_at_trx_commit = 2 innodb_flush_method = O_DIRECT innodb_io_capacity = 1000 innodb_io_capacity_max = 4000 innodb_buffer_pool_size = 64G innodb_buffer_pool_instances = 4 innodb_log_buffer_size = 64M innodb_max_dirty_pages_pct = 85 全局锁读Global Read Lock (GRL） 加锁方法：FTWRL,flush tables with read lock.\n解锁方法：unlock tables；\n可能出现的场景\n记录binlog日志-\u0026gt;不让所有事务提交\nFTWRL-\u0026gt;不让新的修改进入\nsnapshot innodb-\u0026gt; 允许所有的DML语句,但是不允许DDL\n属于类型: MDL(matedatalock)层面锁\n影响情况: 加锁的期间,阻塞所有事务的写入,阻塞所有事务的commit,时间受到lock_wait_timeout=315336000\n全局读锁的排查方法 MySQL [(none)]\u0026gt; USE performance_schema MySQL [performance_schema]\u0026gt; # 5.6需要手动开启 MySQL [performance_schema]\u0026gt; UPDATE setup_instruments SET ENABLED = \u0026#34;YES\u0026#34;,TIMED = \u0026#34;YES\u0026#34; WHERE NAME=\u0026#39;wait/lock/metadata/sql/mdl\u0026#39;; # 查看是否有阻塞问题 MySQL [performance_schema]\u0026gt; SELECT * FROM metadata_locks; mysql\u0026gt; SELECT OBJECT_SCHEMA,OBJECT_NAME,LOCK_TYPE,LOCK_DURATION,LOCK_STATUS,OWNER_THREAD_ID,OWNER_EVENT_ID FROM performance_schema.metadata_locks; 5.7版本全局读锁排查 mysql\u0026gt; SHOW proceslist\\G; mysql\u0026gt; SELECT * FORM sys.schema_table_lock_waits; 经典故障案例 假设模拟一个大的查询或者事物 模拟备份时的TWRL,此时会发现命令阻塞 发起正常查询请求,发现查询被阻塞 5.7版本的Xbackup/mysqldump备份数据库出现锁表状态,所有的查询不能正常进行.\nSELECT *,SLEEP(100) FORM `user` WHERE username = \u0026#39;test1\u0026#39; for update; flush tables with read lock; SELECT * FROM icours.user where username = \u0026#39;test\u0026#39; for update Table Lock(表级锁) 加锁方式: lock table t1 read; 所有会话只读,属于MDL锁。lock table write; 当前会话可以可以RW,属于MDL锁. SELECT FOR UPDATE; SELECT FOR SHARE 解锁方式: unlock tables; 检测方式 [mysqld] performance-schema-instrument = \u0026#39;wait/lock/metadata/sql/mdl=ON\u0026#39; SELECT * FROM performance_schema.metadata_locks; SELECT * FROM performance_schema.threads; MetaDataLock(元数据锁) 作用范围: global、commit、tablespace、schema、table 默认时间： lock_wait_timeout mysql\u0026gt; select @@lock_wait_timeout; +---------------------+ | @@lock_wait_timeout | +---------------------+ | 31536000 | +---------------------+ 1 row in set (0.00 sec) 检测方式 [mysqld] performance-schema-instrument = \u0026#39;wait/lock/metadata/sql/mdl=ON\u0026#39; SELECT * FROM performance_schema.metadata_locks; // 找到阻塞的Id OWNER_THREAD_ID = 12 mysql\u0026gt; SELECT * FROM threads where thread_id = \u0026#39;12\u0026#39;\\G; kill 12; AutoincLock(自增锁) 通过参数: innodb_autoinc_lock_mod = 0 | 1 | 2 0 表锁：每次插入都请求表锁,效率低下 1 mutex： 预计插入多少行,预申请自增序列.如果出现load或者insert select方式会退化为0。 2 : 强制使用mutex的方式,并发插入会更高效！ Innodb Row Lock(行级锁) record lock、gap、next、lock MySQL [(none)]\u0026gt; SHOW STATUS LIKE \u0026#39;innodb_row_lock\u0026#39;; MySQL [information_schema]\u0026gt; SELECT * FROM information_schema.innodb_trx; MySQL [information_schema]\u0026gt; SELECT * FORM sys.schema_table_lock_waits; MySQL [information_schema]\u0026gt; SELECT * FROM performance_schema.threads; MySQL [information_schema]\u0026gt; SELECT * FROM performance_schema.events_statements_current; MySQL [information_schema]\u0026gt; 优化方向 优化索引 减少事务的更新范围 RC级别 拆分语句 // 假设 k1是辅助索引 update t1 set num=num+10 where k1\u0026lt;100; // 改为 select id from t1 where k1\u0026lt;100; update t1 set num=num+10 where id in (20,30,50) Dead Lock死锁 dead lock 多个并发事务之间发生交叉依赖的时候,会出现死锁.\nSHOW ENGINE innodb STATUS\\G; innodb_print——all_deadlocks =1 // 开启记录死锁日志 ","date":"2022-07-20T00:00:00Z","permalink":"http://localhost:1313/mysql/MysqlOptimization/","title":"MySQL小小优化思路简单版本"},{"content":"Docker迁移存储目录 问题起因 由于公司最开始的服务器在/var/lib/docker没有挂载存储,容量只有40G,导致服务器磁盘用满。现将原有的Docker目录数据进行迁移。\n请各位Kubernetes用户不要操作,因为容器编排不支持!\n# 启动容器发现如下报错 ERROR：cannot create temporary directory! 方法一: 软连接方式 # 1.停止docker服务 systemctl stop docker # 2.开始迁移目录 mv /var/lib/docker /data/ # 使用cp命令也可以 cp -arv /var/lib/docker /data/docker # 3.添加软链接 ln -s /data/docker /var/lib/docker # 4.启动docker服务 systemctl start docker 方法二: 修改docker配置文件 注意: 这是一个旧版本docker修改存储目录的方式.\nvim /etc/docker/daemon.json { \u0026#34;graph\u0026#34;: [ \u0026#34;/data/docker/\u0026#34; ] # 更改docker镜像的存储目录 } 新版本修改存储目录方式\n# 请找到你的docker.service存放位置 vim /usr/lib/systemd/system/docker.service 通过加入--data-root=/data/docker进行修改默认的数据存储位置\n[Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd --data-root=/data/docker ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target 修改完成之后重启docker\nsystemctl daemon-reload # 重载service配置 systemctl restart docker Docker存储空间不足 问题一: No space left on device 问题描述：docker run 的时候系统提示No space left on device!\n这个问题无非就两种情况\n磁盘满了\n磁盘inode满了\n因为 ext3 文件系统使用 inode table 存储 inode 信息，而 xfs 文件系统使用 B+ tree 来进行存储。考虑到性能问题，默认情况下这个 B+ tree 只会使用前 1TB 空间，当这 1TB 空间被写满后，就会导致无法写入 inode 信息，报磁盘空间不足的错误。我们可以在 mount 时，指定 inode64 即可将这个 B+ tree 使用的空间扩展到整个文件系统。\n# 查看inde信息 df -i # 删除过多的小文件即可 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda3 593344 56998 536346 10% / tmpfs 238282 1 238281 1% /dev/shm /dev/sda1 51200 39 51161 1% /boot /tmp/1m 128 128 0 100% /app/logs 如果不知道小文件如何查找\n# 查找系统中 目录大小大于1M（目录一般大小为4K，所以目录要是大了那么文件必然很多） find / -size +4k -type d |xargs ls -ldhi 如果是硬盘空间满了的话\n# 查看磁盘使用容量 df -h # 查看到具体哪个目录满了,然后配合 du -sh命令解决即可 Filesystem Size Used Avail Use% Mounted on /dev/sda3 8.8G 8.8G 0 100% / tmpfs 931M 0 931M 0% /dev/shm /dev/sda1 190M 40M 141M 22% /boot 优雅地重启Docker 不停止重启,重启docker是一件多么美妙的事情!\n当 Docker 守护程序终止时，它会关闭正在运行的容器。从 Docker-ce 1.12 开始，可以在配置文件中添加 live-restore 参数，以便在守护程序变得不可用时容器保持运行。需要注意的是 Windows 平台暂时还是不支持该参数的配置。\nvim /etc/docker/daemon.json { \u0026#34;live-restore\u0026#34;: true } 在守护进程关闭的时候保持容器运行\n# 重载docker服务 systemctl reload docker.service [root@VM-0-9-centos ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e58a220f03c3 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 5 minutes ago Up 15 seconds 80/tcp web # 这个时候重启docker服务,web服务并没有停止工作 [root@VM-0-9-centos ~]# systemctl restart docker [root@VM-0-9-centos ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e58a220f03c3 nginx \u0026#34;/docker-entrypoint.…\u0026#34; 7 minutes ago Up About a minute 80/tcp web live-restore的限制 当前的Live Restore特性可以在进行Daemon维护，或者在Daemon发生问题导致不可用的情况，减少容器的停机时间，不过其也有一定的限制。\nDocker版本升级限制 Live Restore仅支持Docker补丁版本升级时可用，也就是 YY.MM.x 最后一位发生变化的升级，而不支持大版本的升级。在进行大版本升级后，可能会导致Daemon无法重新连接到运行中容器的问题，这时候需要手动停止运行的容器。 Daemon选项变更 也就是说Live Restore仅仅在某些Daemon级别的配置选项不发生改变的情况工作，例如Bridge的IP地址，存储驱动类型等。如果在重启Daemon时候，这些选项发生了改变，则可能会到Daemon无法重新连接运行中的容器，这时也需要手动停止这些容器。 影响容器的日志输出 如果Daemon长时间停止，会影响运行容器的日志输出。因为默认情况下，日志管道的缓冲区大小为64k，当缓冲写满之后，必须启动Daemon来刷新缓冲区。 不支持Docker Swarm Live Restore只是独立Docker引擎的特性，而Swarm的服务是由Swarm管理器管理的。当Swarm管理器不可用时，Swarm服务是可以在工作节点上继续运行的，只是不同通过Swarm管理器进行管理，直到Swarm管理恢复工作。 容器内部中文异常 问题描述: 容器内部中文乱码、无法正常显示中文、\n例如显示中文：--------��� # 查看容器内部编码 root@e58a220f03c3:/# locale -a C C.UTF-8 POSIX 然而 POSIX 字符集是不支持中文的，而 C.UTF-8 是支持中文的只要把系统中的环境 LANG 改为 \u0026quot;C.UTF-8\u0026quot; 格式即可解决问题。同理，在 K8S 进入 pod 不能输入中文也可用此方法解决。\nexport LANG=zh_CN.UTF-8 ","date":"2022-07-14T00:00:00Z","permalink":"http://localhost:1313/docker/DockerFrequentlyAskedQuestions/","title":"Docker常见的几个问题处理"},{"content":" playbook是由一个或多个\u0026quot;play\u0026quot;组成的列表 playbook的主要功能在于将预定义的一组主机，装扮成事先通过ansible中的task定义好的角色。 Task实际是调用ansible的一个module，将多个play组织在一个playbook中， 即可以让它们联合起来，按事先编排的机制执行预定义的动作 Playbook采用YAML语言编写 --- - hosts: test # 指定主机列表 remote_user: root # 远程操作以什么身份执行 tasks: - name: Install Redis # 提示字段,表示当前处于什么进度 command: install redis # 当前执行的具体命令操作 1.0 PlayBook核心元素 Hosts：playbook中的每一个play的目的都是为了让特定主机以某个指定的用户身份执行任务,hosts用于指定要执行指定任务的主机，须事先定义在主机清单中.详细请看 remote_user: 可用于Host和task中。也可以通过指定其通过sudo的方式在远程主机上执行任务，其可用于play全局或某任务.此外，甚至可以在sudo时使用sudo_user指定sudo时切换的用户. varniables: 内置变量或自定义变量在playbook中调用 Templates模板 : 可替换模板文件中的变量并实现一些简单逻辑的文件 Handlers和notify: 结合使用，由特定条件触发的操作，满足条件方才执行，否则不执行 tags: 指定某条任务执行，用于选择运行playbook中的部分代码. ansible-playbook -C hello.yaml -C 选项检查剧本是否成功,并不实际执行 1.0.1 忽略错误信息 也可以使用ignore_errors来忽略错误信息\ntasks: - name: run this shell: /usr/bin/ls || /bin/true ignore_errors: True 1.0.2 常用选项 --check: 只检测可能会发生的改变,但是不会执行 --list-hosts: 列出运行任务的主机 --limit: 主机列表,只针对主机列表中的主机执行 -v: 显示过程 --list-tasks: 查看任务列表 ansible-playbook hello.yaml --check ansible-playbook hello.yaml --list-hosts ansible-playbook hello.yaml --limit 10.1.6.111 2.0 Handlers和notify 由于playbook执行会有次序问题,所以当出现次序问题的时候,可以使用handlers结合notify\nHandlers: 是task列表,这些task与前述的task没有本质的区别,用于当不同的资源发生变化的时候,才会采取一定的操作. Notify: 此action可以用在每个play的最后被触发,这样可以避免多次有改变的发生时每次都执行指定的操作,仅仅在所有变化发生完后,一次性执行制定操作,在notify中列出的操作称为hendler，也就是notify中定义的操作. Handlers和notify可以写多个\n--- - hosts: test remote_user: root tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/newfile state=touch - name: \u0026#34;create new user\u0026#34; user: name=test2 system=yes shell=/sbin/nologin - name: \u0026#34;install httpd\u0026#34; yum: name=httpd state=installed notify: restart service # 表示执行完yum操作以后需要执行handlers的操作 - name: \u0026#34;copy log\u0026#34; copy: src=/var/log/httpd/error_log dest=/data handlers: - name: restart service service: name=httpd state=restarted 3.0 PlayBook的tags使用 给特定的内容打上tags可以单独的执行标签内容 --- - hosts: test remote_user: root tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/newfile state=touch tags: newfile - name: \u0026#34;create new user\u0026#34; user: name=test2 system=yes shell=/sbin/nologin tags: newuser - name: \u0026#34;install httpd\u0026#34; yum: name=httpd state=installed notify: restart service # 表示执行完yum操作以后需要执行handlers的操作 - name: \u0026#34;copy log\u0026#34; copy: src=/var/log/httpd/error_log dest=/data handlers: - name: restart service service: name=httpd state=restarted ansible-playbook -t newfile test.yaml # 表示只执行newfile标签的动作 ansible-playbook -t newfile,newuser test.yaml # 表示只执行newfile标签的动作 4.0 PlayBook中变量的使用 变量名：仅能由字母、数字和下划线组成，且只能以字母开头 变量的来源 通过setup模块 在/etc/ansible/hosts中定义 普通变量：主机组中的主机单独定义,优先级高于公共变量 公共变量：针对主机组所有主机定义统一变量 通过命令行指定变量： 优先级最高 4.0.1 通过命令行指定变量 --- - hosts: test remote_user: root tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/{{filename}} state=touch tags: newfile ansible-playbook -e \u0026#39;filename=app1\u0026#39; # /data/app1 4.0.2 在playbook中定义 # 在playbook中定义 --- - hosts: test remote_user: root vars: - filename: app1 tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/{{filename}} state=touch tags: newfile 4.0.3 通过setup模块获取变量 ansible setup facts 远程主机的所有变量都可直接调用 (系统自带变量) setup模块可以实现系统中很多系统信息的显示 ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_nodename\u0026#34;\u0026#39; 查询主机名 ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_memtotal_mb\u0026#34;\u0026#39; 查询主机内存大小 ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_distribution_major_version\u0026#34;\u0026#39; 查询系统版本 ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_processor_vcpus\u0026#34;\u0026#39; 查询主机cpu个数 4.0.4 在hosts中定义变量 定义主机组单独的变量 [test] 192.168.1.1 http_port=81 192.168.1.2 http_port=82 --- - hosts: test remote_user: root tasks: - name: \u0026#34;create new file\u0026#34; hostname: name=www{{http_port}}.baidu.com 定义公共变量 # 针对test主机组当中的所有主机都有效 [test:vars] nodename=www domain=baidu.com 4.0.5 通过文件加载变量 # vars.yaml filename: applications # playbook.yaml - hosts: test remote_user: root vars_files: - vars.yaml tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/{{filename}} 5.0 模板Templates 采用Jinja2语言，使用字面量，有下面形式 数字：整数，浮点数 列表：[item1, item2, …] 元组：(item1, item2, …) 字典：{key1:value1, key2:value2, …} 布尔型：true/false 算术运算：+, -, *, /, //, %, ** 比较操作：==, !=, \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;= 逻辑运算：and，or，not 流表达式：For，If，When # For more information on configuration, see: # * Official English Documentation: http://nginx.org/en/docs/ # * Official Russian Documentation: http://nginx.org/ru/docs/ user nginx; worker_processes {{ansible_processor_vcpus**2}}; # 例如,你可以将nginx核心数动态的设置为主机的CPU数量 error_log /var/log/nginx/error.log; pid /run/nginx.pid; 5.0.1 When语法 条件测试:如果需要根据变量、facts或此前任务的执行结果来做为某task执行与否的前提时要用到条件测试, 通过when语句实现，在task中使用，jinja2的语法格式 在task后添加when子句即可使用条件测试；when语句支持Jinja2表达式语法 当ansible_distribution=CentOS的时候才会去执行template\n--- - hosts: test remote_user: root tasks: - name: \u0026#34;Install Nginx\u0026#34; yum: name=nginx - name: Config conf template: src=/templates/nginx.conf.j2 dest=/etc/nginx/nginx.conf when: ansible_distribution == \u0026#34;CentOS\u0026#34; - name: start nginx service: name=nginx state=started enabled=yes 5.0.2 With_item 迭代写法 --- - hosts: test remote_user: root tasks: - name: \u0026#34;Create new file\u0026#34; file: name=/data/{{items}} state=touch with_items: - app1 - app2 - app3 迭代嵌套子变量 - hosts: test remote_user: root tasks: - name: \u0026#34;Create new file\u0026#34; file: name=/data/{{item.name}}_{{item.date}} state=touch with_items: - {name: \u0026#39;app1\u0026#39;, date: \u0026#39;2022\u0026#39;} 5.0.3 for循环 --- - hosts: test remote_user: root vars: ports: - 81 - 82 - 83 tasks: - name: copy template template: src=/root/templates/for.j2 dest=/data/for.conf # 或者 --- - hosts: test remote_user: root vars: ports: - listen:81 - listen:82 - listen:83 tasks: - name: copy template template: src=/root/templates/for.j2 dest=/data/for.conf # 或者 --- - hosts: test remote_user: root vars: config: - host1: port: 81 name: host1.do.com rootdir: /root/ tasks: - name: copy template template: src=/root/templates/for.j2 desc=/data/for.conf 创建一个模板文件\n{%for i in ports%} server { listen {{i}} } {%endfor%} # 或者 {%for i in ports%} server { listen {{i.listen}} } {%endfor%} # 或者 {%for i in ports%} server { listen {{ i.port }} name {{ i.name }} } {%endfor%} 5.0.4 if判断 {%for i in ports%} server { listen {{ i.port }} {% if i.name is defind %} name {{ i.name }} {% endif %} } {%endfor%}a ","date":"2022-04-01T00:00:00Z","permalink":"http://localhost:1313/posts/playbook/","title":"Playbook的一些简单使用"},{"content":"配置nginx的work_process 查看当前服务的CPU核心数量\n[root@containerd-master1 ~]# grep processor /proc/cpuinfo | wc -l 8 如果你需要修改更多的工作进程,请修改配置文件中的work_process字段\nauto: 根据系统的CPU自动的设置工作进程数量 worker_processes 1; # 可选值 auto 配置work_connections 该参数表示每个工作进程最大处理的连接数,CentOS默认连接数为1024,连接数是可以修改的。 如果需要修改ulimit参数,请修改配置文件/etc/security/limits.conf\nnoproc 是代表最大进程数 nofile 是代表最大文件打开数 本次修改仅仅以Rocky Linux和CentOS为例,不同的系统修改方法可能有所差异.\n* soft nofile 65535 * hard nofile 65535 配置nginx当中的work_connections\nevents { worker_connections 65535; use epoll; } 简单的提一嘴ulimit的作用: 当进程打开的文件数目超过此限制时，该进程就会退出。\n启用gzip压缩 nginx使用 gzip 进行文件压缩和解压缩,您可以节省带宽并在连接缓慢时提高网站的加载时间。\nserver { gzip on; # 开启gzip gzip_vary on; gzip_min_length 10240; gzip_proxied expired no-cache no-store private auth; gzip_types text/plain text/css text/xml text/javascript application/x-javascript application/xml; gzip_disable \u0026#34;MSIE [1-6]\\.\u0026#34;; } 限制nginx连接的超时 主要是为了减少打开和关闭连接时的处理器和网络开销\nclient_body_timeout: 该指令设置请求体（request body）的读超时时间。仅当在一次readstep中，没有得到请求体，就会设为超时。超时后，nginx返回HTTP状态码408 client_header_timeout: 指定等待client发送一个请求头的超时时间（例如:GET / HTTP/1.1)仅当在一次read中，没有收到请求头，才会被记录为超时 keepalive_timeout: 指定了与client的keep-alive连接超时时间,超过这个时间后,服务器会关关闭连接。 send_timeout: 指定客户端的响应超时时间。这个设置不会用于整个转发器，而是在两次客户端读取操作之间。如果在这段时间内，客户端没有读取任何数据，nginx就会关闭连接。 http { client_body_timeout 12; client_header_timeout 12; keepalive_timeout 15; send_timeout 10; } 调整缓冲区大小 调整nginx缓冲区以优化服务器性能。如果缓冲区大小太小，那么nginx将写入一个临时文件，导致大量 I/O 操作不断运行。\nhttp { client_body_buffer_size 10K; client_header_buffer_size 1k; client_max_body_size 8m; large_client_header_buffers 4 4k; } 启用日志访问缓冲区 日志很重要，因为它们有助于解决问题。完全禁用日志不是一个好的做法。在这种情况下，您可以启用访问日志缓冲。这将允许nginx缓冲一系列日志并将它们一次写入日志文件，而不是对每个请求应用不同的日志操作。在nginx配置文件中添加以下行以允许访问日志缓冲\nhttp { log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; # 开启日志缓冲设置 access_log logs/access.log main buffer=32k flush=1m; } 调整静态文件缓存 # 静态文件缓存内容 location ~* \\.(jpg|jpeg|png|gif|ico|css|js)$ { expires 90d; } ","date":"2022-03-20T00:00:00Z","permalink":"http://localhost:1313/posts/nginx%E4%BC%98%E5%8C%96/","title":"Nginx简单的常规优化"},{"content":"先前了解 参考链接 Githubissue kubelet中的Docker支持现在已弃用，并将在未来的版本中删除。kubelet使用了一个名为dockershim的模块，该模块实现了对Docker的CRI支持，并在Kubernetes社区中发现了维护问题。我们鼓励您评估迁移到一个容器运行时的情况，该容器运行时是CRI（v1alpha1或v1兼容）的完整实现。\n也就是说,在后续的Kubernetes1.20x版本以后会删除dockershim组件,但是由于目前Docker的使用用户众多,中间必然会有替换的一个过渡期,所以大家可以更多的关注一下其他的Container Runtime。 例如我们的Podman、Containerd、cri-o等其他容器运行时来运行kubernetes。\n下面我们就具体来看看Kubernetes所提到的弃用dockershim到底是什么东西.\nCRI容器运行时接口 参考链接 CRI：容器运行时接口 container runtime interface，CRI 中定义了容器和镜像两个接口，实现了这两个接口目前主流的是：CRI-O、Containerd。（目前 PCI 产品使用的即为 Containerd）。 CRI接口的具体用处就在于\n对容器操作的接口，包括容器的创建、启动和停止.即create、stop等操作。 对镜像的操作，下载、删除镜像等. 即pull、rmi等操作。 podsandbox OCI开放容器标准 OCI：开放容器标准 open container initiative，OCI 中定义了两个标准：容器运行时标准 和 容器镜像标准，实现了这一标准的主流是：runc（也即我们日常说的 Docker）、Kata-Container。 OCI的作用在于\nImageSpec(容器标准包) 文件系统：以 layer 保存的文件系统，每个 layer 保存了和上层之间变化的部分，layer 应该保存哪些文件，怎么表示增加、修改和删除的文件等 config 文件：保存了文件系统的层级信息（每个层级的 hash 值，以及历史信息），以及容器运行时需要的一些信息（比如环境变量、工作目录、命令参数、mount 列表），指定了镜像在某个特定平台和系统的配置。比较接近我们使用 docker inspect \u0026lt;image_id\u0026gt; 看到的内容 manifest 文件：镜像的 config 文件索引，有哪些 layer，额外的 annotation 信息，manifest 文件中保存了很多和当前平台有关的信息 index 文件：可选的文件，指向不同平台的 manifest 文件，这个文件能保证一个镜像可以跨平台使用，每个平台拥有不同的 manifest 文件，使用 index 作为索引 2.runtimeSpec:\nociVersion(string, REQUIRED):是该州遵守的开放容器倡议运行时规范的版本。 id： 容器的 ID。这在此主机上的所有容器中必须是唯一的。不要求它在主机之间是唯一的。 status(string, REQUIRED): 加冕时容器的几个状态 1. creating 2. created 3. running 4. stopped pid: host上看到的容器进程 bundle：host上容器bundle目录的绝对路径 annotation：容器相关的标注，可选 所以在Json的序列化时,必须遵守以下格式\n{ \u0026#34;ociVersion\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;oci-container1\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;pid\u0026#34;: 4422, \u0026#34;bundle\u0026#34;: \u0026#34;/containers/redis\u0026#34;, \u0026#34;annotations\u0026#34;: { \u0026#34;myKey\u0026#34;: \u0026#34;myValue\u0026#34; } } Dockershim Dockershim 作用：把外部收到的请求转化成 docker daemon 能听懂的请求，让 Docker Daemon 执行创建、删除等容器操作。\n具体看一下kubelet是怎样创建容器的\nKubelet 通过 CRI 接口（gRPC）调用dockershim,请求创建一个容器。CRI 即容器运行时接口，这一步中，Kubelet 可以视作一个简单的CRI Client，而 dockershim 就是接收请求的 Server。目前dockershim是内嵌在 Kubelet 中的，所以接收调用就是 Kubelet 进程。 dockershim收到请求后，转化成 docker daemon的请求，发到docker daemon 上请求创建一个容器。 Docker Daemon 早在 1.12 版本中就已经将针对容器的操作移到另一个守护进程 containerd 中，因此 Docker Daemon 仍然不能帮我们创建容器，而是要请求 containerd 创建一个容器。 containerd 收到请求后，并不会自己直接去操作容器，而是创建一个叫做 containerd-shim 的进程，让 containerd-shim 去操作容器。是因为容器进程需要一个父进程来做诸如收集状态，维持 stdin 等 fd 打开等工作。而假如这个父进程就是 containerd，那每次 containerd 挂掉或升级，整个宿主机上所有的容器都得退出了。而引入了 containerd-shim 就规避了这个问题（containerd 和 shim 并不是父子进程关系）。 我们知道创建容器需要做一些设置 namespaces 和 cgroups，挂载 root filesystem 等等操作，而这些事该怎么做已经有了公开的规范，那就是 OCI。它的一个参考实现叫做 runC。于是，containerd-shim 在这一步需要调用 runC 这个命令行工具，来启动容器。 runC 启动完容器后本身会直接退出，containerd-shim 则会成为容器进程的父进程，负责收集容器进程的状态，上报给 containerd，并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理，确保不会出现僵尸进程。 参考链接 不支持docker我该何去何从？ ","date":"2022-02-13T00:00:00Z","permalink":"http://localhost:1313/docker/DockerShimRead/","title":"什么是dockershim"},{"content":" 此问题引出的是生产环境中所有的资源完全充足,但是会出现更新Pod、删除Pod、新建Pod无法调度的情况。\n生产环境解决问题办法 找到问题跟原所在,默认的maxPods: 110,K8S默认一个节点上的pod调度数是110，当前有限制pod数的需求。 vim /var/lib/kubelet/config.yaml\nmaxPods: 110 # 修改为maxPods: 330 影响Pod调度的情况 requests资源限制 requests：是一种硬限制,Kubernetes在进行Pod请求调度的时候,节点的可用资源必须满足500m的CPU才能进行调度,且使用最大限制为1个CPU,如果该Pod超过请求的最大限制,则Kubernetes将会把该Pod进行Kill重启。 resources: limits: cpu: \u0026#39;1\u0026#39; requests: cpu: 500m 当你设置request为500m以及limit为1000m的时候,当你使用 kubectl describe node查看节点资源的时候可能会与你设置的请求量不符合,这是以你Pod 的实际使用量为标准的。\n节点标签的Label 标签选择器： kubectl label node kubernetes-node1 env_role=dev 通过此命令对相应的节点加入标签 kubectl label node 节点名称 标签名称 spec: nodeSelector: env_role: dev 当然,你也可以通过kubectl get node --show-labels命令查看当前节点的标签\nNAME STATUS ROLES AGE VERSION LABELS master1 Ready,SchedulingDisabled master 141d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master1,kubernetes.io/os=linux,node-role.kubernetes.io/master= master2 Ready,SchedulingDisabled master 139d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master2,kubernetes.io/os=linux,node-role.kubernetes.io/master= master3 Ready,SchedulingDisabled master 139d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master3,kubernetes.io/os=linux,node-role.kubernetes.io/master= node1 Ready worker 141d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux,node-role.kubernetes.io/worker= node2 Ready worker 141d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux,node-role.kubernetes.io/worker= node3 Ready worker 141d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node3,kubernetes.io/os=linux,node-role.kubernetes.io/worker= 节点亲和性 节点亲和性：nodeAffinity和之前nodeSelector基本上是一样的,有的话满足进行调度,如果没有的话则依旧也可以调度。 硬亲和性：requiredDuringSchedulingIgnoreDuringExecution,当前约束的条件表示为在env_role这个键中有dev/test 有的话即满足的调度,如果不满足则不调度。 软亲和性: preferredDuringSchedulingIgnoredDuringExecution,进行尝试是否满足测试,如果满足则满足调度,如果不满足则依旧会进行调度。 支持的操作符：In/Not In/Gt/Lt/DoesNotExists分别为 存在、不存在、大于、小于、不存在。 spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoreDuringExecution: nodeSelectorTerms: - metchExpressions: - key: env_role operator: In values: - dev - test preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 # 表示权重 比例 preference: matchExpressions: - key: group operator: In # 操作符 In values: - otherprod 污点和污点容忍 污点：nodeSelector和nodeAffinityPod调度在某些节点上,是属于Pod的属性,在调度的时候进行实现,而污点是对节点做不分配调度,是节点属性。 污点容忍：当一个污点不允许被调度的时候,同时又想让他可能会参与调度,类似于软亲和性。 场景：作为专用节点、配置特定硬件节点、基于Taint驱逐 NoSchedule：一定不被调度 PreferNoSchdule: 尽量不被调度 NoExecute: 不调度,并且会驱逐在该节点上Pod # 污点容忍 spec: tolerations: - key: \u0026#34;env_role\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;yes\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 使用kubectl describe node kubernetes-master1 | grep Taints进行查看是否为污点。 使用kubectl taint node 节点名称 key=value:污点值\n","date":"2021-12-21T00:00:00Z","image":"https://bj.bcebos.com/baidu-rmb-video-cover-1/2b6495c8749e3f4e4369e28cb50eeb87.png","permalink":"http://localhost:1313/kubernetes/PodSchedulingIssues/","title":"有关于Kubernetes中影响Pod调度的问题"},{"content":" 注意：请各位记住把所有离线包全拿到本地…\n在线部署chartmuseum 直接使用最简单的 docker run 方式，使用local 本地存储方式，通过 -v 映射到宿主机 /opt/charts 更多支持安装方式见官网\nmkdir /opt/charts docker run -d \\ -p 8080:8080 \\ -e DEBUG=1 \\ -e STORAGE=local \\ -e STORAGE_LOCAL_ROOTDIR=/charts \\ -v /opt/charts:/charts \\ chartmuseum/chartmuseum:latest 下载Skywalking包 git clone https://github.com/apache/skywalking-kubernetes.git # 更换仓库 cd skywalking-kubernetes-master/chart/skywalking/ vim Chats.yaml dependencies: - name: elasticsearch version: ~7.12.1 # 官网的版本号为7.5.1 最新的elastic版本为7.12.1 repository: http://localhost:8080 # 修改为你本地的Repo地址 condition: elasticsearch.enabled 添加elasticsearch仓库 helm repo add elastic https://helm.elastic.co helm pull elastic/elasticsearch # 把elasticsearch内容拉下来 上传本地Helm 以防万一请先安装helmpush插件\nhttps://github.com/chartmuseum/helm-push\nhelm repo add chartmuseum http://localhost:8080 curl --data-binary \u0026#34;@elasticsearch-7.12.1.tgz\u0026#34; http://localhost:8080/api/charts helm push /root/skywalking-kubernetes-master/chart/skywalking/ chartmuseum helm repo update # 更新仓库 你可以尝试搜索一下\n保证仓库中存在elasticsarch和skywalking\n[root@k-master1 ~]# helm search repo NAME CHART VERSION APP VERSION DESCRIPTION chartmuseum/elasticsearch 7.12.1 7.12.1 Official Elastic helm chart for Elasticsearch chartmuseum/skywalking 4.0.0 Apache SkyWalking APM System 部署skywalking cd skywalking-kubernetes/chart helm dep up skywalking # change the release name according to your scenario export SKYWALKING_RELEASE_NAME=skywalking # change the namespace according to your scenario export SKYWALKING_RELEASE_NAMESPACE=default helm install \u0026#34;${SKYWALKING_RELEASE_NAME}\u0026#34; skywalking -n \u0026#34;${SKYWALKING_RELEASE_NAMESPACE}\u0026#34; \\ --set oap.image.tag=8.1.0-es7 \\ --set oap.storageType=elasticsearch7 \\ --set ui.image.tag=8.1.0 \\ --set elasticsearch.imageTag=7.5.1 helm uninstall skywalking # 卸载skywalking 准备离线镜像 busybox:1.30 docker.elastic.co/elasticsearch/elasticsearch:7.5.1 apache/skywalking-oap-server:8.1.0-es7 apache/skywalking-ui:8.1.0 chartmuseum/chartmuseum:latest Helm中的Elasticsearch可能会存在问题 你们也可以用我的这个elasticsearch配置 注意修改PVC\nkind: StatefulSet apiVersion: apps/v1 metadata: name: elasticsearch-master namespace: default labels: app: elasticsearch-master app.kubernetes.io/managed-by: Helm chart: elasticsearch heritage: Helm release: skywalking annotations: esMajorVersion: \u0026#39;7\u0026#39; meta.helm.sh/release-name: skywalking meta.helm.sh/release-namespace: default spec: replicas: 3 selector: matchLabels: app: elasticsearch-master template: metadata: name: elasticsearch-master creationTimestamp: null labels: app: elasticsearch-master chart: elasticsearch heritage: Helm release: skywalking spec: initContainers: - name: configure-sysctl image: \u0026#39;docker.elastic.co/elasticsearch/elasticsearch:7.5.1\u0026#39; command: - sysctl - \u0026#39;-w\u0026#39; - vm.max_map_count=262144 resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: IfNotPresent securityContext: privileged: true runAsUser: 0 containers: - name: elasticsearch image: \u0026#39;docker.elastic.co/elasticsearch/elasticsearch:7.5.1\u0026#39; ports: - name: http containerPort: 9200 protocol: TCP - name: transport containerPort: 9300 protocol: TCP volumeMounts: - name: datadir mountPath: /usr/share/elasticsearch/data env: - name: node.name valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: cluster.initial_master_nodes value: \u0026gt;- elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2, - name: discovery.seed_hosts value: elasticsearch-master-headless - name: cluster.name value: elasticsearch - name: network.host value: 0.0.0.0 - name: ES_JAVA_OPTS value: \u0026#39;-Xmx1g -Xms1g\u0026#39; - name: node.data value: \u0026#39;true\u0026#39; - name: node.ingest value: \u0026#39;true\u0026#39; - name: node.master value: \u0026#39;true\u0026#39; resources: limits: cpu: \u0026#39;1\u0026#39; memory: 2Gi requests: cpu: 100m memory: 2Gi readinessProbe: exec: command: - sh - \u0026#39;-c\u0026#39; - \u0026gt; #!/usr/bin/env bash -e # If the node is starting up wait for the cluster to be ready (request params: \u0026#39;wait_for_status=green\u0026amp;timeout=1s\u0026#39; ) # Once it has started only check that the node itself is responding START_FILE=/tmp/.es_start_file http () { local path=\u0026#34;${1}\u0026#34; if [ -n \u0026#34;${ELASTIC_USERNAME}\u0026#34; ] \u0026amp;\u0026amp; [ -n \u0026#34;${ELASTIC_PASSWORD}\u0026#34; ]; then BASIC_AUTH=\u0026#34;-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}\u0026#34; else BASIC_AUTH=\u0026#39;\u0026#39; fi curl -XGET -s -k --fail ${BASIC_AUTH} http://127.0.0.1:9200${path} } if [ -f \u0026#34;${START_FILE}\u0026#34; ]; then echo \u0026#39;Elasticsearch is already running, lets check the node is healthy and there are master nodes available\u0026#39; http \u0026#34;/_cluster/health?timeout=0s\u0026#34; else echo \u0026#39;Waiting for elasticsearch cluster to become cluster to be ready (request params: \u0026#34;wait_for_status=green\u0026amp;timeout=1s\u0026#34; )\u0026#39; if http \u0026#34;/_cluster/health?wait_for_status=green\u0026amp;timeout=1s\u0026#34; ; then touch ${START_FILE} exit 0 else echo \u0026#39;Cluster is not yet ready (request params: \u0026#34;wait_for_status=green\u0026amp;timeout=1s\u0026#34; )\u0026#39; exit 1 fi fi initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 10 successThreshold: 3 failureThreshold: 3 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: IfNotPresent securityContext: capabilities: drop: - ALL runAsUser: 1000 runAsNonRoot: true restartPolicy: Always terminationGracePeriodSeconds: 120 dnsPolicy: ClusterFirst securityContext: runAsUser: 1000 fsGroup: 1000 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - elasticsearch-master topologyKey: kubernetes.io/hostname schedulerName: default-scheduler volumeClaimTemplates: - metadata: name: datadir annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;managed-nfs-storage-class\u0026#34; spec: accessModes: [\u0026#34;ReadWriteMany\u0026#34;] resources: requests: storage: 10Gi serviceName: elasticsearch-master-headless podManagementPolicy: Parallel updateStrategy: type: RollingUpdate revisionHistoryLimit: 10 ","date":"2021-04-07T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/posts/kubernetes-skywallking/","title":"kubernetes-离线部署Skywallking"},{"content":"Redis Cluster（Redis集群）简介 redis是一个开源的key value存储系统，受到了广大互联网公司的青睐。redis3.0版本之前只支持单例模式，在3.0版本及以后才支持集群，我这里用的是redis3.0.0版本； redis集群采用P2P模式，是完全去中心化的，不存在中心节点或者代理节点； redis集群是没有统一的入口的，客户端（client）连接集群的时候连接集群中的任意节点（node）即可，集群内部的节点是相互通信的（PING-PONG机制），每个节点都是一个redis实例； 为了实现集群的高可用，即判断节点是否健康（能否正常使用），redis-cluster有这么一个投票容错机制：如果集群中超过半数的节点投票认为某个节点挂了，那么这个节点就挂了（fail）。这是判断节点是否挂了的方法； 那么如何判断集群是否挂了呢? -\u0026gt; 如果集群中任意一个节点挂了，而且该节点没有从节点（备份节点），那么这个集群就挂了。这是判断集群是否挂了的方法； 那么为什么任意一个节点挂了（没有从节点）这个集群就挂了呢？ -\u0026gt; 因为集群内置了16384个slot（哈希槽），并且把所有的物理节点映射到了这16384[0-16383]个slot上，或者说把这些slot均等的分配给了各个节点。当需要在Redis集群存放一个数据（key-value）时，redis会先对这个key进行crc16算法，然后得到一个结果。再把这个结果对16384进行求余，这个余数会对应[0-16383]其中一个槽，进而决定key-value存储到哪个节点中。所以一旦某个节点挂了，该节点对应的slot就无法使用，那么就会导致集群无法正常工作。 综上所述，每个Redis集群理论上最多可以有16384个节点。 Redis集群至少需要3个节点，因为投票容错机制要求超过半数节点认为某个节点挂了该节点才是挂了，所以2个节点无法构成集群。 要保证集群的高可用，需要每个节点都有从节点，也就是备份节点，所以Redis集群至少需要6台服务器。因为我没有那么多服务器，也启动不了那么多虚拟机，所在这里搭建的是伪分布式集群，即一台服务器虚拟运行6个redis实例，修改端口号为（7001-7006）1+1+1+1+1+1 = 6\n搭建集群 Redis版本6.0.8 Gcc7x.x.x 创建目录 mkdir /usr/local/redis-cluster cd /usr/local/redis-cluster wget http://download.redis.io/releases/redis-6.0.8.tar.gz mkdir {7001..7006} 复制配置文件 tar -zxf redis-6.0.8.tar.gz cd redis-6.0.8/ \u0026amp;\u0026amp; make install cp -a redis-6.0.8/redis.conf 7001/ # 以此类推 cp -a redis-6.0.8/redis.conf 7002/ 如果你不想编译安装的话,你可以把redis中的/bin目录的命令移动到每个node节点文件夹中，这样以方便你使用redis-server命令 编辑配置文件 此文件内容为集群模式最小配置文件内容.\nvim 7001/redis.conf # 以此类推,记得更改端口号和日志文件 bind 127.0.0.1 # IP可更换为内网IP port 7001 cluster-enabled yes cluster-config-file nodes7001.conf cluster-node-timeout 5000 appendonly yes daemonize yes logfile /usr/local/redis-cluster/7001/redis-7001.log maxmemory 4GB requirepass ******* dir /usr/local/redis-cluster/7001 masterauth **** port 7001 Redis运行端口 cluster-enabled yes启用集群模式 cluster-config-file nodes.conf集群模式配置文件 cluster-node-timeout 5000节点的超时时限 appendonly yes开启AOF持久化 daemonize yes开启后台运行 maxmemory 4GBRedis最大可用内存 requirepass连接Redis客户端密码 masterauth Slave连接master需要的认证 启动集群 自己建一个启动脚本,要不然手动启动太麻烦了\n#!/bin/bash redis-server /usr/local/redis-cluster/7001/redis.conf redis-server /usr/local/redis-cluster/7002/redis.conf redis-server /usr/local/redis-cluster/7003/redis.conf redis-server /usr/local/redis-cluster/7004/redis.conf redis-server /usr/local/redis-cluster/7005/redis.conf redis-server /usr/local/redis-cluster/7006/redis.conf chmod +x start.sh sh start.sh [root@bogon redis-cluster]# ps -aux | grep redis root 65558 0.0 0.0 64864 6256 ? Ssl 09:54 0:00 redis-server *:7001 [cluster] root 65564 0.0 0.0 61792 4760 ? Ssl 09:54 0:00 redis-server *:7002 [cluster] root 65566 0.0 0.0 61792 4736 ? Ssl 09:54 0:00 redis-server *:7003 [cluster] root 65572 0.0 0.0 61792 4712 ? Ssl 09:54 0:00 redis-server *:7004 [cluster] root 65578 0.0 0.0 61792 4704 ? Ssl 09:54 0:00 redis-server *:7005 [cluster] root 65580 0.0 0.0 61792 4780 ? Ssl 09:54 0:00 redis-server *:7006 [cluster] 加入集群 现在我们有许多实例正在运行，我们需要通过向节点写入一些有意义的配置来创建集群。\n如果您使用的是Redis 5或更高版本，这很容易完成，因为嵌入到中的Redis Cluster命令行实用程序为我们提供了帮助，该实用程序redis-cli可用于创建新集群，检查或重新分片现有集群等。\n对于Redis版本3或4，有一个称为的旧工具redis-trib.rb，它非常相似。您可以src在Redis源代码分发的目录中找到它。您需要安装redisgem才能运行redis-trib。\n如果你是用的是Redis3.x或者4.x 请前往官网链接 点我进入\n此方法为Redis5或者更高版本 redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 \\ 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 \\ --cluster-replicas 1 Can I set the above configuration? (type \u0026#39;yes\u0026#39; to accept): yes --cluster-replicas 1给Master只分配一个slave 连接集群 redis-cli -c -p 7001 -a *** 127.0.0.1:7001\u0026gt; info # Replication role:master connected_slaves:1 127.0.0.1:7001\u0026gt; set Host Linux7 -\u0026gt; Redirected to slot [16156] located at 127.0.0.1:7003 OK -a是你设置的requirepass密码 注意：出现connected_slaves:1 表示连接到了一个从服务器 如果为0 请查看服务器错误日志\n故障切换 连接到7003的从服务器7005 查看数据是否同步\nredis-cli -c -p 7005 -a *** master_host:127.0.0.1 master_port:7003 127.0.0.1:7005\u0026gt; get Host \u0026#34;Linux7\u0026#34; 宕机7003服务器\n[root@bogon redis-cluster]# ps -aux | grep 7003 root 70467 0.2 0.0 64352 5120 ? Ssl 11:20 0:01 redis-server *:7003 [cluster] root 70871 0.0 0.0 12112 1052 pts/0 S+ 11:29 0:00 grep --color=auto 7003 [root@bogon redis-cluster]# kill -15 70467 通过info发现7005已经成为主服务器\n127.0.0.1:7005\u0026gt; info # Replication role:master connected_slaves:0 再次启动7003发现已经更改为从服务器，并且已经被7005连接到\n127.0.0.1:7005\u0026gt; # Replication role:master connected_slaves:1 总结 首先 先说结论：redis集群无法保证强一致性\n既然无法保证强一致性，也就是说有可能出现写数据丢失的情况，比如一个客户端发一个写请求给master，master再同步到slave之前就给client一个回执。这个时候会存在一个时间窗口，master 和 slave之间的数据是不一致的。但是redis的最终一致性会使master和slave的数据是最终一致。\n另外还有一个可能，在客户端收到了master的一个写请求回执之后，此时master准备把数据同步到slave，同步之前突然挂了，那么这个数据真的就是会丢失了。\n如果为了保证强一致，比如我们每秒刷盘进行持久化，那么牺牲了这个吞吐量，就特别类似我们常说的同步复制了。但是redis集群是没有实现强一致的。\n1、redis保证最终一致性\n2、用最终一致性换取了高吞吐量\n3、主节点挂了的时候，如果数据没有同步到备节点，是会出现数据丢失的情况\n4、发生网络分区的时候也可能会丢数据，这个时候有个node timeout时间概念\n","date":"2021-02-19T00:00:00Z","permalink":"http://localhost:1313/posts/redis-cluster/","title":"Redis集群搭建"},{"content":"Service的简单理解 Service 是一种抽象的对象，它定义了一组 Pod 的逻辑集合和一个用于访问它们的策略，其实这个概念和微服务非常类似。一个 Serivce 下面包含的 Pod 集合是由 Label Selector 来决定的。\n假如我们后端运行了3个副本，这些副本都是可以替代的，因为前端并不关心它们使用的是哪一个后端服务。尽管由于各种原因后端的 Pod 集合会发送变化，但是前端却不需要知道这些变化，也不需要自己用一个列表来记录这些后端的服务，Service 的这种抽象就可以帮我们达到这种解耦的目的。\n三种IP 在继续往下学习 Service 之前，我们需要先弄明白 Kubernetes 系统中的三种IP，因为经常有同学混乱。\nNodeIP：Node 节点的 IP 地址 PodIP: Pod 的 IP 地址 ClusterIP: Service 的 IP 地址 首先，NodeIP是Kubernetes集群中节点的物理网卡IP地址(一般为内网)，所有属于这个网络的服务器之间都可以直接通信，所以Kubernetes集群外要想访问Kubernetes集群内部的某个节点或者服务，肯定得通过Node P进行通信（这个时候一般是通过外网 IP 了）\n然后PodIP是每个Pod的IP地址，它是网络插件进行分配的，前面我们已经讲解过\n最后ClusterIP是一个虚拟的IP，仅仅作用于Kubernetes Service 这个对象，由Kubernetes自己来进行管理和分配地址。\n定义Servcie 定义 Service 的方式和我们前面定义的各种资源对象的方式类型，例如，假定我们有一组 Pod 服务，它们对外暴露了 8080 端口，同时都被打上了 app=beijing-nginx 这样的标签，那么我们就可以像下面这样来定义一个 Service 对象\napiVersion: v1 kind: Service metadata: name: public-beijing-nginx-service spec: selector: app: beijing-nginx ports: - protocol: TCP port: 80 targetPort: 80 # 可以理解成是service的访问端口 name: beijing-nginx-http 然后通过的使用 kubectl create -f myservice.yaml 就可以创建一个名为 myservice 的 Service 对象，它会将请求代理到使用 TCP 端口为 80，具有标签 app=beijing-nginx-http 的 Pod 上，这个 Service 会被系统分配一个我们上面说的 Cluster IP，该 Service 还会持续的监听 selector 下面的 Pod，会把这些 Pod 信息更新到一个名为 myservice 的Endpoints 对象上去，这个对象就类似于我们上面说的 Pod 集合了。\n需要注意的是，Service 能够将一个接收端口映射到任意的 targetPort。默认情况下，targetPort 将被设置为与 port 字段相同的值。可能更有趣的是，targetPort 可以是一个字符串，引用了 backend Pod 的一个端口的名称。因实际指派给该端口名称的端口号，在每个 backend Pod 中可能并不相同，所以对于部署和设计 Service，这种方式会提供更大的灵活性。\n另外 Service 能够支持 TCP 和 UDP 协议，默认是 TCP 协议。\nkube-proxy 前面我们讲到过，在 Kubernetes 集群中，每个 Node 会运行一个 kube-proxy 进程, 负责为 Service 实现一种 VIP（虚拟 IP，就是我们上面说的 clusterIP）的代理形式，现在的 Kubernetes 中默认是使用的 iptables 这种模式来代理。\niptables 这种模式，kube-proxy 会 watch apiserver 对 Service 对象和 Endpoints 对象的添加和移除。对每个 Service，它会添加上 iptables 规则，从而捕获到达该 Service 的 clusterIP（虚拟 IP）和端口的请求，进而将请求重定向到 Service 的一组 backend 中的某一个 Pod 上面。我们还可以使用 Pod readiness 探针 验证后端 Pod 可以正常工作，以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端，这样做意味着可以避免将流量通过 kube-proxy 发送到已知失败的 Pod 中，所以对于线上的应用来说一定要做 readiness 探针。\nptables 模式的 kube-proxy 默认的策略是，随机选择一个后端 Pod。\n比如当创建 backend Service 时，Kubernetes 会给它指派一个虚拟 IP 地址，比如 10.0.0.1。假设 Service 的端口是 1234，该 Service 会被集群中所有的 kube-proxy 实例观察到。当 kube-proxy 看到一个新的 Service，它会安装一系列的 iptables 规则，从 VIP 重定向到 per-Service 规则。 该 per-Service 规则连接到 per-Endpoint 规则，该 per-Endpoint 规则会重定向（目标 NAT）到后端的 Pod。\n优化iptables模式性能 在大型集群（有数万个 Pod 和 Service）中，当 Service（或其 EndpointSlices）发生变化时 iptables 模式的 kube-proxy 在更新内核中的规则时可能要用较长时间。 你可以通过修改kube-proxy的ConfigMap中的选项来调整 kube-proxy 的同步行为：\niptables: minSyncPeriod: 1s syncPeriod: 30s minSyncPeriod: 参数设置尝试同步 iptables 规则与内核之间的最短时长。如果是 0s，那么每次有任一 Service 或 Endpoint 发生变更时，kube-proxy 都会立即同步这些规则。 这种方式在较小的集群中可以工作得很好，但如果在很短的时间内很多东西发生变更时，它会导致大量冗余工作。 例如，如果你有一个由 Deployment 支持的 Service，共有 100 个 Pod，你删除了这个 Deployment， 且设置了 minSyncPeriod: 0s，kube-proxy 最终会从 iptables 规则中逐个删除 Service 的 Endpoint， 总共更新 100 次。使用较大的 minSyncPeriod 值时，多个 Pod 删除事件将被聚合在一起， 因此 kube-proxy 最终可能会进行例如 5 次更新，每次移除 20 个端点， 这样在 CPU 利用率方面更有效率，能够更快地同步所有变更。 默认值 1s 对于中小型集群是一个很好的折衷方案。 在大型集群中，可能需要将其设置为更大的值。 （特别是，如果 kube-proxy 的 sync_proxy_rules_duration_seconds 指标表明平均时间远大于 1 秒， 那么提高 minSyncPeriod 可能会使更新更有效率。）\nsyncPeriod: 参数控制与单次 Service 和 Endpoint 的变更没有直接关系的少数同步操作。 特别是，它控制 kube-proxy 在外部组件已干涉 kube-proxy 的 iptables 规则时通知的速度。 在大型集群中，kube-proxy 也仅在每隔 syncPeriod 时长执行某些清理操作，以避免不必要的工作。 IPVS 在 ipvs 模式下，kube-proxy 监视 Kubernetes Service 和 EndpointSlice， 然后调用 netlink 接口创建 IPVS 规则， 并定期与 Kubernetes Service 和 EndpointSlice 同步 IPVS 规则。 该控制回路确保 IPVS 状态与期望的状态保持一致。 访问 Service 时，IPVS 会将流量导向到某一个后端 Pod。\nIPVS 代理模式基于 netfilter 回调函数，类似于 iptables 模式， 但它使用哈希表作为底层数据结构，在内核空间中生效。 这意味着 IPVS 模式下的 kube-proxy 比 iptables 模式下的 kube-proxy 重定向流量的延迟更低，同步代理规则时性能也更好。 与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。\nIPVS 提供了更多选项来平衡后端 Pod 的流量，默认是 rr，有如下一些策略：\nrr: 轮询 lc: 最少连接（打开连接数最少） dh: 目标地址哈希 sh: 源地址哈希 sed: 最短预期延迟 nq:最少队列 不过现在只能整体修改策略，可以通过 kube-proxy 中配置 –ipvs-scheduler 参数来实现，暂时不支持特定的 Service 进行配置。\n开启ipvs模块\nmodprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 yum install ipvsadm ipset -y 修改kube-proxy的configMap\nkubectl edit configmap kube-proxy -n kube-system # 修改mode为\u0026#34;ipvs\u0026#34; minSyncPeriod: 0s scheduler: \u0026#34;\u0026#34; syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \u0026#34;ipvs\u0026#34; # 修改此处 nodePortAddresses: null 修改完成后记得重启kube-proxy，然后使用ipvsadm -ln校验。正常可以出现很多的规则链条。\n会话亲和性 在这些代理模型中，绑定到 Service IP:Port 的流量被代理到合适的后端， 客户端不需要知道任何关于 Kubernetes、Service 或 Pod 的信息。\n如果要确保来自特定客户端的连接每次都传递给同一个 Pod， 你可以通过设置 Service 的 .spec.sessionAffinity 为 ClientIP 来设置基于客户端 IP 地址的会话亲和性（默认为 None）。\napiVersion: v1 kind: Service metadata: name: demo spec: sessionAffinity: ClientIP 会话超时 你还可以通过设置 Service 的 .spec.sessionAffinityConfig.clientIP.timeoutSeconds 来设置最大会话粘性时间（默认值为 10800，即 3 小时）。\napiVersion: v1 kind: Service metadata: name: demo spec: sessionAffinityConfig: clientIP: imeoutSeconds: 10800 Service 将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。\n使用 Kubernetes，你无需修改应用程序去使用不熟悉的服务发现机制。 Kubernetes 为 Pod 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡。\nKubernetes ServiceTypes 允许指定你所需要的 Service 类型。\nClusterIP：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是你没有为服务显式指定 type 时使用的默认值。 你可以使用 Ingress 或者 Gateway API 向公众暴露服务。 NodePort： 通过每个节点上的 IP 和静态端口（NodePort）暴露服务。 为了让节点端口可用，Kubernetes 设置了集群 IP 地址，这等同于你请求 type: ClusterIP 的服务。 LoadBalancer：使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上。 ExternalName：通过返回 CNAME 记录和对应值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。 无需创建任何类型代理。 NodePort 如果你将 type 字段设置为 NodePort，则 Kubernetes 控制平面将在 --service-node-port-range 标志指定的范围内分配端口（默认值：30000-32767）。 每个节点将那个端口（每个节点上的相同端口号）代理到你的服务中。 你的服务在其 .spec.ports[*].nodePort 字段中报告已分配的端口。\n使用 NodePort 可以让你自由设置自己的负载均衡解决方案， 配置 Kubernetes 不完全支持的环境， 甚至直接暴露一个或多个节点的 IP 地址。\n对于 NodePort 服务，Kubernetes 额外分配一个端口（TCP、UDP 或 SCTP 以匹配服务的协议）。 集群中的每个节点都将自己配置为监听分配的端口并将流量转发到与该服务关联的某个就绪端点。 通过使用适当的协议（例如 TCP）和适当的端口（分配给该服务）连接到所有节点， 你将能够从集群外部使用 type: NodePort 服务。\napiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort selector: app.kubernetes.io/name: MyApp ports: # 默认情况下，为了方便起见，`targetPort` 被设置为与 `port` 字段相同的值。 - port: 80 targetPort: 80 # 可选字段 # 默认情况下，为了方便起见，Kubernetes 控制平面会从某个范围内分配一个端口号（默认：30000-32767） nodePort: 30007 LoadBalancer 在使用支持外部负载均衡器的云提供商的服务时，设置 type 的值为 \u0026quot;LoadBalancer\u0026quot;， 将为 Service 提供负载均衡器。 负载均衡器是异步创建的，关于被提供的负载均衡器的信息将会通过 Service 的 status.loadBalancer 字段发布出去。\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 clusterIP: 10.0.171.239 type: LoadBalancer status: loadBalancer: ingress: - ip: 192.0.2.127 自外部负载均衡器的流量将直接重定向到后端 Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。\n某些云提供商允许设置 loadBalancerIP。 在这些情况下，将根据用户设置的 loadBalancerIP 来创建负载均衡器。 如果没有设置 loadBalancerIP 字段，将会给负载均衡器指派一个临时 IP。 如果设置了 loadBalancerIP，但云提供商并不支持这种特性，那么设置的 loadBalancerIP 值将会被忽略掉。\n要实现 type: LoadBalancer 的服务，Kubernetes 通常首先进行与请求 type: NodePort 服务等效的更改。 cloud-controller-manager 组件然后配置外部负载均衡器以将流量转发到已分配的节点端口。\nExternalName 类型为 ExternalName 的服务将服务映射到 DNS 名称，而不是典型的选择算符，例如 my-service 或者 cassandra。 你可以使用 spec.externalName 参数指定这些服务。\n例如，以下 Service 定义将 prod 名称空间中的 my-service 服务映射到 my.database.example.com\napiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com 自定义Service 假设我们的etcd集群在外部，我们想要通过Service进行访问，我们可以进行自定义的Service。\napiVersion: v1 kind: Service metadata: name: my-service spec: type: ClusterIP ClusterIP: None ports: - name: etcd-port port: 2379 --- apiVersion: v1 kind: Endpoints metadata: name: custom-etcd-svc subsets: - address: - ip: 10.151.30.11 ports: - name: etcd-port port: 2379 获取客户端IP 通常，当集群内的客户端连接到服务的时候，是支持服务的 Pod 可以获取到客户端的 IP 地址的，但是，当通过节点端口接收到连接时，由于对数据包执行了源网络地址转换（SNAT），因此数据包的源 IP 地址会发生变化，后端的 Pod 无法看到实际的客户端 IP，对于某些应用来说是个问题，比如，nginx 的请求日志就无法获取准确的客户端访问 IP 了。\n假设我们现在有一组nginx集群服务，当我从10.1.6.48进行访问的时候我们可以看一下最终呈现给我们的地址\n10.10.207.192 - - [23/Feb/2023:06:09:24 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.61.1\u0026#34; \u0026#34;-\u0026#34; 10.10.207.192 - - [23/Feb/2023:06:10:50 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.61.1\u0026#34; \u0026#34;-\u0026#34; 10.10.207.192 - - [23/Feb/2023:06:10:52 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.61.1\u0026#34; \u0026#34;-\u0026#34; 10.10.207.192 - - [23/Feb/2023:06:10:53 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.61.1\u0026#34; \u0026#34;-\u0026#34; 正常来说我得到的应该是客户端的真实IP地址，而我现在得到的却是tunl0@NONE的IP地址\n这个时候我们可以在 Service 设置 externalTrafficPolicy 来减少网络跳数\nkind: Service apiVersion: v1 metadata: name: public-beijing-nginx-service namespace: default spec: externalTrafficPolicy: Local ports: - name: beijing-nginx-http protocol: TCP port: 80 targetPort: 80 selector: app: beijing-nginx type: ClusterIP sessionAffinity: ClusterIP ipFamilies: - IPv4 ipFamilyPolicy: SingleStack internalTrafficPolicy: Cluster status: loadBalancer: {} 但这可能导致流量分配不均。 没有针对特定 LoadBalancer 服务的任何 Pod 的节点将无法通过自动分配的 .spec.healthCheckNodePort 进行 NLB 目标组的运行状况检查，并且不会收到任何流量。\n","date":"2020-04-13T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/ServiceReader/","title":"kubernetes-Service解读"},{"content":"简单安装使用 最新版本应该是1.4.1\ngit clone https://github.com/nacos-group/nacos-k8s.git 简单使用 如果你使用简单方式快速启动,请注意这是没有使用持久化卷的,可能存在数据丢失风险:!!! cd nacos-k8s chmod +x quick-startup.sh ./quick-startup.sh 演示使用 服务注册 curl -X PUT \u0026#39;http://cluster-ip:8848/nacos/v1/ns/instance?serviceName=nacos.naming.serviceName\u0026amp;ip=20.18.7.10\u0026amp;port=8080\u0026#39; 服务发现 curl -X GET \u0026#39;http://cluster-ip:8848/nacos/v1/ns/instance/list?serviceName=nacos.naming.serviceName\u0026#39; 发布配置 curl -X POST \u0026#34;http://cluster-ip:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId\u0026amp;group=test\u0026amp;content=helloWorld\u0026#34; 获取配置 curl -X GET \u0026#34;http://cluster-ip:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId\u0026amp;group=test\u0026#34; 高级用法 在高级使用中,Nacos在K8S拥有自动扩容缩容和数据持久特性,请注意如果需要使用这部分功能请使用PVC持久卷,Nacos的自动扩容缩容需要依赖持久卷,以及数据持久化也是一样,本例中使用的是NFS来使用PVC.\n部署NFS nfs-client-provisioner 可动态为kubernetes提供pv卷，是Kubernetes的简易NFS的外部provisioner，本身不提供NFS，需要现有的NFS服务器提供存储。持久卷目录的命名规则为: ${namespace}-${pvcName}-${pvName}\n创建角色\nkubectl create -f deploy/nfs/rbac.yaml 修改NFS的yaml\nvim nacos-k8s/deploy/nfs/deployment.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 10.1.6.93 # 修改NFS的IP为你本地的NFSIP地址 - name: NFS_PATH value: /server/nacos volumes: - name: nfs-client-root nfs: server: 10.1.6.93 # 同上 path: /server/nacos 部署NFS-client\nkubectl create -f deploy/nfs/deployment.yaml 部署NFS StorageClass\nkubectl create -f deploy/nfs/class.yaml 验证nfs-client-provisioner是否成功\nkubectl get pod -l app=nfs-client-provisioner 部署mysql kubectl create -f deploy/mysql/mysql-nfs.yaml 部署Nacos 修改 deploy/nacos/nacos-pvc-nfs.yaml 可以自行选择更改\ndata: mysql.master.db.name: \u0026#34;主库名称\u0026#34; mysql.master.port: \u0026#34;主库端口\u0026#34; mysql.slave.port: \u0026#34;从库端口\u0026#34; mysql.master.user: \u0026#34;主库用户名\u0026#34; mysql.master.password: \u0026#34;主库密码\u0026#34; 创建Nacos kubectl create -f nacos-k8s/deploy/nacos/nacos-pvc-nfs.yaml 扩容测试 在扩容前，使用 kubectl exec获取在pod中的Nacos集群配置文件信息 for i in 0 1; do echo nacos-$i; kubectl exec nacos-$i cat conf/cluster.conf; done StatefulSet控制器根据其序数索引为每个Pod提供唯一的主机名。 主机名采用 - 的形式。 因为nacos StatefulSet的副本字段设置为3，所以当前集群文件中只有三个Nacos节点地址\n使用kubectl scale 对Nacos动态扩容 kubectl scale sts nacos --replicas=5 ","date":"2020-04-07T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/InstallNacosCluster/","title":"基于kubernetes部署nacos集群"},{"content":"kube-scheduler 是 kubernetes 的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源，这也是我们选择使用 kubernetes 一个非常重要的理由。如果一门新的技术不能帮助企业节约成本、提供效率，我相信是很难推进的。\n调度流程 kube-scheduler 提供的默认调度器能够满足我们绝大多数的要求，我们前面和大家接触的示例也基本上用的默认的策略，都可以保证我们的 Pod 可以被分配到资源充足的节点上运行。但是在实际的线上项目中，可能我们自己会比 kubernetes 更加了解我们自己的应用，比如我们希望一个 Pod 只能运行在特定的几个节点上，或者这几个节点只能用来运行特定类型的应用，这就需要我们的调度器能够可控。\n发起创建Deployment请求-\u0026gt;API Server,这个时候APIServer会进行一系列的逻辑处理,例如: 鉴权、查看你是否有权限操作、Deployment创建是否合法等等,然后将请求存储到etcd当中并且转发给Controller Manager Controller Manager会监听API Server,这个时候假设监听到的是一个创建Deployment的请求,则会把请求转发到Deployment Controller Deployment Controller接受到请求后创建ReplicaSet,然后ReplicaSet Controller会根据yaml当中定义的template模板来进行创建Pod,然后返回给API Server 在创建之初的Pod属性中nodeName为空,也就是没有被调度过的,这个时候调度器就会对它进行调度,调度去watchPod对象,然后分析那个节点最适合这个Pod,然后将节点的名字通过类似于bind的这种方法写入到nodeName当中。 然后该节点的kubelet会进行一系列的判断,然后进入Create Pod的流程,然后进行一系列的CNI和CSI的过程。 这也就是我们常说的往往越简单的东西,背后实现的越复杂。\n调度阶段 kube-scheduler调度分为两个阶段\npredicate: 过滤阶段，过滤不符合条件的节点。 priority: 优先级排序，选择优先级最高的节点，也就是给节点打分。 Predicates策略 PodFitsHostPorts: 检查是否有Host Ports冲突 PodFitsPorts: 同上 PodFitsResources: 检查Node的资源是否充足，包括允许的Pod数量、CPU、内存、GPU个数以及其他的OpaqueIntResources。 HostName:检查pod.Spec.NodeName是否与候选节点一致 MatchNodeSelector:检查候选节点的pod.Spec.NodeSelector是否匹配 NoVolumeZoneConflict:检查volume zone是否冲突 Priority策略 SelectorSpreadPriority: 优先减少节点上属于同一个Service或Replication Controller的Pod数量。 InterPodAffinityPriority: 优先将Pod调度到相同的拓扑上 LeastRequestedPriority:优先调度到请求资源少的节点上 BalancedResourceAllocation: 优先平衡各节点的资源使用 NodePreferAvoidPodsPriority:权重判断 太多了可以自己去官网了解一下，这些策略都可以通过scheduler配置文件去配置，其实一般来说我们不太需要，我觉得kubernetes的调度是最让我们省心的。\n资源需求 requests:属于调度器调度的时候所参考的指标，也就是说我这个应用最少需要250m的cpu和256m的内存才能运行。 kind: Deployment apiVersion: apps/v1 metadata: name: nginx-deployment namespace: default labels: app: nginx version: qa spec: replicas: 2 selector: matchLabels: app: nginx version: qa template: metadata: creationTimestamp: null labels: app: nginx version: qa spec: volumes: - name: host-time hostPath: path: /etc/localtime type: \u0026#39;\u0026#39; containers: - name: nginx image: nginx:latest ports: - name: http-web containerPort: 80 protocol: TCP resources: limits: cpu: \u0026#39;1\u0026#39; memory: 2Gi requests: cpu: 250m memory: 256Mi 可以查看你节点的一些资源状态\n[root@master1 ~]# kubectl get nodes -o yaml allocatable: cpu: 15600m ephemeral-storage: 104278276Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: \u0026#34;38390677064\u0026#34; pods: \u0026#34;330\u0026#34; capacity: cpu: \u0026#34;16\u0026#34; ephemeral-storage: 104278276Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: 40003048Ki pods: \u0026#34;330\u0026#34; 可以看看这个deployment运行以后我们的cgroup对他做了如何的限制\n\u0026#34;CgroupParent\u0026#34;: \u0026#34;/kubepods/burstable/pod1ceee26d-2ec2-43a8-96ef-5aa9ac99779b\u0026#34; 进入这个目录\n[root@node1 ~]# cd /sys/fs/cgroup/cpu/kubepods/burstable/pod1ceee26d-2ec2-43a8-96ef-5aa9ac99779b [root@node1 pod1ceee26d-2ec2-43a8-96ef-5aa9ac99779b]# cat cpu.shares 358 kubernetes对于不同的QOS的处理方式是不一样的。\nLimit-range 一个 LimitRange（限制范围） 对象提供的限制能够做到：\n在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制。 在一个命名空间中实施对每个 PersistentVolumeClaim 能申请的最小和最大的存储空间大小的限制。 在一个命名空间中实施对一种资源的申请值和限制值的比值的控制。 设置一个命名空间中对计算资源的默认申请/限制值，并且自动的在运行时注入到多个 Container 中。 当某命名空间中有一个 LimitRange 对象时，将在该命名空间中实施 LimitRange 限制。\napiVersion: v1 kind: LimitRange metadata: name: cpu-resource-constraint spec: limits: - default: # 此处定义默认限制值 cpu: 500m defaultRequest: # 此处定义默认请求值 cpu: 500m max: # max 和 min 定义限制范围 cpu: \u0026#34;1\u0026#34; min: cpu: 100m type: Container 这东西其实是不太常用的\n生产环境需要考虑的问题 是否公平调度 资源是否高效利用 QOS affinity和anti-affinity 数据本地化 内部负载干扰(inter-workload interference) deadlines ","date":"2020-03-14T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/posts/scheduler/","title":"kubernetes-Scheduler简单详解"}]