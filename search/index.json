[{"content":"å“ï¼Œæœ¬æ¥30å·å°±å¯ä»¥åˆ°å¹¿å·çš„ï¼Œå› ä¸ºå¹¿å·çš„å¤©æ°”æ‰€ä»¥30å·åŸºæœ¬ä¸Šæ‰€æœ‰åˆ°å¹¿å·çš„é£æœºéƒ½å–æ¶ˆäº†ã€‚çœŸæ˜¯æ²¡åŠæ³•çš„ï¼Œæœ€è¿‘å¹¿å·çš„é›·é›¨å¤©æ°”å¤ªå¤šäº†ï¼Œä¹Ÿèƒ½ç†è§£å§ã€‚ ç´¢æ€§æˆ‘å°±ç›´æ¥æ”¹ç­¾åˆ°äº†5æœˆ1å·æ—©ä¸Šæœ€æ—©çš„ä¸€ç­é£æœºé£å¹¿å·äº†ã€‚\næ¥çœ‹çœ‹æ—¥å‡ºçš„åŒ—äº¬å›½é™…å¤§å…´æœºåœºï¼Œè¢«èª‰ä¸ºâ€œæ–°ä¸–ç•Œä¸ƒå¤§å¥‡è¿¹â€ä¹‹é¦–ã€‚ æ—¥å‡ºçœŸçš„å¤ªç¾å’¯ã€‚ ä¸å¤šåºŸè¯äº†ï¼Œç›´æ¥å‡ºå‘è¯´ä¸€ä¸‹è¿™å‡ å¤©çš„è¡Œç¨‹ã€‚\nè¡Œç¨‹ä¸€è§ˆ æˆ‘ç®—äº†ä¸€ä¸‹è®¡åˆ’å¤§æ¦‚æ˜¯è¿™æ ·çš„ï¼Œå› ä¸ºäº”ä¸€å½“å¤©æ‰èƒ½åˆ°è¾¾å¹¿å·ç„¶åè·¯ç¨‹å°±ä¸è®¡äº†ã€‚\nDay1: å»33å°é•‡é€›é€›å½“åœ°çš„ä¸€äº›æ–‡åˆ› Day2: é³’é±¼æ´²æ–‡åˆ›å›­+å·¥å†œ8å·+ä¸‹ååŠ+ä¸œèæ¤ç‰©å›­ 33å°é•‡æ‰“å¡ å½“å¤©å°±æ¥åˆ°çš„33å°é•‡è¿›è¡Œæ‰“å¡ï¼Œæ”»ç•¥æ˜¯åœ¨åå…­ç•ªä¸Šé¢çœ‹çš„,å½“ç„¶äº†æˆ‘æ˜¯çº¯çº¯è‡ªå·±çœ‹å¾—ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨åå…­ç•ªä¸Šç”¨è‡ªå¸¦çš„ä¸€é”®åº”ç”¨ ç„¶åå°±å¯ä»¥æŒ‰ç…§å½“å‰è§„åˆ’çš„è·¯çº¿è¿›è¡Œæ¸¸ç©äº†ã€‚å…¶å®æ€»ä½“æˆ‘é€›ä¸‹æ¥è§‰å¾—è¿˜æ˜¯ä¸é”™çš„ï¼Œä½†æ˜¯æ²¡å¿…è¦ä¸ºäº†ä¸“é—¨æ‹ç…§å»ä¸€è¶Ÿå“ˆå“ˆå“ˆã€‚\nä½†æ˜¯ï¼ï¼ï¼ å’±å¦‚æœå°±æ˜¯æƒ³è¦å»æ‹ç…§ï¼Œå’±å°±ä¸ºäº†æ‹ç…§å»ä¸€è¶Ÿã€‚é‡Œé¢å¤§å¤šæ•°æ˜¯ä¸€äº›æ–‡åˆ›çš„ç‰©å“ï¼Œè¿˜æœ‰ä¸€äº›éŸ³ä¹èŠ‚å’Œå±•ä¼šä¹‹ç±»çš„ã€‚ å¤§å¤šæ•°æ˜¯è¥¿é¤åº—å’Œä¸€äº›æ¸¯å¼èŒ¶é¤å…ä¹‹ç±»çš„ã€‚è™½ç„¶æˆ‘æ²¡è¿›å»å“å°ä½†æ˜¯çœ‹é—¨åº—æ„Ÿè§‰èµ·æ¥è¿˜æ˜¯ä¸é”™çš„(æ²¡æ‰“å¹¿å‘Šå“ˆå“ˆ)\nå…¶å®æˆ‘è¿™ä¸ªäººå‡ºå»ç©å„¿æœ‰ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯\nåœ¨è‡ªå·±èƒ½æ¥å—çš„èŒƒå›´ä¹‹å†…æƒ³åƒä»€ä¹ˆå°±åƒä»€ä¹ˆï¼Œæƒ³ä¹°ä»€ä¹ˆå°±ä¹°ä»€ä¹ˆã€‚ å…¶å®å¤§å®¶å‡ºå»ç©å„¿å°±æ˜¯ä¸ºäº†ç©å„¿çš„å¼€å¿ƒç©å„¿çš„èˆ’æœï¼Œå¹³å¸¸ä¹Ÿä¼šé‡åˆ°ä¸å°‘å®°å®¢ä¹‹ç±»çš„ï¼Œä½†æˆ‘è§‰å¾—ä¸€èˆ¬æ¥è¯´éƒ½æ¯”è¾ƒå°‘æ•°å§ï¼Œåªæ˜¯å¸Œæœ›å¤§å®¶è‡ªå·±ä¸åƒäºç©å„¿çš„å¼€å¿ƒå°±å¥½å•¦ã€‚ \u003c!DOCTYPE html\u003e ","date":"2024-05-01T00:00:00Z","image":"https://img11.360buyimg.com/ddimg/jfs/t1/244924/12/7691/82854/663216bfF4f4f3c1a/985a4b4d1b72840a.jpg","permalink":"http://localhost:1313/tourists/dongguan/","title":"æ—…è¡Œæ—¥è®°-å¹¿ä¸œ-ä¸œè"},{"content":"é…å¥—Bilibiliè§†é¢‘å·²ç»æ›´æ–°ï¼šç‚¹æˆ‘è§‚çœ‹\nå‡†å¤‡SealOS æœºå™¨ä¿¡æ¯å¦‚ä¸‹ï¼š\næœåŠ¡å™¨åç§° IP Role ready-kubernetes-master1 10.1.11.100 Control-Plane ready-kubernetes-master2 10.1.11.101 Control-Plane ready-kubernetes-master3 10.1.11.102 Control-Plane ready-kubernetes-node1 10.1.11.103 Node ready-kubernetes-node2 10.1.11.104 Node ready-kubernetes-node3 10.1.11.105 Node é€šè¿‡SealOSéƒ¨ç½²çš„å‰ææ¡ä»¶ SealOS For Kubernetes æ¯ä¸ªé›†ç¾¤èŠ‚ç‚¹åº”è¯¥æœ‰ä¸åŒçš„ä¸»æœºåã€‚ä¸»æœºåä¸è¦å¸¦ä¸‹åˆ’çº¿ã€‚ æ‰€æœ‰èŠ‚ç‚¹çš„æ—¶é—´éœ€è¦åŒæ­¥ã€‚ éœ€è¦åœ¨ K8s é›†ç¾¤çš„ç¬¬ä¸€ä¸ª master èŠ‚ç‚¹ä¸Šè¿è¡Œ sealos run å‘½ä»¤ï¼Œç›®å‰é›†ç¾¤å¤–çš„èŠ‚ç‚¹ä¸æ”¯æŒé›†ç¾¤å®‰è£…ã€‚ å»ºè®®ä½¿ç”¨å¹²å‡€çš„æ“ä½œç³»ç»Ÿæ¥åˆ›å»ºé›†ç¾¤ã€‚ä¸è¦è‡ªå·±è£… Dockerï¼ æ”¯æŒå¤§å¤šæ•° Linux å‘è¡Œç‰ˆï¼Œä¾‹å¦‚ï¼šUbuntuã€CentOSã€Rocky linuxã€‚ æ”¯æŒ Docker Hub ä¸­çš„æ‰€æœ‰ Kubernetes ç‰ˆæœ¬ã€‚ æ”¯æŒä½¿ç”¨ Containerd ä½œä¸ºå®¹å™¨è¿è¡Œæ—¶ã€‚ åœ¨å…¬æœ‰äº‘ä¸Šå®‰è£…è¯·ä½¿ç”¨ç§æœ‰ IPã€‚ è·å–å½“å‰ç¨³å®šç‰ˆæœ¬çš„SealOSåˆ—è¡¨ # è·å–ébetaç‰ˆæœ¬ curl --silent \u0026#34;https://api.github.com/repos/labring/sealos/releases\u0026#34; | jq -r \u0026#39;map(select(.tag_name | test(\u0026#34;beta\u0026#34;; \u0026#34;i\u0026#34;) | not)) | .[].tag_name\u0026#39; ä¸‹è½½æœ€æ–°ç¨³å®šç‰ˆæœ¬çš„SealOSï¼Œç‰ˆæœ¬å·ä¸ºv4.3.7 # åœ¨ä¸€å°ä¸»æœºä¸Šæ‰§è¡Œå°±è¡Œäº† VERSION=v4.3.7 wget https://mirror.ghproxy.com/https://github.com/labring/sealos/releases/download/${VERSION}/sealos_${VERSION#v}_linux_amd64.tar.gz \\ \u0026amp;\u0026amp; tar zxvf sealos_${VERSION#v}_linux_amd64.tar.gz sealos \u0026amp;\u0026amp; chmod +x sealos \u0026amp;\u0026amp; mv sealos /usr/bin éªŒè¯SealOSæ˜¯å¦å®‰è£…å®Œæˆ [root@localhost ~]# sealos version SealosVersion: buildDate: \u0026#34;2023-10-30T16:19:05Z\u0026#34; compiler: gc gitCommit: f39b2339 gitVersion: 4.3.7 goVersion: go1.20.10 platform: linux/amd64 æ­£å¸¸èƒ½æ˜¾ç¤ºå‡ºæ¥ç‰ˆæœ¬å·ä¿¡æ¯å°±è¡¨ç¤ºå®‰è£…æ­£å¸¸ã€‚\nå¿«é€Ÿéƒ¨ç½²é«˜å¯ç”¨é›†ç¾¤ é»˜è®¤ä½¿ç”¨çš„å®¹å™¨è¿è¡Œæ—¶ä¸ºContainerd å¼€å§‹ä½¿ç”¨sealOSæ¥éƒ¨ç½²å¤šèŠ‚ç‚¹é›†ç¾¤ sealos run registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com/labring/helm:v3.9.4 registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.13.4 \\ --masters 10.1.11.100,10.1.11.101,10.1.11.102 \\ --nodes 10.1.11.103,10.1.11.104,10.1.11.105 -p 123..com --masters: kubernetes-masterçš„èŠ‚ç‚¹åœ°å€åˆ—è¡¨ --nodes: kubernetes-nodeçš„èŠ‚ç‚¹åœ°å€åˆ—è¡¨ -p: è¿œç¨‹ä¸»æœºçš„SSHç™»å½•å¯†ç  æ³¨æ„éƒ¨ç½²çš„æ—¶å€™æ³¨æ„æœåŠ¡å™¨çš„HostNameå¿…é¡»å”¯ä¸€ä¸å†²çª\n2. æ£€æŸ¥æ˜¯å¦å®‰è£…æˆåŠŸ å‡ºç°å¦‚ä¸‹å†…å®¹è¡¨ç¤ºå®‰è£…æˆåŠŸ\n2024-04-11T15:20:01 info Executing pipeline RunGuest in CreateProcessor. â„¹ï¸ Using Cilium version 1.13.4 ğŸ”® Auto-detected cluster name: kubernetes ğŸ”® Auto-detected datapath mode: tunnel ğŸ”® Auto-detected kube-proxy has been installed 2024-04-11T15:20:03 info succeeded in creating a new cluster, enjoy it! 2024-04-11T15:20:03 info ___ ___ ___ ___ ___ ___ /\\ \\ /\\ \\ /\\ \\ /\\__\\ /\\ \\ /\\ \\ /::\\ \\ /::\\ \\ /::\\ \\ /:/ / /::\\ \\ /::\\ \\ /:/\\ \\ \\ /:/\\:\\ \\ /:/\\:\\ \\ /:/ / /:/\\:\\ \\ /:/\\ \\ \\ _\\:\\~\\ \\ \\ /::\\~\\:\\ \\ /::\\~\\:\\ \\ /:/ / /:/ \\:\\ \\ _\\:\\~\\ \\ \\ /\\ \\:\\ \\ \\__\\ /:/\\:\\ \\:\\__\\ /:/\\:\\ \\:\\__\\ /:/__/ /:/__/ \\:\\__\\ /\\ \\:\\ \\ \\__\\ \\:\\ \\:\\ \\/__/ \\:\\~\\:\\ \\/__/ \\/__\\:\\/:/ / \\:\\ \\ \\:\\ \\ /:/ / \\:\\ \\:\\ \\/__/ \\:\\ \\:\\__\\ \\:\\ \\:\\__\\ \\::/ / \\:\\ \\ \\:\\ /:/ / \\:\\ \\:\\__\\ \\:\\/:/ / \\:\\ \\/__/ /:/ / \\:\\ \\ \\:\\/:/ / \\:\\/:/ / \\::/ / \\:\\__\\ /:/ / \\:\\__\\ \\::/ / \\::/ / \\/__/ \\/__/ \\/__/ \\/__/ \\/__/ \\/__/ Website: https://www.sealos.io/ Address: github.com/labring/sealos Version: 4.3.7-f39b2339 ç®€å•çš„éªŒè¯ä¸€ä¸‹kuberneteså·¥ä½œæ˜¯å¦æ­£å¸¸ [root@ready-kubernetes-master1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION ready-kubernetes-master1 Ready control-plane 5m26s v1.27.7 ready-kubernetes-master2 Ready control-plane 4m50s v1.27.7 ready-kubernetes-master3 Ready control-plane 4m v1.27.7 ready-kubernetes-node1 Ready \u0026lt;none\u0026gt; 3m48s v1.27.7 ready-kubernetes-node2 Ready \u0026lt;none\u0026gt; 3m47s v1.27.7 ready-kubernetes-node3 Ready \u0026lt;none\u0026gt; 3m49s v1.27.7 ","date":"2024-04-15T00:00:00Z","image":"https://img10.360buyimg.com/ddimg/jfs/t1/244560/31/7618/14121/661de12eFfa7f6ba5/0239fa873abb0bd5.jpg","permalink":"http://localhost:1313/kubernetes/sealos%E2%80%94install/","title":"åŸºäºSealOSéƒ¨ç½²é«˜å¯ç”¨çš„kubernetesé›†ç¾¤"},{"content":"é‚£ä¸€å¹´ï¼Œåˆåçš„é˜³å…‰æ€»æ˜¯é‚£ä¹ˆç‚½çƒ­ï¼Œä»¿ä½›èƒ½å°†ä¸€åˆ‡éƒ½èåŒ–ã€‚è¯¾é—´çš„æ“åœºä¸Šï¼ŒåŒå­¦ä»¬çš„æ¬¢å£°ç¬‘è¯­æ­¤èµ·å½¼ä¼ï¼Œæ±—æ°´åœ¨ä»–ä»¬çš„é¢å¤´ä¸Šé—ªè€€ç€å…‰èŠ’ã€‚æˆ‘ä»¬åœ¨è¿™é‡ŒæŒ¥æ´’ç€é’æ˜¥ï¼Œæ¯ä¸€æ¬¡å¥”è·‘éƒ½æ˜¯å¯¹ç”Ÿæ´»çš„çƒ­çˆ±å’Œå¯¹æœªæ¥çš„è¿½é€ã€‚\næ”¾å­¦åçš„è·¯ä¸Šï¼Œå‡ ä¸ªå¥½æœ‹å‹ä»¬ä¸€è¾¹èµ°ä¸€è¾¹è®¨è®ºç€ä»Šå¤©å­¦åˆ°çš„æ–°çŸ¥è¯†ã€‚æˆ‘ä»¬çš„è°ˆè¯ä¸­å……æ»¡äº†å¥½å¥‡å’Œæ¢ç´¢ï¼Œæ¯ä¸€æ­¥éƒ½è¸åœ¨å……æ»¡å¸Œæœ›çš„é“è·¯ä¸Šã€‚å¤å¤©çš„é£ï¼Œå¸¦ç€ç”œç”œçš„èŠ±é¦™ï¼Œä¹Ÿå¸¦ç€å“¥å‡ ä¸ªå¯¹å½¼æ­¤çš„é¼“åŠ±å’Œç¬‘å£°ã€‚\nè¿™è®©æˆ‘è§‰å¾—é’æ˜¥æ˜¯ä¸€é¦–æ°¸ä¸è¤ªè‰²çš„æ­Œï¼Œè€Œå¤å¤©ï¼Œæ­£æ˜¯è¿™é¦–æ­Œæœ€ç¾çš„é«˜æ½®ã€‚åœ¨è¿™ä¸ªå­£èŠ‚é‡Œï¼Œæˆ‘å°†ç”¨æˆ‘çš„ç¬”ï¼Œè®°å½•ä¸‹æ¯ä¸€åˆ»çš„æ„ŸåŠ¨ï¼Œç”¨å¿ƒå»æ„Ÿå—ç”Ÿæ´»çš„æ¯ä¸€åˆ†çƒ­åº¦ã€‚è¿™æ˜¯æˆ‘æ›¾ç»å­¦ç”Ÿæ—¶ä»£ï¼Œæ˜¯æˆ‘çš„é’æ˜¥ç¯‡ç« ã€‚\næ˜¯çš„ï¼Œæ—¶é—´çš„è½¦è½®æ€»æ˜¯å•å‘å‰è¡Œï¼Œå¸¦èµ°äº†æˆ‘ä»¬ç”Ÿå‘½ä¸­çš„è®¸å¤šæ—¥å­ï¼Œç•™ä¸‹äº†å›å¿†å’Œæˆé•¿çš„ç—•è¿¹ã€‚ä¸Šå­¦çš„çš„å¤å¤©ï¼Œé‚£äº›æ— å¿§æ— è™‘çš„æ—¥å­ï¼Œä»¿ä½›æ˜¯ä¸€æ®µç¾å¥½çš„æ—…ç¨‹ï¼Œè™½ç„¶æ— æ³•å†æ¬¡ç»å†ï¼Œä½†å®ƒä»¬åœ¨æˆ‘å¿ƒä¸­ç•™ä¸‹äº†æ·±åˆ»çš„å°è®°ã€‚\næ‘„å½± å¸Œæœ›è¿™ä¸ªå¤å¤©å¯ä»¥ç»™æˆ‘å¸¦æ¥æ›´åŠ ç¾å¥½çš„å›å¿†å§\n\u003c!DOCTYPE html\u003e éœ€è¦åŸå›¾è”ç³»: beilanzhisen@163.com\n","date":"2024-03-17T00:00:00Z","image":"https://img13.360buyimg.com/ddimg/jfs/t1/227509/9/15108/245995/65f6f6e1F8dfc4427/2fce5d471f06a599.jpg","permalink":"http://localhost:1313/tourists/119/","title":"æ—¥è®°-åˆå¼€å§‹æ€€å¿µå¤å¤©"},{"content":"æ—¶é—´è¿‡å¾—çœŸå¿«å•Šï¼Œä¸€è½¬çœ¼å°±æ˜¯2024å¹´äº†ï¼Œçª—å¤–çš„é­ç‚®å£°æ–­æ–­ç»­ç»­çš„ä¼ æ¥ï¼Œä¼¼ä¹åœ¨è¯‰è¯´ç€æ–°æ˜¥æ¥ä¸´çš„å–œæ‚¦ã€‚\næˆ‘æƒ³è¿™ä¹Ÿæ˜¯\u0026quot;å¹´\u0026quot;å¸¦ç»™äººä»¬æœ€æœ´ç´ å’Œæœ€æ·±åˆ»çš„è®°å¿†å§ã€‚\næˆ‘ä¸ç”±è‡ªä¸»åœ°å›å¿†èµ·ç«¥å¹´æ¬¢åº¦æ˜¥èŠ‚çš„æƒ…æ™¯ã€‚è™½ç„¶äººä»¬å¸¸è¯´ç°åœ¨çš„å¹´å‘³æ¸æ·¡ï¼Œå˜å¾—ç®€çº¦è€Œå¹³æ·¡ï¼Œä½†ä»”ç»†æ·±æ€ï¼Œå…¶å®å˜çš„ä¸æ˜¯å¹´å‘³ï¼Œè€Œæ˜¯æˆ‘ä»¬â€”â€”ä»å­©ç«¥æˆé•¿ä¸ºæˆäººçš„æˆ‘ä»¬ã€‚\nä¸ºä»€ä¹ˆæˆ‘ä»¬ä¼šå¦‚æ­¤æ€€å¿µå„¿æ—¶çš„å¹´å‘³ï¼Ÿé‚£æ˜¯å› ä¸ºé‚£æ—¶çš„å¹´æ˜¯å®¶çš„å›¢èšï¼Œæ˜¯äº²æˆšé—´çƒ­é—¹éå‡¡çš„èšä¼šï¼›æ˜¯æ¯äº²åœ¨å¨æˆ¿é‡Œå¿™ç¢Œçš„èº«å½±ï¼Œçƒ¹é¥ªå‡ºè‰²é¦™ä¿±ä½³çš„å¹´å¤œé¥­ï¼›æ˜¯çˆ¶äº²å’Œå”å”ä»¬é—¨å‰è´´æ˜¥è”ã€æ‚¬æŒ‚çº¢ç¯ç¬¼çš„çƒ­é—¹æ™¯è±¡ï¼›æ˜¯æˆ‘ä»¬ç©¿ç€å´­æ–°çš„è¡£æœï¼Œå¿ƒæ€€æœŸå¾…åœ°è¿æ¥æ–°å¹´ï¼›æ˜¯è¡—å¤´å°ä¼™ä¼´ç‚¹ç‡ƒçš„\u0026quot;é»‘èœ˜è››ç‚®\u0026quot;ï¼Œä»¥åŠæ·±å¤œç©ºä¸­ç»šä¸½å¤ºç›®çš„çƒŸèŠ±ç§€ã€‚\nå¦‚ä»Šï¼Œå°½ç®¡æˆ‘ä»¬æ¯ä¸ªäººæˆ–è®¸éƒ½æ²‰æµ¸åœ¨å„è‡ªç”Ÿæ´»çš„å–§åš£ä¸­ï¼Œæ˜¥èŠ‚çš„ä¼ ç»Ÿä¹ ä¿—å¯èƒ½ä¹Ÿéšç€æ—¶é—´æ‚„ç„¶å‘ç”Ÿäº†å˜åŒ–ï¼Œä½†é‚£ä»½å¯¹ç¾å¥½äº‹ç‰©çš„è¿½æ±‚å’Œæ¸´æœ›æ°¸è¿œä¸ä¼šæ¶ˆå¤±ã€‚æˆ‘ä»¬ä»ç„¶ä¼šåœ¨æ˜¥èŠ‚æœŸé—´å¯»æ‰¾é‚£ä»½å¹´å‘³ï¼Œé‚£ä»½å±äºå®¶çš„æ¸©æš–å’Œå–œæ‚¦ã€‚\nåœ¨è¿™ä¸ªæ–°æ˜¥ä¹‹é™…ï¼Œæˆ‘æƒ³æˆ‘ä»¬éƒ½å¯ä»¥æ‰¾åˆ°å±äºè‡ªå·±çš„åº†ç¥æ–¹å¼ï¼Œæ— è®ºæ˜¯ä¸å®¶äººå›´åä¸€èµ·ï¼Œè¿˜æ˜¯ä¸æœ‹å‹æ¬¢èšä¸€å ‚ï¼ŒæŠ‘æˆ–æ˜¯ç‹¬è‡ªå“å‘³ä¸€æœ¬å¥½ä¹¦ã€å¬ä¸€æ›²æ‚ æ‰¬çš„éŸ³ä¹ã€‚æ¯ä¸€ä¸ªç»†å¾®çš„ç¬é—´éƒ½æ˜¯æˆ‘ä»¬ç”Ÿå‘½ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œå€¼å¾—æˆ‘ä»¬å»çæƒœå’Œçºªå¿µã€‚\nè¿™ä¸€è·¯é‡ä¸Šäº†å¾ˆå¤šé£æ™¯æˆ‘å¾ˆå–œæ¬¢ä¸‹é¢çœ‹çœ‹ç…§ç‰‡å’¯\nè¿™æ˜¯åœ¨å§¨å§¥å®¶é—¨å£çš„æ‘å¤´æ—¥è½ï¼Œå°±æ‹äº†ä¸¤å¼ æ²¡å¤šæ‹å“ˆå“ˆå“ˆ\nå½“ç„¶äº†ï¼Œè¿˜æœ‰ä¸¤åªè¶…çº§å¯çˆ±çš„å’©å’©ç¾Š,ä¸çŸ¥é“å°ç¾Šç¾”å­ç°åœ¨æ˜¯ä¸æ˜¯è¢«å®°äº†\u0026hellip;\u0026hellip;\næ‘å£å±…ç„¶èƒ½çœ‹è§é›ªå±± ä½†æ˜¯ä¸çŸ¥é“æ˜¯ä»€ä¹ˆå±±\nå®é™…ä¸Šä»Šå¹´ä¸€ç‚¹éƒ½ä¸å¼€å¿ƒï¼Œæˆ‘å§¥çˆ·çš„èº«ä½“ä¸æ˜¯å¾ˆå¥½ï¼Œå¯èƒ½ä¹Ÿç†¬ä¸è¿‡ä»Šå¹´äº†ã€‚\nå§¥çˆ·å¯¹æˆ‘çœŸçš„å¾ˆå¥½ï¼Œå¯æ˜¯çœŸçš„æ²¡æœ‰åŠæ³•ï¼Œä½“ä¼šåˆ°äº†é‚£ç§æ— å¥ˆçš„æ— åŠ›æ„Ÿã€‚\næ‰€ä»¥æ— è®ºå¦‚ä½•ï¼Œæ—¶é—´ä¼šç»§ç»­å‰è¡Œï¼Œå²æœˆä¼šç•™ä¸‹å°è®°ï¼Œè€Œæˆ‘ä»¬æ‰€ç»å†çš„æ¯ä¸ªæ˜¥èŠ‚éƒ½å°†æˆä¸ºæˆ‘ä»¬å›å¿†ä¸­æœ€å®è´µçš„ä¸€éƒ¨åˆ†ã€‚ æ–°çš„ä¸€å¹´é‡Œé¢ç»§ç»­åŠ æ²¹å§ï¼ï¼ï¼\n","date":"2024-02-13T00:00:00Z","image":"https://img12.360buyimg.com/ddimg/jfs/t1/168052/22/37891/82207/65ccb958Fa2667100/adca896d87e12693.jpg","permalink":"http://localhost:1313/posts/2024/","title":"æ–°æ˜¥çš„åºç« "},{"content":"ä»Šå¤©èŠèŠå¦‚ä½•æ¥ç®¡ç†æˆ‘ä»¬çš„ä»£ç ä»“åº“ åœ¨è½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­ï¼Œä»£ç ä»“åº“æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ç»„æˆéƒ¨åˆ†ã€‚å®ƒä¸ä»…æ˜¯å­˜å‚¨ä»£ç çš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯å›¢é˜Ÿåä½œå’Œç‰ˆæœ¬æ§åˆ¶çš„åŸºç¡€ã€‚å› æ­¤ï¼Œç®¡ç†å¥½è‡ªå·±çš„ä»£ç ä»“åº“è‡³å…³é‡è¦ã€‚\næœ¬æ–‡å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨Gitæ¥ç®¡ç†è‡ªå·±çš„ä»£ç ä»“åº“ã€‚\néƒ¨ç½²GitLab ç›¸è¾ƒäºä¼ åŠ¨çš„éƒ¨ç½²æ–¹å¼æ¯”è¾ƒç¹çï¼Œæˆ‘è¿™é‡Œç›´æ¥é‡‡ç”¨dockerçš„éƒ¨ç½²æ–¹å¼æ¥éƒ¨ç½²gitlabä»¥æ–¹ä¾¿åç»­ç®¡ç†ã€‚\nè¯´ä¸€ä¸‹ä½¿ç”¨Dockeræ¥éƒ¨ç½²çš„ä¸€äº›ç—›ç‚¹ï¼š\næ•°æ®å¤‡ä»½ï¼šåœ¨Dockerä¸­è¿è¡ŒGitlabï¼Œéœ€è¦å®šæœŸå¤‡ä»½æ•°æ®ä»¥é˜²æ­¢æ•°æ®ä¸¢å¤±ã€‚ä½†æ˜¯å¤‡ä»½æ•°æ®çš„è¿‡ç¨‹å¯èƒ½ä¼šå¾ˆéº»çƒ¦ï¼Œå¹¶ä¸”éœ€è¦è®¾ç½®åˆé€‚çš„ç­–ç•¥æ¥é¿å…æ•°æ®ä¸¢å¤±ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘è§‰å¾—ä¸ç®¡æ˜¯GitLabè¿˜æ˜¯å…¶ä»–çš„åº”ç”¨ï¼Œä¿è¯æ•°æ®çš„å®Œæ•´å¯é æ€§æ˜¯è‡³å…³é‡è¦çš„ã€‚ ç‰ˆæœ¬æ›´æ–°ï¼šDockeréƒ¨ç½²Gitlabéœ€è¦æ—¶åˆ»å…³æ³¨ç‰ˆæœ¬æ›´æ–°ï¼Œéœ€è¦è¿›è¡Œå‡çº§æˆ–è€…è¿ç§»ï¼Œä¸»è¦æ˜¯æ•°æ®è¿ç§»çš„æ•°æ®ä¿éšœå·¥ä½œéœ€è¦é¢å¤–æ³¨æ„ã€‚ å¥½äº†æˆ‘ä»¬å¸¦ç€ä¸Šé¢çš„ä¸¤ç‚¹é—®é¢˜ï¼Œæˆ‘ä»¬å…ˆæ¥ä½¿ç”¨dockeréƒ¨ç½²GitLabç„¶åå†æ…¢æ…¢æ¢ç´¢ã€‚\nDockerCompose æˆ‘ä»¬ä½¿ç”¨docker-composeæ¥éƒ¨ç½²gitlabä»“åº“ç¨‹åºï¼Œæ³¨æ„é•œåƒç‰ˆæœ¬ã€‚å¦‚æœä½ æ˜¯å°Šè´µçš„eeç”¨æˆ·è¯·ä¿®æ”¹é•œåƒã€‚å¦‚æœä½ æ˜¯æ™®é€šçš„ceç”¨æˆ·ç›´æ¥å¤åˆ¶å°±è¡Œã€‚ version: \u0026#39;3.6\u0026#39; services: web: image: \u0026#39;gitlab/gitlab-ce:latest\u0026#39; restart: always # æŒ‡å®šgitlabä¸»æœºåç§° hostname: \u0026#39;gitlab.example.com\u0026#39; environment: GITLAB_OMNIBUS_CONFIG: | # gitlabè®¿é—®åœ°å€ external_url \u0026#39;http://10.1.6.100\u0026#39; ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;2212:22\u0026#39; # é˜²æ­¢ä¸æœ¬åœ°çš„22ç«¯å£è¿›è¡Œå†²çª volumes: - \u0026#39;/data/gitlab-app/config:/etc/gitlab\u0026#39; - \u0026#39;/data/gitlab-app/logs:/var/log/gitlab\u0026#39; - \u0026#39;/data/gitlab-app/data:/var/opt/gitlab\u0026#39; shm_size: \u0026#39;256m\u0026#39; å¯åŠ¨gitlabç¨‹åº [root@localhost gitlab-app]# docker-compose up -d [+] Running 1/1 âœ” Container gitlab-app-web-1 Started éƒ¨ç½²å®Œæˆåçš„ä¸€äº›ç›¸å…³è®¾ç½®å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ å®é™…ä¸Šåˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„ä¸€ä¸ªä»£ç ä»“åº“å°±å·²ç»åˆ›å»ºå®Œæˆäº†ï¼Œå‰©ä¸‹çš„å·¥ä½œå°±æ˜¯åœ¨UIç•Œé¢ç‚¹ç‚¹ç‚¹ã€‚æˆ‘å°±ä¸å¤šèµ˜è¿°äº†ä¸»è¦è®²è®²å¦‚ä½•å¤‡ä»½çš„é—®é¢˜ã€‚\nå¦‚ä½•é«˜æ•ˆçš„è¿›è¡Œå®¹å™¨å¤‡ä»½ å½“ç„¶äº†å¦‚æœä½ æƒ³ä¿è¯æ›´å®‰å…¨çš„æ•°æ®å¤‡ä»½å¯ä»¥åœ¨æ·±å¤œstopæ‰ä½ çš„ä»£ç ä»“åº“ä»è€Œè¿›è¡Œåœæœºå¤‡ä»½(æš‚ä¸é‡‡å–) ç®€å•å¤‡ä»½å’Œæ‰©å±•å¤‡ä»½ ç®€å•å¤‡ä»½ å¦‚æœæ‚¨ä½¿ç”¨æ•°æ®é‡å°‘äº 100 GBçš„å¯ä»¥ä½¿ç”¨ä¸€ä¸‹ä¸‰ä¸ªæ­¥éª¤è¿›è¡Œå¤‡ä»½\nè¿è¡Œå¤‡ä»½å‘½ä»¤ GitLab 12.2 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼š docker exec -t \u0026lt;container name\u0026gt; gitlab-backup create è¾“å‡ºç¤ºä¾‹\nDumping database tables: - Dumping table events... [DONE] - Dumping table issues... [DONE] - Dumping table keys... [DONE] - Dumping table merge_requests... [DONE] - Dumping table milestones... [DONE] - Dumping table namespaces... [DONE] - Dumping table notes... [DONE] - Dumping table projects... [DONE] - Dumping table protected_branches... [DONE] - Dumping table schema_migrations... [DONE] - Dumping table services... [DONE] - Dumping table snippets... [DONE] - Dumping table taggings... [DONE] - Dumping table tags... [DONE] - Dumping table users... [DONE] - Dumping table users_projects... [DONE] - Dumping table web_hooks... [DONE] - Dumping table wikis... [DONE] Dumping repositories: - Dumping repository abcd... [DONE] Creating backup archive: \u0026lt;backup-id\u0026gt;_gitlab_backup.tar [DONE] Deleting tmp directories...[DONE] Deleting old backups... [SKIPPING] è¿™é‡Œè¯´æ˜ä¸€ä¸‹å…³äºå¤‡ä»½ç­–ç•¥é€‰é¡¹çš„é€‰é¡¹\næ›´å¤šåœ°å¤‡ä»½é€‰é¡¹å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ taré»˜è®¤å¤‡ä»½ç­–ç•¥æœ¬è´¨ä¸Šæ˜¯ä½¿ç”¨ Linux å‘½ä»¤å’Œå°†æ•°æ®ä»ç›¸åº”çš„æ•°æ®ä½ç½®æµå¼ä¼ è¾“åˆ°å¤‡ä»½gzipã€‚è¿™åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å·¥ä½œæ­£å¸¸ï¼Œä½†å½“æ•°æ®å¿«é€Ÿå˜åŒ–æ—¶å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜ã€‚\ntarå½“è¯»å–æ•°æ®æ—¶æ•°æ®å‘ç”Ÿå˜åŒ–ï¼Œfile changed as we read itå¯èƒ½ä¼šå‘ç”Ÿé”™è¯¯ï¼Œå¹¶å¯¼è‡´å¤‡ä»½è¿‡ç¨‹å¤±è´¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åä¸º çš„å¤‡ä»½ç­–ç•¥copyã€‚tarè¯¥ç­–ç•¥åœ¨è°ƒç”¨å’Œä¹‹å‰å°†æ•°æ®æ–‡ä»¶å¤åˆ¶åˆ°ä¸´æ—¶ä½ç½®gzipï¼Œä»¥é¿å…é”™è¯¯ã€‚\nå‰¯ä½œç”¨æ˜¯å¤‡ä»½è¿‡ç¨‹ä¼šé¢å¤–å ç”¨ 1X çš„ç£ç›˜ç©ºé—´ã€‚è¯¥è¿‡ç¨‹ä¼šå°½åŠ›æ¸…ç†æ¯ä¸ªé˜¶æ®µçš„ä¸´æ—¶æ–‡ä»¶ï¼Œå› æ­¤é—®é¢˜ä¸ä¼šå˜å¾—å¤æ‚ï¼Œä½†å¯¹äºå¤§å‹å®‰è£…æ¥è¯´ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„å˜åŒ–ã€‚ è¦ä½¿ç”¨è¯¥copyç­–ç•¥è€Œä¸æ˜¯é»˜è®¤çš„æµç­–ç•¥ï¼Œè¯· STRATEGY=copyåœ¨ Rake ä»»åŠ¡å‘½ä»¤ä¸­æŒ‡å®šã€‚\ndocker exec -t \u0026lt;container name\u0026gt; gitlab-backup create STRATEGY=copy è€ƒè™‘å°†å¤‡ä»½å‡ºæ¥çš„ç›¸å…³æ–‡ä»¶ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨: ä¾‹å¦‚ S3ã€Minioç­‰ç¨‹åºã€‚\næ‰‹åŠ¨å¤‡ä»½gitlab.rbå’Œgitlab-secrets.jsonã€‚æ‚¨å¯èƒ½è¿˜éœ€è¦å¤‡ä»½æ‰€æœ‰ TLS å¯†é’¥å’Œè¯ä¹¦ /etc/gitlab/sslã€/etc/gitlab/trusted-certs ä»¥åŠ SSH ä¸»æœºå¯†é’¥ã€‚\nå¦‚æœè¿™ä¸¤ç§æ–‡ä»¶ä¸¢å¤±è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£\næ¢å¤ä¿å­˜çš„æ•°æ® é¦–å…ˆæ¢å¤æ•°æ®è¦æ»¡è¶³çš„ä¸€äº›å‰ææ¡ä»¶\nç›®æ ‡ GitLab å®ä¾‹å¿…é¡»å·²åœ¨è¿è¡Œ ç›®æ ‡ GitLab å®ä¾‹å¿…é¡»å…·æœ‰å®Œå…¨ç›¸åŒçš„ç‰ˆæœ¬ å¿…é¡»æ¢å¤ GitLab æœºå¯†é…ç½®æ–‡ä»¶ æŸäº› GitLab é…ç½®å¿…é¡»ä¸åŸå§‹å¤‡ä»½ç¯å¢ƒåŒ¹é…ï¼šä¾‹å¦‚TLSè¯ä¹¦ç­‰å†…å®¹ æ¢å¤ä½œä¸ºæŒ‚è½½ç‚¹çš„ç›®å½•ï¼šè¯¦ç»†å‚è€ƒ ä¸‹é¢æˆ‘ä»¬æ¥å…·ä½“çœ‹ä¸€ä¸‹å¦‚ä½•æ¢å¤å·²ç»ä»gitlabä¸­å¤‡ä»½çš„æ•°æ®å†…å®¹ä»¥åŠä»“åº“ç­‰ä¿¡æ¯ã€‚\nå¦‚æœä½¿ç”¨Docker Swarmï¼Œå®¹å™¨å¯èƒ½ä¼šåœ¨æ¢å¤è¿‡ç¨‹ä¸­é‡æ–°å¯åŠ¨ï¼Œå› ä¸ºPumaå·²å…³é—­ï¼Œå› æ­¤å®¹å™¨è¿è¡ŒçŠ¶å†µæ£€æŸ¥å¤±è´¥ã€‚è¦è§£å†³æ­¤é—®é¢˜ï¼Œè¯·æš‚æ—¶ç¦ç”¨è¿è¡ŒçŠ¶å†µæ£€æŸ¥æœºåˆ¶ã€‚ # ä¿®æ”¹docker-compose.yaml healthcheck: disable: true éƒ¨ç½²å †æ ˆä¿¡æ¯(ä»…é™äºDockerSwarm)ï¼Œå…³äºä¸ºå•¥è¿™æ ·åšè¯·å‚è€ƒissuse6846 docker stack deploy --compose-file docker-compose.yml mystack æ¢å¤æ­¥éª¤ # é¦–å…ˆåœæ­¢pumaå’Œsidekiq docker exec -it \u0026lt;name of container\u0026gt; gitlab-ctl stop puma docker exec -it \u0026lt;name of container\u0026gt; gitlab-ctl stop sidekiq # ç„¶åæŸ¥çœ‹gitlabç›¸å…³ç»„ä»¶çš„çŠ¶æ€ puma:down sidekiq:down docker exec -it \u0026lt;name of container\u0026gt; gitlab-ctl status # å¼€å§‹æ¢å¤æŒ‡å®šçš„å¤‡ä»½æ–‡ä»¶ docker exec -it \u0026lt;name of container\u0026gt; gitlab-backup restore BACKUP=11493107454_2018_04_25_10.6.4-ce # é‡å¯ä½ çš„gitlab docker-compose restart # æ£€æŸ¥ç›¸å…³çš„å…ƒæ•°æ®ä¿¡æ¯ docker exec -it \u0026lt;name of container\u0026gt; gitlab-rake gitlab:check SANITIZE=true è¿™å¤§æ¦‚å°±æ˜¯ä¸€ä¸ªgitlabå¤‡ä»½å®Œæ•´çš„æ¢å¤è¿‡ç¨‹ï¼Œå½“ç„¶æœ¬æ–‡æ²¡æœ‰æ¶‰åŠåˆ°çš„ç›¸å…³å†…å®¹å¯ä»¥é€šè¿‡å‚è€ƒå®˜ç½‘æ–‡æ¡£(ä¸Šé¢æœ‰å†™)æ¥è¿›è¡Œæ‰©å……ã€‚\n","date":"2024-01-16T00:00:00Z","permalink":"http://localhost:1313/gitlab/GitLabRepositoryManagement/","title":"ç®¡ç†å¥½å†…éƒ¨çš„ä»£ç ä»“åº“-GitLabç¯‡"},{"content":"Kubernetes ä¸­æ¯”è¾ƒæµè¡Œçš„æ—¥å¿—æ”¶é›†è§£å†³æ–¹æ¡ˆæ˜¯ Elasticsearchã€Fluentd å’Œ Kibanaï¼ˆEFKï¼‰æŠ€æœ¯æ ˆï¼Œä¹Ÿæ˜¯å®˜æ–¹ç°åœ¨æ¯”è¾ƒæ¨èçš„ä¸€ç§æ–¹æ¡ˆã€‚\nElasticsearch æ˜¯ä¸€ä¸ªå®æ—¶çš„ã€åˆ†å¸ƒå¼çš„å¯æ‰©å±•çš„æœç´¢å¼•æ“ï¼Œå…è®¸è¿›è¡Œå…¨æ–‡ã€ç»“æ„åŒ–æœç´¢ï¼Œå®ƒé€šå¸¸ç”¨äºç´¢å¼•å’Œæœç´¢å¤§é‡æ—¥å¿—æ•°æ®ï¼Œä¹Ÿå¯ç”¨äºæœç´¢è®¸å¤šä¸åŒç±»å‹çš„æ–‡æ¡£ã€‚\nElasticsearch é€šå¸¸ä¸ Kibana ä¸€èµ·éƒ¨ç½²ï¼ŒKibana æ˜¯ Elasticsearch çš„ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„æ•°æ®å¯è§†åŒ– Dashboardï¼ŒKibana å…è®¸ä½ é€šè¿‡ web ç•Œé¢æ¥æµè§ˆElasticsearch æ—¥å¿—æ•°æ®ã€‚\nFluentdæ˜¯ä¸€ä¸ªæµè¡Œçš„å¼€æºæ•°æ®æ”¶é›†å™¨ï¼Œæˆ‘ä»¬å°†åœ¨ Kubernetes é›†ç¾¤èŠ‚ç‚¹ä¸Šå®‰è£… Fluentdï¼Œé€šè¿‡è·å–å®¹å™¨æ—¥å¿—æ–‡ä»¶ã€è¿‡æ»¤å’Œè½¬æ¢æ—¥å¿—æ•°æ®ï¼Œç„¶åå°†æ•°æ®ä¼ é€’åˆ° Elasticsearch é›†ç¾¤ï¼Œåœ¨è¯¥é›†ç¾¤ä¸­å¯¹å…¶è¿›è¡Œç´¢å¼•å’Œå­˜å‚¨ã€‚\næˆ‘ä»¬å…ˆæ¥é…ç½®å¯åŠ¨ä¸€ä¸ªå¯æ‰©å±•çš„ Elasticsearch é›†ç¾¤ï¼Œç„¶ååœ¨ Kubernetes é›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ª Kibana åº”ç”¨ï¼Œæœ€åé€šè¿‡ DaemonSet æ¥è¿è¡Œ Fluentdï¼Œä»¥ä¾¿å®ƒåœ¨æ¯ä¸ª Kubernetes å·¥ä½œèŠ‚ç‚¹ä¸Šéƒ½å¯ä»¥è¿è¡Œä¸€ä¸ª Podã€‚\nå®‰è£… Elasticsearch é›†ç¾¤ å…ˆåˆ›å»ºä¸€ä¸ªå‘½åç©ºé—´ï¼Œæˆ‘ä»¬å°†åœ¨å…¶ä¸­å®‰è£…æ‰€æœ‰æ—¥å¿—ç›¸å…³çš„èµ„æºå¯¹è±¡ã€‚\nkubectl create ns kube-logging ç¯å¢ƒå‡†å¤‡ ElasticSearch å®‰è£…æœ‰æœ€ä½å®‰è£…è¦æ±‚ï¼Œå¦‚æœå®‰è£…å Pod æ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œè¯·æ£€æŸ¥æ˜¯å¦ç¬¦åˆæœ€ä½è¦æ±‚çš„é…ç½®ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\nèŠ‚ç‚¹ CPUæœ€ä½è¦æ±‚ å†…å­˜æœ€ä½è¦æ±‚ elasticsearch-master æ ¸å¿ƒæ•°\u0026gt;2 å†…å­˜\u0026gt;2G elasticsearch-data æ ¸å¿ƒæ•°\u0026gt;1 å†…å­˜\u0026gt;2G elasticsearch-client æ ¸å¿ƒæ•°\u0026gt;1 å†…å­˜\u0026gt;2G é›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯\né›†ç¾¤ èŠ‚ç‚¹ç±»å‹ å‰¯æœ¬æ•°ç›® å­˜å‚¨å¤§å° ç½‘ç»œæ¨¡å¼ æè¿° elasticsearch master 3 5Gi ClusterIP ä¸»èŠ‚ç‚¹ elasticsearch-data data 3 50Gi ClusterIP æ•°æ®èŠ‚ç‚¹ elasticsearch-client client 2 æ—  NodePort è´Ÿè´£å¤„ç†ç”¨æˆ·è¯·æ±‚ å»ºè®®ä½¿ç”¨ StorageClass æ¥åšæŒä¹…åŒ–å­˜å‚¨ï¼Œå½“ç„¶å¦‚æœä½ æ˜¯çº¿ä¸Šç¯å¢ƒå»ºè®®ä½¿ç”¨ Local PV æˆ–è€… Ceph RBD ä¹‹ç±»çš„å­˜å‚¨æ¥æŒä¹…åŒ– Elasticsearch çš„æ•°æ®ã€‚\nç”±äº ElasticSearch 7.x ç‰ˆæœ¬é»˜è®¤å®‰è£…äº† X-Pack æ’ä»¶ï¼Œå¹¶ä¸”éƒ¨åˆ†åŠŸèƒ½å…è´¹ï¼Œéœ€è¦æˆ‘ä»¬é…ç½®ä¸€äº›å®‰å…¨è¯ä¹¦æ–‡ä»¶ã€‚\nå‡†å¤‡ç”Ÿæˆè¯ä¹¦æ–‡ä»¶ æ³¨æ„ï¼šç”±äºæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯containerdæ‰€ä»¥ä½¿ç”¨nerdctlæ¥è¿è¡Œä¸€ä¸ªå®¹å™¨\nmkdir -p elastic-certs nerdctl run --name elastic-certs -v elastic-certs:/app -it -w /app elasticsearch:7.17.3 /bin/sh -c \\ \u0026#34;elasticsearch-certutil ca --out /app/elastic-stack-ca.p12 --pass \u0026#39;\u0026#39; \u0026amp;\u0026amp; \\ elasticsearch-certutil cert --name security-master --dns \\ security-master --ca /app/elastic-stack-ca.p12 --pass \u0026#39;\u0026#39; --ca-pass \u0026#39;\u0026#39; --out /app/elastic-certificates.p12\u0026#34; # æ‰¾åˆ°nerdctlæŒ‚è½½çš„ç›®å½• cd /var/lib/nerdctl/1935db59/volumes/default/elastic-certs/_data/ # è¿™ä¸ªæ¯ä¸ªäººæ˜¯ä¸ä¸€æ ·çš„ å¯ä»¥è‡ªå·±æœç´¢ä¸€ä¸‹ mv * /root/elastic-certs/ cd /root/elastic-certs/ \u0026amp;\u0026amp; openssl pkcs12 -nodes -passin pass:\u0026#39;\u0026#39; -in elastic-certificates.p12 -out elastic-certificate.pem æ·»åŠ è¯ä¹¦åˆ°kubernetes kubectl create secret -n kube-logging generic elastic-certs --from-file=elastic-certificates.p12 # è®¾ç½®é›†ç¾¤ç”¨æˆ·åå’Œå¯†ç  kubectl create secret -n kube-logging generic elastic-auth --from-literal=username=elastic --from-literal=password=elastic-master å‡†å¤‡å®‰è£…Elasticé›†ç¾¤ é‡‡ç”¨Helmçš„æ–¹å¼æ¥æ·»åŠ elasticsearchä»“åº“ helm repo add elastic https://helm.elastic.co helm repo update ElaticSearch å®‰è£…éœ€è¦å®‰è£…ä¸‰æ¬¡ï¼Œåˆ†åˆ«å®‰è£… Masterã€Dataã€Client èŠ‚ç‚¹ï¼ŒMaster èŠ‚ç‚¹è´Ÿè´£é›†ç¾¤é—´çš„ç®¡ç†å·¥ä½œï¼›Data èŠ‚ç‚¹è´Ÿè´£å­˜å‚¨æ•°æ®ï¼›Client èŠ‚ç‚¹è´Ÿè´£ä»£ç† ElasticSearch Cluster é›†ç¾¤ï¼Œè´Ÿè½½å‡è¡¡ã€‚ 2. æ‹‰å–elasticsearch\nhelm pull elastic/elasticsearch --untar --version 7.17.3 cd elasticsearch/ åœ¨Chartç›®å½•ä¸‹åˆ›å»ºå¯¹åº”èŠ‚ç‚¹èŠ‚ç‚¹çš„valuesæ–‡ä»¶ ä»¥ä¸‹æ˜¯master-value.yaml\n# è®¾ç½®é›†ç¾¤åç§° clusterName: \u0026#34;elasticsearch\u0026#34; # è®¾ç½®èŠ‚ç‚¹åç§° nodeGroup: \u0026#34;master\u0026#34; # è®¾ç½®è§’è‰² roles: master: \u0026#34;true\u0026#34; ingest: \u0026#34;false\u0026#34; data: \u0026#34;false\u0026#34; # é•œåƒ image: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch\u0026#34; imageTag: \u0026#34;7.17.3\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # å‰¯æœ¬æ•° replicas: 3 # ---èµ„æºé…ç½®--- esJavaOpts: \u0026#34;-Xmx1g -Xms1g\u0026#34; resources: requests: cpu: \u0026#34;2000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; limits: cpu: \u0026#34;2000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; # æ•°æ®æŒä¹…å·é…ç½® persistence: enabled: true # å­˜å‚¨æ•°æ®å¤§å°é…ç½® volumeClaimTemplate: storageClassName: managed-nfs-storage # å®šä¹‰ä½ è‡ªå·±çš„å­˜å‚¨ç±» accessModes: [\u0026#39;ReadWriteOnce\u0026#39;] resources: requests: storage: 5Gi # ---å®‰å…¨è®¾ç½®--- # è®¾ç½®åè®®ï¼Œå¯é…ç½®ä¸º httpã€https protocol: http # è¯ä¹¦æŒ‚è½½é…ç½®ï¼Œè¿™é‡Œæˆ‘ä»¬æŒ‚å…¥ä¸Šé¢åˆ›å»ºçš„è¯ä¹¦ secretMounts: - name: elastic-certs secretName: elastic-certs path: /usr/share/elasticsearch/config/certs defaultMode: 0755 # å…è®¸æ‚¨åœ¨/usr/share/elasticsearch/config/ä¸­æ·»åŠ ä»»ä½•è‡ªå®šä¹‰é…ç½®æ–‡ä»¶,ä¾‹å¦‚ elasticsearch.ymlã€log4j2.properties # ElasticSearch 7.x é»˜è®¤å®‰è£…äº† x-pack æ’ä»¶ï¼Œéƒ¨åˆ†åŠŸèƒ½å…è´¹ï¼Œè¿™é‡Œæˆ‘ä»¬é…ç½®ä¸‹ # ä¸‹é¢æ³¨æ‰çš„éƒ¨åˆ†ä¸ºé…ç½® https è¯ä¹¦ï¼Œé…ç½®æ­¤éƒ¨åˆ†è¿˜éœ€è¦é…ç½® helm å‚æ•° protocol å€¼æ”¹ä¸º https esConfig: elasticsearch.yml: | xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.enabled: true # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # ç¯å¢ƒå˜é‡é…ç½®ï¼Œè¿™é‡Œå¼•å…¥ä¸Šé¢è®¾ç½®çš„ç”¨æˆ·åã€å¯†ç  secret æ–‡ä»¶ extraEnvs: - name: ELASTIC_USERNAME valueFrom: secretKeyRef: name: elastic-auth key: username - name: ELASTIC_PASSWORD valueFrom: secretKeyRef: name: elastic-auth key: password # ---è°ƒåº¦è®¾ç½®--- # è®¾ç½®è°ƒåº¦ç­–ç•¥ # - hardï¼šåªæœ‰å½“æœ‰è¶³å¤Ÿçš„èŠ‚ç‚¹æ—¶ Pod æ‰ä¼šè¢«è°ƒåº¦ï¼Œå¹¶ä¸”å®ƒä»¬æ°¸è¿œä¸ä¼šå‡ºç°åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Š # - softï¼šå°½æœ€å¤§åŠªåŠ›è°ƒåº¦ antiAffinity: \u0026#39;soft\u0026#39; tolerations: - operator: \u0026#34;Exists\u0026#34; # å®¹å¿å…¨éƒ¨æ±¡ç‚¹ ä»¥ä¸‹æ˜¯data-value.yamlçš„å†…å®¹\n# è®¾ç½®é›†ç¾¤åç§° clusterName: \u0026#34;elasticsearch\u0026#34; # è®¾ç½®èŠ‚ç‚¹åç§° nodeGroup: \u0026#34;data\u0026#34; # è®¾ç½®è§’è‰² roles: master: \u0026#34;false\u0026#34; ingest: \u0026#34;true\u0026#34; data: \u0026#34;true\u0026#34; # é•œåƒ image: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch\u0026#34; imageTag: \u0026#34;7.17.3\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # å‰¯æœ¬æ•° replicas: 3 # ---èµ„æºé…ç½®--- esJavaOpts: \u0026#34;-Xmx1g -Xms1g\u0026#34; resources: requests: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; limits: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; # æ•°æ®æŒä¹…å·é…ç½® persistence: enabled: true # å­˜å‚¨æ•°æ®å¤§å°é…ç½® volumeClaimTemplate: storageClassName: managed-nfs-storage # å®šä¹‰ä½ è‡ªå·±çš„å­˜å‚¨ç±» accessModes: [\u0026#39;ReadWriteOnce\u0026#39;] resources: requests: storage: 20Gi # ---å®‰å…¨è®¾ç½®--- # è®¾ç½®åè®®ï¼Œå¯é…ç½®ä¸º httpã€https protocol: http # è¯ä¹¦æŒ‚è½½é…ç½®ï¼Œè¿™é‡Œæˆ‘ä»¬æŒ‚å…¥ä¸Šé¢åˆ›å»ºçš„è¯ä¹¦ secretMounts: - name: elastic-certs secretName: elastic-certs path: /usr/share/elasticsearch/config/certs defaultMode: 0755 # å…è®¸æ‚¨åœ¨/usr/share/elasticsearch/config/ä¸­æ·»åŠ ä»»ä½•è‡ªå®šä¹‰é…ç½®æ–‡ä»¶,ä¾‹å¦‚ elasticsearch.ymlã€log4j2.properties # ElasticSearch 7.x é»˜è®¤å®‰è£…äº† x-pack æ’ä»¶ï¼Œéƒ¨åˆ†åŠŸèƒ½å…è´¹ï¼Œè¿™é‡Œæˆ‘ä»¬é…ç½®ä¸‹ # ä¸‹é¢æ³¨æ‰çš„éƒ¨åˆ†ä¸ºé…ç½® https è¯ä¹¦ï¼Œé…ç½®æ­¤éƒ¨åˆ†è¿˜éœ€è¦é…ç½® helm å‚æ•° protocol å€¼æ”¹ä¸º https esConfig: elasticsearch.yml: | xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.enabled: true # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # ç¯å¢ƒå˜é‡é…ç½®ï¼Œè¿™é‡Œå¼•å…¥ä¸Šé¢è®¾ç½®çš„ç”¨æˆ·åã€å¯†ç  secret æ–‡ä»¶ extraEnvs: - name: ELASTIC_USERNAME valueFrom: secretKeyRef: name: elastic-auth key: username - name: ELASTIC_PASSWORD valueFrom: secretKeyRef: name: elastic-auth key: password ä»¥ä¸‹æ˜¯client-value.yaml\n# è®¾ç½®é›†ç¾¤åç§° clusterName: \u0026#34;elasticsearch\u0026#34; # è®¾ç½®èŠ‚ç‚¹åç§° nodeGroup: \u0026#34;client\u0026#34; # è®¾ç½®è§’è‰² roles: master: \u0026#34;false\u0026#34; ingest: \u0026#34;false\u0026#34; data: \u0026#34;false\u0026#34; # é•œåƒ image: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch\u0026#34; imageTag: \u0026#34;7.17.3\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # å‰¯æœ¬æ•° replicas: 1 # ---èµ„æºé…ç½®--- esJavaOpts: \u0026#34;-Xmx1g -Xms1g\u0026#34; resources: requests: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; limits: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; # æ•°æ®æŒä¹…å·é…ç½® persistence: enabled: false # ---å®‰å…¨è®¾ç½®--- # è®¾ç½®åè®®ï¼Œå¯é…ç½®ä¸º httpã€https protocol: http # è¯ä¹¦æŒ‚è½½é…ç½®ï¼Œè¿™é‡Œæˆ‘ä»¬æŒ‚å…¥ä¸Šé¢åˆ›å»ºçš„è¯ä¹¦ secretMounts: - name: elastic-certs secretName: elastic-certs path: /usr/share/elasticsearch/config/certs defaultMode: 0755 # å…è®¸æ‚¨åœ¨/usr/share/elasticsearch/config/ä¸­æ·»åŠ ä»»ä½•è‡ªå®šä¹‰é…ç½®æ–‡ä»¶,ä¾‹å¦‚ elasticsearch.ymlã€log4j2.properties # ElasticSearch 7.x é»˜è®¤å®‰è£…äº† x-pack æ’ä»¶ï¼Œéƒ¨åˆ†åŠŸèƒ½å…è´¹ï¼Œè¿™é‡Œæˆ‘ä»¬é…ç½®ä¸‹ # ä¸‹é¢æ³¨æ‰çš„éƒ¨åˆ†ä¸ºé…ç½® https è¯ä¹¦ï¼Œé…ç½®æ­¤éƒ¨åˆ†è¿˜éœ€è¦é…ç½® helm å‚æ•° protocol å€¼æ”¹ä¸º https esConfig: elasticsearch.yml: | xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.enabled: true # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 # ç¯å¢ƒå˜é‡é…ç½®ï¼Œè¿™é‡Œå¼•å…¥ä¸Šé¢è®¾ç½®çš„ç”¨æˆ·åã€å¯†ç  secret æ–‡ä»¶ extraEnvs: - name: ELASTIC_USERNAME valueFrom: secretKeyRef: name: elastic-auth key: username - name: ELASTIC_PASSWORD valueFrom: secretKeyRef: name: elastic-auth key: password # ----æœåŠ¡è®¾ç½®---- service: type: NodePort nodePort: \u0026#39;30200\u0026#39; å¼€å§‹éƒ¨ç½²ç›¸å…³èŠ‚ç‚¹ helm upgrade --install elasticsearch-master -f master-values.yaml --namespace kube-logging ./ # éƒ¨ç½²master helm upgrade --install elasticsearch-data -f data-values.yaml --namespace kube-logging ./ # éƒ¨ç½²data helm upgrade --install elasticsearch-client -f client-values.yaml --namespace kube-logging ./ # éƒ¨ç½² client æ­£å¸¸æƒ…å†µä¸‹çœ‹åˆ°æ‰€æœ‰èŠ‚ç‚¹éƒ½å¤„äºrunningçŠ¶æ€å³å¯\n[root@Online-Beijing-master1 elasticsearch]# kubectl get pods -n kube-logging NAME READY STATUS RESTARTS AGE elasticsearch-client-0 1/1 Running 0 13m elasticsearch-data-0 1/1 Running 0 17m elasticsearch-data-1 1/1 Running 0 17m elasticsearch-data-2 1/1 Running 0 17m elasticsearch-master-0 1/1 Running 0 43m elasticsearch-master-1 1/1 Running 0 43m elasticsearch-master-2 1/1 Running 0 43m å®‰è£…Kibana ä¾æ—§ä½¿ç”¨helmçš„æ–¹å¼è¿›è¡Œéƒ¨ç½²\nä½¿ç”¨helm pullæ‹‰å–KibanaåŒ…æ¥è¿›è¡Œè§£å‹ helm pull elastic/kibana --untar --version 7.17.3 cd kibana å®šä¹‰ä¸€ä¸ªåå­—ä¸ºcustom-value.yamlçš„æ–‡ä»¶ # æŒ‡å®šé•œåƒä¸é•œåƒç‰ˆæœ¬ image: \u0026#39;kibana\u0026#39; imageTag: \u0026#39;7.17.3\u0026#39; # é…ç½® ElasticSearch åœ°å€ elasticsearchHosts: \u0026#39;http://elasticsearch-client:9200\u0026#39; # ç¯å¢ƒå˜é‡é…ç½®ï¼Œè¿™é‡Œå¼•å…¥ä¸Šé¢è®¾ç½®çš„ç”¨æˆ·åã€å¯†ç  secret æ–‡ä»¶ extraEnvs: - name: \u0026#39;ELASTICSEARCH_USERNAME\u0026#39; valueFrom: secretKeyRef: name: elastic-auth key: username - name: \u0026#39;ELASTICSEARCH_PASSWORD\u0026#39; valueFrom: secretKeyRef: name: elastic-auth key: password resources: requests: cpu: \u0026#39;500m\u0026#39; memory: \u0026#39;1Gi\u0026#39; limits: cpu: \u0026#39;500m\u0026#39; memory: \u0026#39;1Gi\u0026#39; # kibana é…ç½®ä¸­æ·»åŠ è¯­è¨€é…ç½®ï¼Œè®¾ç½® kibana ä¸ºä¸­æ–‡ kibanaConfig: kibana.yml: | i18n.locale: \u0026#34;zh-CN\u0026#34; service: type: NodePort nodePort: \u0026#39;30601\u0026#39; éƒ¨ç½²kibana helm install kibana -f custom-value.yaml --namespace kube-logging . éƒ¨ç½²Fluentd Fluentd æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ—¥å¿—èšåˆå™¨ï¼Œæ˜¯ç”¨ Ruby ç¼–å†™çš„ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå¥½åœ°æ‰©å±•ã€‚å¯¹äºå¤§éƒ¨åˆ†ä¼ä¸šæ¥è¯´ï¼ŒFluentd è¶³å¤Ÿé«˜æ•ˆå¹¶ä¸”æ¶ˆè€—çš„èµ„æºç›¸å¯¹è¾ƒå°‘ï¼Œå¦å¤–ä¸€ä¸ªå·¥å…· Fluent-bit æ›´è½»é‡çº§ï¼Œå ç”¨èµ„æºæ›´å°‘ï¼Œä½†æ˜¯æ’ä»¶ç›¸å¯¹ Fluentd æ¥è¯´ä¸å¤Ÿä¸°å¯Œï¼Œæ‰€ä»¥æ•´ä½“æ¥è¯´ï¼ŒFluentd æ›´åŠ æˆç†Ÿï¼Œä½¿ç”¨æ›´åŠ å¹¿æ³›ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ Fluentd æ¥ä½œä¸ºæ—¥å¿—æ”¶é›†å·¥å…·ã€‚\nå·¥ä½œåŸç† Fluentd é€šè¿‡ä¸€ç»„ç»™å®šçš„æ•°æ®æºæŠ“å–æ—¥å¿—æ•°æ®ï¼Œå¤„ç†åï¼ˆè½¬æ¢æˆç»“æ„åŒ–çš„æ•°æ®æ ¼å¼ï¼‰å°†å®ƒä»¬è½¬å‘ç»™å…¶ä»–æœåŠ¡ï¼Œæ¯”å¦‚ Elasticsearchã€å¯¹è±¡å­˜å‚¨ç­‰ç­‰ã€‚Fluentd æ”¯æŒè¶…è¿‡ 300 ä¸ªæ—¥å¿—å­˜å‚¨å’Œåˆ†ææœåŠ¡ï¼Œæ‰€ä»¥åœ¨è¿™æ–¹é¢æ˜¯éå¸¸çµæ´»çš„ã€‚ä¸»è¦è¿è¡Œæ­¥éª¤å¦‚ä¸‹ï¼š\né¦–å…ˆ Fluentd ä»å¤šä¸ªæ—¥å¿—æºè·å–æ•°æ® ç»“æ„åŒ–å¹¶ä¸”æ ‡è®°è¿™äº›æ•°æ® ç„¶åæ ¹æ®åŒ¹é…çš„æ ‡ç­¾å°†æ•°æ®å‘é€åˆ°å¤šä¸ªç›®æ ‡æœåŠ¡å» å®˜æ–¹æ–‡æ¡£ æ—¥å¿—æºé…ç½® æ¯”å¦‚æˆ‘ä»¬è¿™é‡Œä¸ºäº†æ”¶é›† Kubernetes èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰å®¹å™¨æ—¥å¿—ï¼Œå°±éœ€è¦åšå¦‚ä¸‹çš„æ—¥å¿—æºé…ç½®ï¼š\nidï¼šè¡¨ç¤ºå¼•ç”¨è¯¥æ—¥å¿—æºçš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œè¯¥æ ‡è¯†å¯ç”¨äºè¿›ä¸€æ­¥è¿‡æ»¤å’Œè·¯ç”±ç»“æ„åŒ–æ—¥å¿—æ•°æ® typeï¼šFluentd å†…ç½®çš„æŒ‡ä»¤ï¼Œtail è¡¨ç¤º Fluentd ä»ä¸Šæ¬¡è¯»å–çš„ä½ç½®é€šè¿‡ tail ä¸æ–­è·å–æ•°æ®ï¼Œå¦å¤–ä¸€ä¸ªæ˜¯ http è¡¨ç¤ºé€šè¿‡ä¸€ä¸ª GET è¯·æ±‚æ¥æ”¶é›†æ•°æ®ã€‚ pathï¼štail ç±»å‹ä¸‹çš„ç‰¹å®šå‚æ•°ï¼Œå‘Šè¯‰ Fluentd é‡‡é›† /var/log/containers ç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—ï¼Œè¿™æ˜¯ docker åœ¨ Kubernetes èŠ‚ç‚¹ä¸Šç”¨æ¥å­˜å‚¨è¿è¡Œå®¹å™¨ stdout è¾“å‡ºæ—¥å¿—æ•°æ®çš„ç›®å½•ã€‚ pos_fileï¼šæ£€æŸ¥ç‚¹ï¼Œå¦‚æœ Fluentd ç¨‹åºé‡æ–°å¯åŠ¨äº†ï¼Œå®ƒå°†ä½¿ç”¨æ­¤æ–‡ä»¶ä¸­çš„ä½ç½®æ¥æ¢å¤æ—¥å¿—æ•°æ®æ”¶é›†ã€‚ tagï¼šç”¨æ¥å°†æ—¥å¿—æºä¸ç›®æ ‡æˆ–è€…è¿‡æ»¤å™¨åŒ¹é…çš„è‡ªå®šä¹‰å­—ç¬¦ä¸²ï¼ŒFluentd åŒ¹é…æº/ç›®æ ‡æ ‡ç­¾æ¥è·¯ç”±æ—¥å¿—æ•°æ®ã€‚ \u0026lt;source\u0026gt; @id fluentd-containers.log @type tail # Fluentd å†…ç½®çš„è¾“å…¥æ–¹å¼ï¼Œå…¶åŸç†æ˜¯ä¸åœåœ°ä»æºæ–‡ä»¶ä¸­è·å–æ–°çš„æ—¥å¿—,ç±»ä¼¼äºtailå‘½ä»¤ path /var/log/containers/*.log # æŒ‚è½½çš„å®¿ä¸»æœºå®¹å™¨æ—¥å¿—åœ°å€ pos_file /var/log/es-containers.log.pos tag raw.kubernetes.* # è®¾ç½®æ—¥å¿—æ ‡ç­¾ read_from_head true \u0026lt;parse\u0026gt; # å¤šè¡Œæ ¼å¼åŒ–æˆJSON @type multi_format # ä½¿ç”¨ multi-format-parser è§£æå™¨æ’ä»¶ \u0026lt;pattern\u0026gt; format json # JSON è§£æå™¨ time_key time # æŒ‡å®šäº‹ä»¶æ—¶é—´çš„æ—¶é—´å­—æ®µ time_format %Y-%m-%dT%H:%M:%S.%NZ # æ—¶é—´æ ¼å¼ \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; è¿‡æ»¤ ç”±äº Kubernetes é›†ç¾¤ä¸­åº”ç”¨å¤ªå¤šï¼Œä¹Ÿè¿˜æœ‰å¾ˆå¤šå†å²æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åªå°†æŸäº›åº”ç”¨çš„æ—¥å¿—è¿›è¡Œæ”¶é›†ï¼Œæ¯”å¦‚æˆ‘ä»¬åªé‡‡é›†å…·æœ‰ discovery-log=true è¿™ä¸ª Label æ ‡ç­¾çš„ Pod æ—¥å¿—ï¼Œè¿™ä¸ªæ—¶å€™å°±éœ€è¦ä½¿ç”¨ filterã€‚\n# åˆ é™¤æ— ç”¨çš„å±æ€§ \u0026lt;filter kubernetes.**\u0026gt; @type record_transformer remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash \u0026lt;/filter\u0026gt; # åªä¿ç•™å…·æœ‰discovery-log=trueæ ‡ç­¾çš„Podæ—¥å¿— \u0026lt;filter kubernetes.**\u0026gt; @id filter_log @type grep \u0026lt;regexp\u0026gt; key $.kubernetes.labels.discovery-log pattern ^true$ \u0026lt;/regexp\u0026gt; \u0026lt;/filter\u0026gt; è·¯ç”±è®¾ç½® \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true type_name fluentd host \u0026#34;#{ENV[\u0026#39;OUTPUT_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;OUTPUT_PORT\u0026#39;]}\u0026#34; logstash_format true \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size \u0026#34;#{ENV[\u0026#39;OUTPUT_BUFFER_CHUNK_LIMIT\u0026#39;]}\u0026#34; queue_limit_length \u0026#34;#{ENV[\u0026#39;OUTPUT_BUFFER_QUEUE_LIMIT\u0026#39;]}\u0026#34; overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; matchï¼šæ ‡è¯†ä¸€ä¸ªç›®æ ‡æ ‡ç­¾ï¼Œåé¢æ˜¯ä¸€ä¸ªåŒ¹é…æ—¥å¿—æºçš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œæˆ‘ä»¬è¿™é‡Œæƒ³è¦æ•è·æ‰€æœ‰çš„æ—¥å¿—å¹¶å°†å®ƒä»¬å‘é€ç»™ Elasticsearchï¼Œæ‰€ä»¥éœ€è¦é…ç½®æˆ**ã€‚ idï¼šç›®æ ‡çš„ä¸€ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦ã€‚ typeï¼šæ”¯æŒçš„è¾“å‡ºæ’ä»¶æ ‡è¯†ç¬¦ï¼Œæˆ‘ä»¬è¿™é‡Œè¦è¾“å‡ºåˆ° Elasticsearchï¼Œæ‰€ä»¥é…ç½®æˆ elasticsearchï¼Œè¿™æ˜¯ Fluentd çš„ä¸€ä¸ªå†…ç½®æ’ä»¶ã€‚ log_levelï¼šæŒ‡å®šè¦æ•è·çš„æ—¥å¿—çº§åˆ«ï¼Œæˆ‘ä»¬è¿™é‡Œé…ç½®æˆ infoï¼Œè¡¨ç¤ºä»»ä½•è¯¥çº§åˆ«æˆ–è€…è¯¥çº§åˆ«ä»¥ä¸Šï¼ˆINFOã€WARNINGã€ERRORï¼‰çš„æ—¥å¿—éƒ½å°†è¢«è·¯ç”±åˆ° Elsasticsearchã€‚ host/portï¼šå®šä¹‰ Elasticsearch çš„åœ°å€ï¼Œä¹Ÿå¯ä»¥é…ç½®è®¤è¯ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„ Elasticsearch ä¸éœ€è¦è®¤è¯ï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥æŒ‡å®š host å’Œ port å³å¯ã€‚ logstash_formatï¼šElasticsearch æœåŠ¡å¯¹æ—¥å¿—æ•°æ®æ„å»ºåå‘ç´¢å¼•è¿›è¡Œæœç´¢ï¼Œå°† logstash_format è®¾ç½®ä¸º trueï¼ŒFluentd å°†ä¼šä»¥ logstash æ ¼å¼æ¥è½¬å‘ç»“æ„åŒ–çš„æ—¥å¿—æ•°æ®ã€‚ Bufferï¼š Fluentd å…è®¸åœ¨ç›®æ ‡ä¸å¯ç”¨æ—¶è¿›è¡Œç¼“å­˜ï¼Œæ¯”å¦‚ï¼Œå¦‚æœç½‘ç»œå‡ºç°æ•…éšœæˆ–è€… Elasticsearch ä¸å¯ç”¨çš„æ—¶å€™ã€‚ç¼“å†²åŒºé…ç½®ä¹Ÿæœ‰åŠ©äºé™ä½ç£ç›˜çš„ IOã€‚ å¼€å§‹éƒ¨ç½²Fluentd è¦æ”¶é›† Kubernetes é›†ç¾¤çš„æ—¥å¿—ï¼Œç›´æ¥ç”¨DasemonSet æ§åˆ¶å™¨æ¥éƒ¨ç½² Fluentd åº”ç”¨ï¼Œè¿™æ ·ï¼Œå®ƒå°±å¯ä»¥ä» Kubernetes èŠ‚ç‚¹ä¸Šé‡‡é›†æ—¥å¿—ï¼Œç¡®ä¿åœ¨é›†ç¾¤ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä¸Šå§‹ç»ˆè¿è¡Œä¸€ä¸ª Fluentd å®¹å™¨ã€‚å½“ç„¶å¯ä»¥ç›´æ¥ä½¿ç”¨ Helm æ¥è¿›è¡Œä¸€é”®å®‰è£…ï¼Œä¸ºäº†èƒ½å¤Ÿäº†è§£æ›´å¤šå®ç°ç»†èŠ‚ï¼Œæˆ‘ä»¬è¿™é‡Œè¿˜æ˜¯é‡‡ç”¨æ‰‹åŠ¨æ–¹æ³•æ¥è¿›è¡Œå®‰è£…ã€‚\nå®‰è£…æ–‡æ¡£ é¦–å…ˆåˆ›å»ºfluentdçš„configmap # fluentd-configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: fluentd-conf namespace: kube-logging data: # containerdçš„å®¹å™¨æ—¥å¿— containerd.input.conf: |- \u0026lt;source\u0026gt; @id containerd-fluentd-beta.log # å”¯ä¸€Idï¼šè¿è¡Œæ—¶+æ”¶é›†æ’ä»¶+ç¯å¢ƒ @type tail # Fluentd å†…ç½®çš„è¾“å…¥æ–¹å¼ï¼Œå…¶åŸç†æ˜¯ä¸åœåœ°ä»æºæ–‡ä»¶ä¸­è·å–æ–°çš„æ—¥å¿— path /var/log/containers/*.log # Docker å®¹å™¨æ—¥å¿—è·¯å¾„ pos_file /var/log/es-containers.log.pos # è®°å½•è¯»å–çš„ä½ç½® tag raw.kubernetes.* # è®¾ç½®æ—¥å¿—æ ‡ç­¾ read_from_head true # ä»å¤´è¯»å– \u0026lt;parse\u0026gt; # å¤šè¡Œæ ¼å¼åŒ–æˆJSON # å¯ä»¥ä½¿ç”¨æˆ‘ä»¬ä»‹ç»è¿‡çš„ multiline æ’ä»¶å®ç°å¤šè¡Œæ—¥å¿— @type multi_format # ä½¿ç”¨ multi-format-parser è§£æå™¨æ’ä»¶ \u0026lt;pattern\u0026gt; format json # JSONè§£æå™¨ time_key time # æŒ‡å®šäº‹ä»¶æ—¶é—´çš„æ—¶é—´å­—æ®µ time_format %Y-%m-%dT%H:%M:%S.%NZ # æ—¶é—´æ ¼å¼ \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; # åœ¨æ—¥å¿—è¾“å‡ºä¸­æ£€æµ‹å¼‚å¸¸(å¤šè¡Œæ—¥å¿—)ï¼Œå¹¶å°†å…¶ä½œä¸ºä¸€æ¡æ—¥å¿—è½¬å‘ # https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions \u0026lt;match raw.kubernetes.**\u0026gt; # åŒ¹é…tagä¸ºraw.kubernetes.**æ—¥å¿—ä¿¡æ¯ @id raw.kubernetes @type detect_exceptions # ä½¿ç”¨detect-exceptionsæ’ä»¶å¤„ç†å¼‚å¸¸æ ˆä¿¡æ¯ remove_tag_prefix raw # ç§»é™¤ raw å‰ç¼€ message log multiline_flush_interval 5 \u0026lt;/match\u0026gt; \u0026lt;filter **\u0026gt; # æ‹¼æ¥æ—¥å¿— @id filter_concat @type concat # Fluentd Filter æ’ä»¶ï¼Œç”¨äºè¿æ¥å¤šä¸ªæ—¥å¿—ä¸­åˆ†éš”çš„å¤šè¡Œæ—¥å¿— key message multiline_end_regexp /\\n$/ # ä»¥æ¢è¡Œç¬¦â€œ\\nâ€æ‹¼æ¥ separator \u0026#34;\u0026#34; \u0026lt;/filter\u0026gt; # æ·»åŠ  Kubernetes metadata æ•°æ® \u0026lt;filter kubernetes.**\u0026gt; @id filter_kubernetes_metadata @type kubernetes_metadata \u0026lt;/filter\u0026gt; # ä¿®å¤ ES ä¸­çš„ JSON å­—æ®µ # æ’ä»¶åœ°å€ï¼šhttps://github.com/repeatedly/fluent-plugin-multi-format-parser \u0026lt;filter kubernetes.**\u0026gt; @id filter_parser @type parser # multi-format-parserå¤šæ ¼å¼è§£æå™¨æ’ä»¶ key_name log # åœ¨è¦è§£æçš„æ—¥å¿—ä¸­æŒ‡å®šå­—æ®µåç§° reserve_data true # åœ¨è§£æç»“æœä¸­ä¿ç•™åŸå§‹é”®å€¼å¯¹ remove_key_name_field true # key_name è§£ææˆåŠŸååˆ é™¤å­—æ®µ \u0026lt;parse\u0026gt; @type multi_format \u0026lt;pattern\u0026gt; format json \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format none \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt; # åˆ é™¤ä¸€äº›å¤šä½™çš„å±æ€§ \u0026lt;filter kubernetes.**\u0026gt; @type record_transformer remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash \u0026lt;/filter\u0026gt; # åªä¿ç•™å…·æœ‰kubernetes.log.kubernetes.log/fluentdæ ‡ç­¾çš„Podæ—¥å¿— \u0026lt;filter kubernetes.**\u0026gt; @id filter_log @type grep \u0026lt;regexp\u0026gt; key $.kubernetes.labels.kubernetes.log/fluentd pattern ^true$ \u0026lt;/regexp\u0026gt; \u0026lt;/filter\u0026gt; ###### ç›‘å¬é…ç½®ï¼Œä¸€èˆ¬ç”¨äºæ—¥å¿—èšåˆç”¨ ###### forward.input.conf: |- # ç›‘å¬é€šè¿‡TCPå‘é€çš„æ¶ˆæ¯ \u0026lt;source\u0026gt; @id forward @type forward \u0026lt;/source\u0026gt; output.conf: |- \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true host elasticsearch-client port 9200 user elastic # FLUENT_ELASTICSEARCH_USER | FLUENT_ELASTICSEARCH_PASSWORD password elastic-master logstash_format true logstash_prefix kubernetes-cluster request_timeout 30s \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; åˆ›å»ºç›¸å…³çš„Rbacæƒé™ --- apiVersion: v1 kind: ServiceAccount metadata: name: fluentd namespace: kube-logging --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: fluentd rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods - namespaces verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd roleRef: kind: ClusterRole name: fluentd apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: fluentd namespace: kube-logging åˆ›å»ºfluentdçš„daemonset è¿™ä¸ªæ˜¯æœ€æ–°çš„ç‰ˆæœ¬è¿˜åœ¨ç ”ç©¶ä¸­,ç”¨ä¸‹é¢çš„ç‰ˆæœ¬ã€‚\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: kube-logging labels: k8s-app: fluentd-logging version: v1 spec: selector: matchLabels: k8s-app: fluentd-logging version: v1 template: metadata: labels: k8s-app: fluentd-logging version: v1 spec: serviceAccount: fluentd serviceAccountName: fluentd tolerations: - key: node-role.kubernetes.io/control-plane effect: NoSchedule - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch env: - name: K8S_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: FLUENT_ELASTICSEARCH_HOST value: \u0026#34;elasticsearch-client-headless.kube-logging.svc.cluster.local\u0026#34; - name: FLUENT_ELASTICSEARCH_PORT value: \u0026#34;9200\u0026#34; - name: FLUENT_ELASTICSEARCH_SCHEME value: \u0026#34;http\u0026#34; # Option to configure elasticsearch plugin with self signed certs # ================================================================ - name: FLUENT_ELASTICSEARCH_SSL_VERIFY value: \u0026#34;true\u0026#34; # Option to configure elasticsearch plugin with tls # ================================================================ - name: FLUENT_ELASTICSEARCH_SSL_VERSION value: \u0026#34;TLSv1_2\u0026#34; # X-Pack Authentication # ===================== - name: FLUENT_ELASTICSEARCH_USER value: \u0026#34;elastic\u0026#34; - name: FLUENT_ELASTICSEARCH_PASSWORD value: \u0026#34;elastic-master\u0026#34; - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX value: \u0026#34;kubernetes-cluster\u0026#34; resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: fluentconfig mountPath: /fluentd/etc/custom - name: dockercontainerlogdirectory mountPath: /var/log/pods readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: fluentconfig configMap: name: fluentd-conf - name: dockercontainerlogdirectory hostPath: path: /var/log/pods éº»çƒ¦ç”¨ä¸‹é¢çš„è¿›è¡Œéƒ¨ç½²\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: kube-logging labels: app: fluentd kubernetes.io/cluster-service: \u0026#39;true\u0026#39; spec: selector: matchLabels: app: fluentd template: metadata: labels: app: fluentd kubernetes.io/cluster-service: \u0026#39;true\u0026#39; spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule serviceAccountName: fluentd containers: - name: fluentd image: quay.io/fluentd_elasticsearch/fluentd:v3.4.0 volumeMounts: - name: fluentconfig mountPath: /etc/fluent/config.d - name: varlog mountPath: /var/log volumes: - name: fluentconfig configMap: name: fluentd-conf - name: varlog hostPath: path: /var/log å…³äºä¿ç•™æŒ‡å®šæ ‡ç­¾çš„é—®é¢˜ éƒ¨ç½²å®Œæˆä»¥åæˆ‘å‘ç°ä¸€ç›´æœ‰ä¸€ä¸ªå°é—®é¢˜ï¼Œå°±æ˜¯æ— è®ºæˆ‘å¦‚ä½•è®¾ç½®labeléƒ½æ— æ³•è®©elasticsearchè·å–åˆ°æ­£å¸¸çš„æ•°æ®ã€‚\næ ¹æ®è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘è¿›è¡Œäº†æ›´ç»†è‡´çš„æ’æŸ¥ã€‚ç°åœ¨å¾—å‡ºäº†å¦‚ä¸‹çš„ç»“è®ºã€‚\nå¯èƒ½æ˜¯ç”±äºæˆ‘å¯¹çŸ¥è¯†çš„ç¼ºä¹ï¼Œæˆ‘å®šä¹‰çš„æ˜¯Deploymentå½“ä¸­çš„labelæ ‡ç­¾ï¼Œä½†æ˜¯è¿™ä¸ªlabelæ ‡è¯†åªä½œç”¨äºDeploymentæœ¬èº«ï¼Œé€šå¸¸ç”¨ä½œkuberneteé›†ç¾¤ä¸­çš„é€‰æ‹©å™¨åŒ¹é…ï¼Œä¾‹å¦‚æˆ‘ä»¬çš„Serviceè¦å»åŒ¹é…æŸä¸ªDeploymentã€‚\nå…³äºspec.template.metadata.labelsï¼Œæˆ‘å‘ç°è¿™ä¸ªæ‰æ˜¯æˆ‘ä»¬æ­£ç¡®è¦åŒ¹é…çš„labelæ ‡ç­¾é€‰é¡¹ï¼Œå› ä¸ºè¿™äº›æ ‡ç­¾ç”¨äºæ ‡è¯†Deploymentæ‰€åˆ›å»ºçš„Pod\næ‰€ä»¥æœ€åæ€»ç»“å‡ºæ¥çš„é—®é¢˜å°±æ˜¯ï¼Œæˆ‘ä»¬ä¸Šé¢çš„fluentdä¸­å†™çš„è¿‡æ»¤æ’ä»¶ key $.kubernetes.labels.kubernetes.log/fluentdä¸­æ‰€åŒ¹é…çš„labelæ ‡ç­¾åº”å½“æ˜¯spec.template.metadata.labelsçš„label\nspec: replicas: 2 selector: matchLabels: app: canary template: metadata: creationTimestamp: null labels: app: canary kubernetes.log/fluentd: \u0026#39;true\u0026#39; ","date":"2023-12-08T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/177175/31/35833/30306/64cb08eeF5ba90f46/1da6311bbbec8921.jpg","permalink":"http://localhost:1313/kubernetes/efk/","title":"kubernetesåŸºäºEFKçš„æ—¥å¿—è½åœ°å®ç°"},{"content":"äººç”Ÿæ€»è¦å»ä¸€è¶Ÿè¿œæ–¹å§ ä¸‹ä¸€ç«™è¡Œç¨‹ æš‚å®šè¥¿è—ğŸš„\nå®åœ¨æ˜¯ä¸æƒ³ä¸Šç­ï¼Œæˆ‘æ‰¿è®¤æˆ‘è¢«æˆ‘å¼ºå¤§çš„æ„å¿µå é¢†äº†ï¼Œæˆ‘æ²¡å¿ä½ã€‚è¯·å‡ç›´æ¥ä»åŒ—äº¬å»äº†é˜¿åç”˜å­œã€‚\nå¥½äº†ä¸å¼€ç©ç¬‘äº†å“ˆå“ˆå“ˆ\nèµ·åˆæˆ‘ä»¬æ‰“ç®—è‡ªé©¾ï¼Œå¥ˆä½•æˆ‘æ˜¯ä¸ªå°è¶´èœï¼Œå…ˆä¸è¯´é˜¿åé‚£è¾¹çš„è·¯å†µå¥½ä¸å¥½èµ°ï¼Œå°±å¹³æ—¶å¼€ä¸ªå¸‚åŒºæˆ‘éƒ½è´¹åŠ²ï¼Œæ‰€ä»¥è¿™æ¡ç›´æ¥è¢«æˆ‘PASSäº†ã€‚\næ‰€ä»¥é€‰æ‹©äº†æœ€æœ€æœ€æ–¹ä¾¿çš„é£æœºâœˆï¸å‡ºè¡Œ,æ‚¨è¿˜åˆ«è¯´è¿™äº¤é€šå·¥å…·æœ‰ä¸¤æŠŠåˆ·å­(ä¸å¥½æ„æ€ï¼Œæˆ‘çœŸæ˜¯ç¬¬ä¸€æ¬¡åé£æœº),å…¶å®åé£æœºå¤§å®¶æƒ³å¾—é‚£ä¹ˆå¤æ‚ åŸºæœ¬æµç¨‹å…¶å®å°±å¦‚ä¸‹å‡ ç‚¹\nä¹°ç¥¨: ä¸ä¹°ç¥¨æƒ³åƒéœ¸ç‹æœºğŸ¤ª å–ç¥¨ï¼šå¦‚æœä½ éœ€è¦æŠ¥é”€ä»€ä¹ˆçš„å…¶å®å¯ä»¥å»å¯¹åº”çš„èˆªç©ºå…¬å¸æŸœå°å»å–ç™»æœºç‰Œ,å¦‚æœä¸ªäººå‡ºè¡Œçš„è¯å¯ä»¥ç›´æ¥ä½¿ç”¨ç”µå­ç™»æœºç‰Œè¿›è¡Œç›¸å…³çš„ç™»æœºæ“ä½œ å®‰æ£€: å–å®Œç¥¨ä¹‹åå°±å·®ä¸å¤šè¦å®‰æ£€äº†,åŸºæœ¬ä¸Šå®‰æ£€ä¹Ÿå°±æ˜¯ååˆ†é’Ÿå·¦å³çš„äº‹æƒ…,å®‰æ£€å®Œæˆå°±æ­£å¸¸è¿›å…¥å€™æœºæ¥¼æ‰¾ä½ é£æœºæ‰€å¯¹åº”çš„ç™»æœºçª—å£å°±ğŸ‘ŒğŸ» å‡ºå‘ ä¸€å¤§æ—©çš„æˆ‘éƒ½æ¥æœºåœºç­‰ç€äº†ï¼Œå› ä¸ºç¬¬ä¸€æ¬¡çœ‹ç½‘ä¸Šè¯´è¦æå‰2å°æ—¶åˆ°æœºåœºï¼Œæˆ‘åˆ°äº†ä»¥åå‘ç°å…¶å®å¥½åƒä¹Ÿå¹¶æ²¡æœ‰é‚£ä¹ˆå¤¸å¼  è¯´è¯´æˆ‘è¿™æ¬¡å‡†å¤‡çš„ä¸œè¥¿å§\nè¡£æœæ–¹é¢è¡£ğŸ§£: ä¸ªäººå»ºè®®è¿˜æ˜¯ç¾½ç»’æœ+å†²é”‹è¡£èµ·æ­¥,å¸¦å¥½å›´è„–ã€å›´å·¾ã€æ‰‹å¥—ã€é›ªåœ°é´ç­‰ç‰©å“ï¼Œå› ä¸ºé‚£è¾¹å…¶å®è¿˜æ˜¯å¾ˆå†·çš„ï¼Œå› ä¸ºæˆ‘åœ¨é›ªå±±ä¸Šã€‚ ç‰©å“æ–¹é¢ï¼šæˆ‘æ˜¯ä¸ªç”·å­©å­å…¶å®æ²¡æœ‰å¥³ç”Ÿå¸¦çš„é‚£ä¹ˆå¤šç‰©å“ï¼ŒåŸºæœ¬çš„æ¢æ´—è¢œå­ä¸€ä»¶å¤–è¡£å’Œä¸€ä»¶è£¤å­è¶³å¤Ÿ,ç„¶åå°±æ˜¯å‡†å¤‡æ‘„å½±çš„ä¸œè¥¿ï¼Œå¸¦ç€æˆ‘å¹´è¿ˆçš„SONY A6000å’Œå®ƒçš„ä¸¤ä¸ªå¯æ‹†å¸é•œå¤´å“ˆå“ˆå“ˆï¼Œæ°§æ°”ç“¶åŠ¡å¿…å‡†å¤‡å……è¶³,ä¸ªäººç©å®¶ä¸€èˆ¬æƒ…å†µä¸‹æ¥è¯´3ç“¶å­è¶³çŸ£ã€‚å›¢é˜Ÿç©å®¶å»ºè®®å›¢è´­(ä¿å‘½çš„ä¸œè¥¿å•Š)ã€‚ è¯ç‰©æ–¹é¢ï¼šç›¸å…³æ–¹é¢çš„è¯ç‰©å°½é‡è¿˜æ˜¯è¦å¸¦ä¸Šçš„æ¯”å¦‚ä»€ä¹ˆè‚ èƒƒè¯ã€æ„Ÿå†’è¯ã€å’³å—½å‘çƒ§è¯ç­‰ç­‰ã€‚æœ‰äººè¯´å»é«˜æµ·æ‹”åœ°åŒºè¦æå‰åƒä»€ä¹ˆçº¢æ™¯å¤©ä¹‹ç±»çš„ä¸œè¥¿ï¼Œåªèƒ½è¯´è¿™ä¸ªä¸œè¥¿æ˜¯å› äººè€Œå¼‚çš„å§ã€‚ è¯ä»¶ä¿¡æ¯ï¼šèº«ä»½è¯ã€æŠ¤ç…§ã€è¿˜æœ‰æ‰‹æœºå•¥çš„è¿™äº›ä¸œè¥¿è€é“ä»¬åº”è¯¥å¿˜ä¸äº†å§ã€‚ æˆ‘ä»¬ä¹°çš„æ˜¯æ—©ä¸Š7.15çš„æœºç¥¨ï¼Œå¤§æ¦‚æå‰ä¸€ä¸ªåŠå°æ—¶å°±åˆ°äº†æœºåœºï¼Œå…¶å®ç›¸å¯¹äºæ¥è¯´è¿˜æ˜¯æŒºæ—©çš„ï¼Œå“å‘€ä¸è¯´å•¦æ¿€åŠ¨åœ°æˆ‘æ˜¯ä¸€é¡µéƒ½æ²¡ç¡ã€‚ æˆ‘çš„è¡Œç¨‹å¤§æ¦‚å°±æ˜¯åœ¨æ·˜å®è®¢äº†ä¸€ä¸ªå°å›¢ï¼Œä»·æ ¼æ˜¯1340ï¼Œè¡Œç¨‹æ˜¯ä¸¤å¤©ã€‚å¤§æ¦‚çš„è·¯çº¿å°±æ˜¯æ¯•æ£šæ²Ÿ+è¾¾å¤å†°å·ã€‚ å…·ä½“è¡Œç¨‹ä¸‹é¢ä»‹ç»å“ˆå“ˆå“ˆ\nç™»æœºäº†ï¼Œè€³æœºé‡Œç›´æ¥èµ°èµ·ç»æµèˆ±çš„BGMï¼š46Aæˆ‘é çª—è¾¹åä¸‹ï¼Œrapperåç»æµèˆ±é¢å­ä¼¤ä¸ä¼¤\n## ç¬¬ä¸€å¤©è¡Œç¨‹ æˆ‘æŠ¥çš„æ˜¯ä¸€ä¸ª2-6äººçš„å°å›¢ï¼ŒåŸºæœ¬ä¸Šäººéƒ½æ˜¯æ»¡çš„ï¼Œå¤§æ¦‚7.00åˆ°7.30å·¦å³å¸æœºå“¥ä¼šä¸Šé—¨æ¥äººï¼Œç„¶åå‡ºå‘å‰å¾€`æ¯•æ£šæ²Ÿæ™¯åŒº`, å› ä¸ºç¬¬ä¸€å¤©çš„æ—©ä¸­æ™šéƒ½æ˜¯è‡ªç†é¥­è´¹ï¼Œå“¥å‡ ä¸ªç›´æ¥å¹²è„†è·¯ä¸Šæ‰¾äº†ä¸ªé¥­åº—ä¸€èµ·AAçš„åƒäº†ä¸€é¡¿ã€‚ æ¯•æ£šæ²Ÿè¿™ä¸ªåœ°æ–¹è¿˜æŒºéš¾ä¸Šçš„ï¼Œå¸æœºç»™æˆ‘ä»¬å¸¦åˆ°åŠå±±è…°ä¸Š,åŠå±±è…°ä¼šæœ‰ä¸€ä¸ªæ—…å®¢æœåŠ¡å¤§å…ï¼Œåœ¨å¤§å…å¤–è¿˜éœ€è¦å†ç­‰å¤§å·´è½¦æŠŠä½ ä»¬ç»Ÿç»Ÿéƒ½å¸¦èµ°ï¼ï¼ï¼ å…¶å®è¿˜æ˜¯æŒºå–œæ¬¢è¿™ç§æ„Ÿè§‰çš„,åœ¨é«˜æµ·æ‹”åœ°åŒºä¸€å®šè¦æ…¢æ…¢çš„èµ°ï¼Œå°½é‡ä¸è¦å»å¥”è·‘è·³è·ƒï¼Œå› ä¸ºéƒ¨åˆ†äººç¾¤å¾ˆå®¹æ˜“å› æ­¤è€Œé«˜åã€‚ ä¸€å¤©äº†ä¹Ÿç©å„¿ç´¯äº†ï¼Œåƒåƒå–å–å½“æ—¶ä¹Ÿå‡†å¤‡ç¡è§‰äº† é¡ºå¸¦æä¸€å˜´ï¼šå…³äºé«˜åè¿™ä¸ªäº‹æƒ…æˆ‘å¥½æƒ³æ²¡æœ‰å¤ªå¤§çš„ååº”ï¼Œå› ä¸ºåªæ˜¯å•çº¯çš„æ„Ÿè§‰èµ°è·¯æ¯”å¹³æ—¶å–˜çš„æ›´å¿«äº†ï¼Œå¹³æ—¶èµ°åæ­¥å–˜ä¸€å£ï¼Œåœ¨è¿™å„¿å¯èƒ½èµ°äº”æ­¥å°±éœ€è¦å–˜ä¸€å£ã€‚ å¦å¤–æˆ‘å¸¦çš„æ°§æ°”åœ¨æ¯•æ£šæ²Ÿè¿˜æ²¡ç”¨ä¸Šï¼Œæ‰€ä»¥æé†’ä¸€ä¸‹å¤§å®¶å»é«˜æµ·æ‹”åœ°åŒºé‡åŠ›è€Œè¡Œã€‚ ç¬¬äºŒå¤©è¡Œç¨‹ åç»­çš„è¡Œç¨‹å› ä¸ºä½å®¿çš„åœ°æ–¹åœ¨æµ·æ‹”ä¸‰åƒç±³ä»¥ä¸Šï¼Œæˆ‘çš„å¤©é‚£å«ä¸€ä¸ªçœŸçš„å†·ï¼Œæˆ‘ä»¬è¿›äº†é…’åº— èµ¶ç´§æŠŠæ‰€æœ‰èƒ½å–æš–çš„ä¸œè¥¿å…¨éƒ¨Openäº†ã€‚å°±è¿™æ ·åº¦è¿‡äº†è¿™ä¸€å¤©ã€‚\nç¬¬äºŒå¤©ä¸€æ—©è¦åƒè¿‡æ—©é¥­ä»¥åä»ç¾ŠèŒ¸å“ˆå¾·å‰å¾€é»‘æ°´å¿ï¼Œå¤§æ¦‚ä¹Ÿå°±åŠä¸ªå°æ—¶å·¦å³çš„è·¯ç¨‹ï¼Œå› ä¸ºè¾¾å¤å†°å·åœ¨é»‘æ°´å¿å¢ƒå†… æµ·æ‹”å¤§æ¦‚5000ç±³æœ€é«˜ï¼Œå†¬å¤©å…¶å®æ¯”è¾ƒé€‚åˆä¸€äº›æ”€å†°ç­‰æé™è¿åŠ¨(æ™®é€šäººè¯·åˆ«ä½œæ­»å•Šå•Šå•Š)ã€‚\nè€æ ·å­ä¾æ—§æ˜¯ç»™ä½ æ‰”åˆ°åŠå±±è…°ï¼Œè¿™ä¸€æ¬¡åˆ°ä¸æ˜¯åƒä¸Šä¸€æ¬¡ä¸€æ ·å»åšå¤§å·´äº†ï¼Œè¿™æ¬¡æ˜¯ç›´æ¥åœ¨åŠå±±è…°åç¼†è½¦ä¸Šå±±ã€‚æˆ‘å¯»æ€å‘¢è¿™è¦æ˜¯è®©æˆ‘çˆ¬ä¸Šå»æˆ‘å¾—çˆ¬åˆ°ä»€ä¹ˆæ—¶å€™å“ˆå“ˆå“ˆ æˆ‘ä»¬å»çš„æ—¶å€™äººç‰¹åˆ«å¤šï¼Œç¼†è½¦æ’é˜Ÿæ—¶é—´æŒºé•¿çš„å¤§æ¦‚åœ¨1å°æ—¶åˆ°ä¸¤ä¸ªå°æ—¶ä¹‹é—´å·¦å³çš„è¿™ä¸ªåŒºé—´ã€‚å› ä¸ºä»å±±ä¸‹åˆ°å±±ä¸Šåç¼†è½¦è¿˜éœ€è¦15-20åˆ†é’Ÿå·¦å³ã€‚\nä¸­å›½æœºé•¿é‚£ä¸ªé£è·ƒé›ªå±±ğŸ”å°±æ˜¯åœ¨è¿™é‡Œæ‹æ‘„çš„\n\u003c!DOCTYPE html\u003e å…¶å®åˆ°è¿™é‡Œå‘¢æˆ‘ä»¬ä¹Ÿå°±è¿å¤œå›åˆ°äº†æˆéƒ½ä½äº†ä¸€å¤œï¼Œç„¶åæ‰¾ä¸ªé…’åº—ä½äº†ä¸€æ™šï¼Œç¬¬äºŒå¤©ä¸€æ—©èµ¶å›äº†åŒ—äº¬ã€‚\nè¿™ä¸€æ¬¡æ—…è¡Œå¯¹äºæˆ‘æ¥è¯´æŒºéœ‡æ’¼çš„ï¼Œç¬¬ä¸€æ¬¡å»è¿™ä¹ˆè¿œçš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯ç¬¬ä¸€æ¬¡å»è¿™ä¹ˆæ”¾æ¾çš„åœ°æ–¹,å¾ˆé‚£å®šä¹‰æˆ‘çœŸæ­£æ‹¥æœ‰è¿‡ä»€ä¹ˆï¼Œäºæ˜¯æ˜æ²‰å½“ä¸­è¿™ä¸€å¹´åˆè¦è¿‡å»å•¦ã€‚\nä¸‹ä¸€æ¬¡çš„æ—…é€”æ‰“ç®—ä¸ºæœŸ7å¤©çš„è¥¿è—,åä¸Šç»¿çš®ç«è½¦ã€‚å¸Œæœ›å°½å¿«æä¸Šæ—¥ç¨‹å§ã€‚ç”Ÿæ´»åŠ æ²¹,å·¥ä½œåŠ æ²¹â¤ï¸ã€‚\n","date":"2023-11-23T00:00:00Z","image":"https://img13.360buyimg.com/ddimg/jfs/t1/230405/24/4739/81218/65635877F5b2d86e7/53cc1edf42100b84.jpg?width=300px\u0026height=240px","permalink":"http://localhost:1313/tourists/sichuan/","title":"æ—…è¡Œæ—¥è®°-å››å·Â·é˜¿åç”˜å­œè—æ—è‡ªæ²»å·"},{"content":"ä¸ºä»€ä¹ˆéœ€è¦kubernetesï¼Ÿ å¤§è§„æ¨¡å¤šèŠ‚ç‚¹å®¹å™¨è°ƒåº¦ å¿«é€Ÿæ‰©ç¼©å®¹ æ•…éšœè‡ªæ„ˆ å¼¹æ€§ä¼¸ç¼© æŠ€æœ¯è¶‹åŠ¿ ä¸€è‡´æ€§ã€ä¸é”å®š æ—©æœŸå‹å¤šçš„ä¸€äº›æœåŠ¡éƒ½å±äºå•ä½“æœåŠ¡ã€å•èŠ‚ç‚¹ã€å•è¿›ç¨‹çš„ä¸€ç§å•ä½“æœåŠ¡æ¶æ„ï¼Œåç»­éšç€æŠ€æœ¯çš„å‘å±•è¡ç”Ÿå‡ºäº†å®¹å™¨æŠ€æœ¯ã€‚å®¹å™¨æŠ€æœ¯å…¶å®ä¹Ÿä¸èƒ½æ»¡è¶³æˆ‘ä»¬çš„å¤šèŠ‚ç‚¹ã€åˆ†å¸ƒå¼çš„åº”ç”¨æ¶æ„ä½“ç³»ï¼Œä»è€Œè¡ç”Ÿå‡ºäº†kuberneteså®¹å™¨ç¼–æ’å¼•æ“ã€‚ é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ—©æœŸå•ä½“å®¹å™¨æ¶æ„\nå…¶å®å¯¹äºå®¹å™¨åŒ–æŠ€æœ¯å¸¦æ¥äº†é‚£äº›ä¼˜åŠ¿å‘¢?\nå…¶å®æˆ‘è§‰å¾—å®¹å™¨åŒ–å¸¦æ¥çš„æœ€å¤§çš„ä¼˜åŠ¿å°±æ˜¯äº¤ä»˜å’Œéƒ¨ç½²çš„ä¼˜åŠ¿ é‚£ä¹ˆéšä¹‹è€Œæ¥å¸¦æ¥çš„é—®é¢˜æ˜¯:\né‚£ä¹ˆç”±äºDockerçš„å®¹å™¨é•œåƒå¯ä»¥åœ¨Aã€Bã€Cä»»æ„ä¸€å°æœºå™¨ä¸Šè¿è¡Œ,é‚£ä¹ˆæ˜¯å¦å¯ä»¥å½“Aæœºå™¨æ‰€è¿è¡Œçš„é•œåƒæŒ‚æ‰ä»¥åè‡ªåŠ¨çš„å¸®æˆ‘åœ¨Bæœºå™¨ä¸Šè¿›è¡Œé‡å¯å‘¢?\nokey å¸¦ç€è¿™ä¸ªé—®é¢˜ ä¸€èµ·å¾€ä¸‹è¿›è¡Œã€‚\nkubernesç»„ä»¶ å…ˆçœ‹ä¸€å¼ å®˜æ–¹ç»™å‡ºçš„kubernetesçš„æ¶æ„å›¾ å›¾ä¸­åˆ—å‡ºäº†kubernetesçš„ç»„æˆä»¥åŠç›¸å¯¹åº”çš„ç»„ä»¶\nControlPlane: æ§åˆ¶å¹³é¢èŠ‚ç‚¹ Node: å·¥ä½œèŠ‚ç‚¹ Kubelet: ç”¨äºæ§åˆ¶staticPod,å…¶ä¸»è¦å°±æ˜¯ç”¨æ¥æ§åˆ¶é™æ€Podï¼Œå› ä¸ºé™æ€Podä¸å—ApiServerçš„å½±å“ã€‚ Oh ä¸æ’ä¸€å¥å˜´ å­¦åˆ°äº†ä¸€ä¸ªæ–°çš„å‘½ä»¤\n# jqå‘½ä»¤æ˜¯ä¸€ä¸ªç”¨äºå¤„ç†jsonçš„å‘½ä»¤ kubectl get deploy wecho-canary -o json | jq .spec okey ç»§ç»­\u0026hellip; æˆ‘ä»¬æ—¶é•¿è°ˆèµ·åˆ°çš„control-planeå®é™…ä¸Šå¹¶ä¸æ˜¯ä¸€å°æœºå™¨ä»–åªæ˜¯ä¸€ä¸ªæŠ½è±¡å‡ºæ¥çš„æ¦‚å¿µ,å®é™…ä¸Šæˆ‘ä»¬æ˜¯åœ¨è¯´æ‰€è°“çš„control-planeå±‚é¢çš„ç»„ä»¶ã€‚ä¹Ÿå°±æ˜¯è¯´è¿™äº›ç»„ä»¶å¯ä»¥è¿è¡Œåœ¨æ§åˆ¶é¢çš„æœºå™¨ä¸ŠåŒæ—¶ä¹Ÿå¯ä»¥è¿è¡Œåœ¨Nodeæœºå™¨ä¸Š\nkubernetesæ ¸å¿ƒæ¦‚å¿µ ResourceObject: æ˜¯æˆ‘è®¤ä¸ºç›¸å¯¹è€Œè¨€kubernetesé›†ç¾¤å½“ä¸­æ¯”è¾ƒæ ¸å¿ƒçš„èµ„æºå¯¹è±¡,å…¶å®ä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„Podã€Deploymentã€Daemonsetç­‰kubernetesçš„èµ„æºç±»å‹ å¯¹äºä¸€ä¸ªPodè€Œè¨€,kuberneteså¯¹å…¶å®šä¹‰çš„é”®å€¼æ— éä»¥ä¸‹çš„å‡ ç§ [root@Online-Beijing-master1 ~]# kubectl get deploy wecho-canary -o json | jq keys [ \u0026#34;apiVersion\u0026#34;, \u0026#34;kind\u0026#34;, \u0026#34;metadata\u0026#34;, \u0026#34;spec\u0026#34;, // specæè¿°çš„æ˜¯Podé¢„æœŸçš„çŠ¶æ€ \u0026#34;status\u0026#34; ] ä½ å¯ä»¥é€šè¿‡kubectl api-resourceæ¥è·å–kubernetesç›¸å¯¹åº”çš„èµ„æºç±»å‹ã€‚\nkubernetsçš„èµ„æºæäº¤ æˆ‘ä»¬å¹³æ—¶ä½¿ç”¨çš„kubectl run nginx-$RANDOM --image=\u0026quot;nginx:alpine\u0026quot;ç©¶ç«Ÿæ˜¯æ‰§è¡Œäº†ä»€ä¹ˆæ ·çš„å†…å®¹ï¼Ÿ\næ­£å¸¸æ¥è¯´Api-Serveræœ¬èº«å°±æ˜¯æœåŠ¡,é‚£ä¹ˆå½“æˆ‘æŠŠè¯·æ±‚å‘é€ç»™Api-Serverçš„æ—¶å€™,æˆ‘æ˜¯ä»¥ä»€ä¹ˆæ ·çš„è¯·æ±‚å†…å®¹è¿›è¡Œäº†æäº¤?é‚£Api-Serveræ¥æ”¶äº†æˆ‘çš„è¯·æ±‚å†…å®¹åˆå¯¹æˆ‘çš„è¯·æ±‚å†…å®¹åšå‡ºäº†ä»€ä¹ˆæ ·çš„å¤„ç†å‘¢?\né¦–å…ˆæˆ‘ä»¬æ¥çœ‹å®é™…ä½œä¸ºå®¢æˆ·ç«¯,ä¹Ÿå°±æ˜¯clientç«¯æäº¤çš„è¯·æ±‚\n# å¯ä»¥é€šè¿‡--dry-run=clientæ¥æ¨¡æ‹Ÿå®¢æˆ·ç«¯æäº¤çš„è¯·æ±‚å†…å®¹ kubectl run nginx-$RANDOM --image=\u0026#34;nginx:alpine\u0026#34; --dry-run=client -ojson -v6 æ­£å¸¸çš„è¿”å›å“åº”åº”è¯¥å¦‚ä¸‹,è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬æ­£å¸¸é€šè¿‡kubeletåˆ›å»ºä¸€ä¸ªPodæ‰€å‘é€çš„è¯·æ±‚ä½“å†…å®¹,ä½†ä½œä¸ºclinetåªä¼šåœ¨æˆ‘ä»¬æœ¬åœ°è¿›è¡Œå¤„ç†,æ‰€ä»¥ä½ ä¹Ÿå¯ä»¥çœ‹åˆ°è¿”å›çš„ç»“æ„å†…å®¹ä¸­å¸¦æœ‰I1108 21:54:49.922270 1219589 loader.go:374] Config loaded from file: /root/.kube/config,ä¹Ÿå°±è¯æ˜äº†å®ƒå¹¶æ²¡æœ‰åƒApi-Serverå‘é€ä»»ä½•è¯·æ±‚,åªæ˜¯è¯»å–äº†ç›¸å…³çš„é…ç½®ä¿¡æ¯ã€‚\n{ \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx-27147\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null, \u0026#34;labels\u0026#34;: { \u0026#34;run\u0026#34;: \u0026#34;nginx-27147\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;nginx-27147\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;nginx:alpine\u0026#34;, \u0026#34;resources\u0026#34;: {} } ], \u0026#34;restartPolicy\u0026#34;: \u0026#34;Always\u0026#34;, \u0026#34;dnsPolicy\u0026#34;: \u0026#34;ClusterFirst\u0026#34; }, \u0026#34;status\u0026#34;: {} } å¾ˆå¥½ä¸Šé¢æˆ‘ä»¬æ¨¡æ‹Ÿäº†ä¸€ä¸ªclientç«¯æ‰€ç”Ÿäº§çš„å†…å®¹,é‚£ä¹ˆä¸‹é¢æˆ‘ä»¬çœ‹çœ‹å½“å®é™…å‘é€ç»™Api-Serverçš„æ—¶å€™äº§ç”Ÿäº†å“ªäº›å†…å®¹\nkubectl run nginx-$RANDOM --image=\u0026#34;nginx:alpine\u0026#34; --dry-run=server -ojson -v6 æˆ‘ä»¬å¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°æ—¥å¿—çš„è¾“å‡º\né¦–å…ˆç¬¬ä¸€æ­¥åŠ è½½äº†kubernetesç›¸å…³çš„é…ç½®æ–‡ä»¶ä¿¡æ¯ã€‚ å‘é€äº†ç›¸å…³è¯·æ±‚(ç›²çŒœåº”è¯¥æ˜¯éªŒè¯api-serveræ˜¯å¦æ­£å¸¸) å‘https://resk8s.api.beijing.io:8443/api/v1/namespaces/default/pods?dryRun=All\u0026amp;fieldManager=kubectl-runå‘é€äº†POSTè¯·æ±‚ç”¨äºåˆ›å»ºPod I1108 21:58:14.866131 1221713 loader.go:374] Config loaded from file: /root/.kube/config I1108 21:58:14.884329 1221713 round_trippers.go:553] GET https://resk8s.api.beijing.io:8443/openapi/v2?timeout=32s 200 OK in 15 milliseconds I1108 21:58:14.946553 1221713 round_trippers.go:553] POST https://resk8s.api.beijing.io:8443/api/v1/namespaces/default/pods?dryRun=All\u0026amp;fieldManager=kubectl-run 201 Created in 15 milliseconds äº‹å®ä¸Šæˆ‘ä»¬ç”±æ­¤å¯è§å‘é€åˆ°Api-Serverçš„è¯·æ±‚å†…å®¹å¤šäº†å¾ˆå¤šä¸œè¥¿\n{ \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx-14483\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;d27e1836-8bac-40d4-805b-420f4bca4ee1\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2023-11-08T14:05:36Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;run\u0026#34;: \u0026#34;nginx-14483\u0026#34; }, \u0026#34;annotations\u0026#34;: { \u0026#34;kubernetes.customized/fabric-networks\u0026#34;: \u0026#34;default\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;volumes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;kube-api-access-vwtvp\u0026#34;, \u0026#34;projected\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;serviceAccountToken\u0026#34;: { \u0026#34;expirationSeconds\u0026#34;: 3607, \u0026#34;path\u0026#34;: \u0026#34;token\u0026#34; } }, { \u0026#34;configMap\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kube-root-ca.crt\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;ca.crt\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;ca.crt\u0026#34; } ] } }, { \u0026#34;downwardAPI\u0026#34;: { \u0026#34;items\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;namespace\u0026#34;, \u0026#34;fieldRef\u0026#34;: { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldPath\u0026#34;: \u0026#34;metadata.namespace\u0026#34; } } ] } } ], \u0026#34;defaultMode\u0026#34;: 420 } } ], \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;nginx-14483\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;nginx:alpine\u0026#34;, \u0026#34;resources\u0026#34;: {}, \u0026#34;volumeMounts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;kube-api-access-vwtvp\u0026#34;, \u0026#34;readOnly\u0026#34;: true, \u0026#34;mountPath\u0026#34;: \u0026#34;/var/run/secrets/kubernetes.io/serviceaccount\u0026#34; } ], \u0026#34;terminationMessagePath\u0026#34;: \u0026#34;/dev/termination-log\u0026#34;, \u0026#34;terminationMessagePolicy\u0026#34;: \u0026#34;File\u0026#34;, \u0026#34;imagePullPolicy\u0026#34;: \u0026#34;IfNotPresent\u0026#34; } ], \u0026#34;restartPolicy\u0026#34;: \u0026#34;Always\u0026#34;, \u0026#34;terminationGracePeriodSeconds\u0026#34;: 30, \u0026#34;dnsPolicy\u0026#34;: \u0026#34;ClusterFirst\u0026#34;, \u0026#34;serviceAccountName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;serviceAccount\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;securityContext\u0026#34;: {}, \u0026#34;schedulerName\u0026#34;: \u0026#34;default-scheduler\u0026#34;, \u0026#34;tolerations\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;node.kubernetes.io/not-ready\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;Exists\u0026#34;, \u0026#34;effect\u0026#34;: \u0026#34;NoExecute\u0026#34;, \u0026#34;tolerationSeconds\u0026#34;: 300 }, { \u0026#34;key\u0026#34;: \u0026#34;node.kubernetes.io/unreachable\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;Exists\u0026#34;, \u0026#34;effect\u0026#34;: \u0026#34;NoExecute\u0026#34;, \u0026#34;tolerationSeconds\u0026#34;: 300 } ], \u0026#34;priority\u0026#34;: 0, \u0026#34;dnsConfig\u0026#34;: { \u0026#34;options\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;single-request-reopen\u0026#34; } ] }, \u0026#34;enableServiceLinks\u0026#34;: true, \u0026#34;preemptionPolicy\u0026#34;: \u0026#34;PreemptLowerPriority\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;phase\u0026#34;: \u0026#34;Pending\u0026#34;, // å¯ä»¥çœ‹åˆ°å½“å‰å¤„äºPendingé˜¶æ®µ \u0026#34;qosClass\u0026#34;: \u0026#34;BestEffort\u0026#34; } } å¦å¤–: å½“ä½ éœ€è¦åˆ›å»ºèµ„æºç±»å‹çš„æ—¶å€™,æˆ‘å¹¶ä¸å»ºè®®ä½ ä»å¤´å¼€å§‹å»ç¼–å†™ç›¸å…³æ–‡ä»¶,å¯ä»¥çµæ´»çš„åº”ç”¨kubectl runæ¥è¿›è¡Œå¡«å……ç›¸å…³çš„å­—æ®µä¿¡æ¯ã€‚\nkubernetesè®¾è®¡ç†å¿µ å£°æ˜å¼ï¼šå…¸å‹å°±æ˜¯åœ¨èµ„æºæ–‡ä»¶ä¸­è¿›è¡Œå£°æ˜ æ— ä¾µå…¥æ€§ å¯ç§»æ¤: æ‰€æœ‰ç¬¦åˆkubernetesæ ‡å‡†çš„kuberneteså¹³å°éƒ½å¯ä»¥è¿›è¡Œè¿ç§» æ˜¾ç¤ºæ¥å£ï¼šæ‰€æœ‰çš„æ“ä½œéƒ½æ˜¯å¼€æ”¾æ€§çš„,ä¸ä¼šå­˜åœ¨ç§æœ‰æ¥å£,æ— è®ºæ˜¯Api-Serveræˆ–è€…Clinet-goæ‰€æ“ä½œçš„æ¥å£éƒ½æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚ åˆ›å»ºèµ„æºçš„å·¥ä½œæµç¨‹ é¦–å…ˆå½“ç”¨æˆ·çš„è¯·æ±‚è¿›å…¥åˆ°Api-Serveråä¼šè¿›å…¥åˆ°Authorizationè®¤è¯æˆæƒçš„å¤„ç†æ¥å£å®é™…ä¸Šå°±æ˜¯åŠ è½½æˆ‘ä»¬çš„configé…ç½®æ–‡ä»¶,å…¶å…·ä½“ä»£ç åœ¨loader.go:372è¿›è¡Œå®ç° æœåŠ¡å‘ç°åŸç†å’Œåº”ç”¨ kubernetesä¸­Podçš„é€šä¿¡ æ¯ä¸ªPodéƒ½æœ‰è‡ªå·±çš„IPåˆ†é… Podé—´çš„å¯ä»¥é€šè¿‡IPè¿›è¡Œé€šä¿¡ Podçš„IPæ˜¯å¯å˜çš„ Podçš„IPé€šå¸¸ä¸èƒ½è¢«æå‰è·å–,ä¸€èˆ¬éƒ½æ˜¯ç½‘ç»œæ’ä»¶è¿›è¡Œåˆ†é… kubernetesçš„Serviceé€šä¿¡ å®ƒæ˜¯ä¸€ç§æŠ½è±¡ï¼Œå¸®åŠ©ä½ å°† Pod é›†åˆåœ¨ç½‘ç»œä¸Šå…¬å¼€å‡ºå»ã€‚ æ¯ä¸ª Service å¯¹è±¡å®šä¹‰ç«¯ç‚¹çš„ä¸€ä¸ªé€»è¾‘é›†åˆï¼ˆé€šå¸¸è¿™äº›ç«¯ç‚¹å°±æ˜¯ Podï¼‰ä»¥åŠå¦‚ä½•è®¿é—®åˆ°è¿™äº› Pod çš„ç­–ç•¥ã€‚ æˆ‘ä»¬é€šå¸¸å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥ç®€å•çš„æš´éœ²ä¸€ä¸ªService\nkubectl expose deploy/nginx --port=80 é€šå¸¸æ¥è¯´æˆ‘ä»¬è®¿é—®æŸä¸ªæœåŠ¡éƒ½æ˜¯è®¿é—®æœåŠ¡çš„IPåœ°å€ï¼Œå½“ç„¶äº†,åœ¨kubernetesä¸­è®¿é—®Serviceå¯¹åº”çš„åœ°å€ä¹Ÿå¯ä»¥è®¿é—®åˆ°æœåŠ¡\n[root@Online-Beijing-master1 ~]# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wecho-canary ClusterIP 10.10.166.79 \u0026lt;none\u0026gt; 80/TCP,9113/TCP 69d kubernetes ClusterIP 10.10.0.1 \u0026lt;none\u0026gt; 443/TCP 279d å®é™…ä¸Šä¹Ÿå°±æ˜¯è¯´å½“æˆ‘éœ€è¦è®¿é—®wecho-canaryè¿™ä¸ªæœåŠ¡çš„æ—¶å€™,æˆ‘æ— éœ€å…³å¿ƒä»–åç«¯endå¦‚ä½•è¿›è¡Œå˜åŒ–,æˆ‘åªéœ€è¦è®°ä½è®¿é—®wecho-canaryçš„Serviceæ‰€ç»™å‡ºçš„ClusterIPå³å¯è¿›è¡Œè®¿é—®åˆ°ç›¸åº”çš„æœåŠ¡ã€‚\nå¦å¤–çš„ä¸€ç§æ–¹å¼å°±æ˜¯é€šè¿‡æˆ‘ä»¬çš„Serviceåç§°æ¥è¿›è¡Œè®¿é—®ï¼Œä¸‹é¢è¯´ä¸€ä¸‹å…·ä½“æ˜¯å¦‚ä½•é€šè¿‡Serviceåç§°è¿›è¡Œè®¿é—®çš„ã€‚\næˆ‘ä»¬å‡è®¾ä½¿ç”¨ä¸Šé¢çš„wecho-canaryåç§°è¿›è¡Œä¸€æ¬¡è®¿é—®\ncurl -v wecho-canary # å‡è®¾è®¿é—®æ­£å¸¸ å¾ˆç®€å•,å¤§å®¶éƒ½çŸ¥é“å½“æˆ‘ä»¬è®¿é—®ä¸€ä¸ªåŸŸåçš„æ—¶å€™èƒŒåè‚¯å®šåˆDNSæœåŠ¡å™¨æ¥è§£æå…¶æ‰€å¯¹åº”çš„IPåœ°å€,é‚£ä¹ˆå…¶å®åœ¨kuberneteså½“ä¸­ä¹Ÿæœ‰ä¸€ä¸ªå†…éƒ¨çš„DNSæœåŠ¡å™¨,åå­—å«åškube-dns æˆ‘ä»¬å¯ä»¥é€šè¿‡kubectl get svc -A è¿›è¡ŒæŸ¥çœ‹,å…¶ä¸­æ‰€æŒ‡å®šçš„10.10.0.10å°±æ˜¯æˆ‘ä»¬kube-dnsçš„IPåœ°å€.\nkube-system kube-dns ClusterIP 10.10.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 279d çœ‹åˆ°æˆ‘ä»¬kube-dnsæœ‰ä»¥ä¸‹ç«¯ç‚¹\n[root@Online-Beijing-master1 ~]# kubectl get ep -n kube-system | grep kube-dns kube-dns 10.10.151.159:53,10.10.4.95:53,10.10.151.159:53 + 3 more... 279d kube-dns-upstream 10.10.151.159:53,10.10.4.95:53,10.10.151.159:53 + 1 more... 81d å…¶æ¬¡å¯ä»¥çœ‹åˆ°æˆ‘ä»¬å®¹å™¨å†…æ‰€å¯¹åº”çš„resolv.confæ‰€å†™å†…å®¹çš„é…ç½®æ–‡ä»¶\nsearch jiashicang.svc.cluster.local svc.cluster.local cluster.local nameserver 169.254.20.10 options ndots:5 single-request-reopen ç”±äºæˆ‘ä½¿ç”¨äº†NodeLocalDnsæ‰€ä»¥æˆ‘è¿™ä¸ªåœ°æ–¹çš„nameserverä¸å¤ªä¸€æ ·,å…³äºNodeLocalDnsæˆ‘ä»¬åç»­å†è¯´ã€‚ æ€»ç»“ä»¥ä¸‹å‡ ç‚¹:\nå½“æˆ‘ä»¬è®¿é—®wecho-canaryçš„æ—¶å€™å®é™…ä¸Šæ˜¯è®¿é—®äº†wecho-canary.default.cluster.local.svc,å³ServiceName.Namespace.cluster.local.svc æ‰€æœ‰çš„è§£æåŸŸåè¯·æ±‚éƒ½ä¼šè¯·æ±‚åˆ°kube-dnså½“ä¸­,å½“kube-dnsæ— æ³•å®Œæˆè§£æçš„æ—¶å€™,æˆ‘ä»¬ä¼šå°†è¯·æ±‚forwardåˆ°æœ¬åœ°çš„è§£ææ–‡ä»¶å½“ä¸­,å¦‚æœæœ¬åœ°è§£ææ–‡ä»¶ä¹Ÿæ— æ³•è§£æåˆ™è®¤ä¸ºå¤±è´¥ã€‚å…¸å‹çš„å°±æ˜¯è¶…æ—¶ã€æ— æ³•è§£æç­‰é—®é¢˜ã€‚ Serviceé€šè¿‡ç›‘è§†API serverçš„æ•°æ®å˜åŒ–æ¥æ„ŸçŸ¥åç«¯Podçš„å˜åŒ–,å¹¶åŠæ—¶æ›´æ–°è´Ÿè½½å‡è¡¡è§„åˆ™ã€‚å…·ä½“æ¥è¯´,Kubernetes ApiServerè´Ÿè´£ç®¡ç†é›†ç¾¤çŠ¶æ€,å®ƒä¼šè®°å½•æ¯ä¸€ä¸ªå¯¹è±¡(åŒ…æ‹¬Pod)çš„Specå’ŒStatusã€‚Serviceå¯¹è±¡ä½¿ç”¨ApiServerçš„watchæ¥å£,ç›‘è§†åç«¯å…³è”Podå¯¹è±¡çš„å˜åŒ–äº‹ä»¶ã€‚æ¯”å¦‚Podå®ä¾‹åŠ å…¥æˆ–é”€æ¯æ—¶,ApiServerä¼šä¸»åŠ¨é€šçŸ¥Serviceã€‚ä¸€æ—¦æ£€æµ‹åˆ°Podå˜åŒ–,Serviceä¼šç«‹å³ä½¿ç”¨æ–°çš„Podåˆ—è¡¨,é‡æ–°è®¡ç®—å¹¶æ›´æ–°è‡ªå·±è´Ÿè½½å‡è¡¡çš„ç«¯å£è½¬å‘è§„åˆ™ã€‚ä¾‹å¦‚ä½¿ç”¨iptablesè§„åˆ™æˆ–IPVSè¡¨æ›´æ–°åç«¯ç›®æ ‡åœ°å€ã€‚è¿™æ ·,æ— è®ºPodæ˜¯åŠ¨æ€ä¼¸ç¼©è¿˜æ˜¯æ•…éšœè½¬ç§»,Serviceéƒ½èƒ½å³æ—¶æ„ŸçŸ¥,ä¿æŒè´Ÿè½½å‡è¡¡å…¥å£åœ°å€çš„é«˜å¯é æ€§ã€‚è¿™å°±æ˜¯Serviceå¦‚ä½•å®ç°åŠ¨æ€æ›´æ–°è´Ÿè½½å‡è¡¡è§„åˆ™çš„åŸç†ã€‚ Serviceç±»å‹ ClusterIP: é€šè¿‡é›†ç¾¤çš„å†…éƒ¨IPå…¬å¼€Serviceï¼Œé€‰æ‹©è¯¥å€¼æ—¶Serviceåªèƒ½å¤Ÿåœ¨é›†ç¾¤å†…éƒ¨è®¿é—®ã€‚è¿™ä¹Ÿæ˜¯ä½ æ²¡æœ‰ä¸ºæœåŠ¡æ˜¾å¼æŒ‡å®štypeæ—¶ä½¿ç”¨çš„é»˜è®¤å€¼ã€‚ NodePort: é€šè¿‡æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„IPå’Œé™æ€ç«¯å£NodePortå…¬å¼€ Serviceã€‚ä¸ºäº†è®©Serviceå¯é€šè¿‡èŠ‚ç‚¹ç«¯å£è®¿é—®ï¼ŒKubernetes ä¼šä¸º Serviceé…ç½®é›†ç¾¤IPåœ°å€ï¼Œ ç›¸å½“äºä½ è¯·æ±‚äº†type:ClusterIPçš„æœåŠ¡ã€‚ LoadBalancer:ä½¿ç”¨äº‘å¹³å°çš„è´Ÿè½½å‡è¡¡å™¨å‘å¤–éƒ¨å…¬å¼€Service,ä¸€èˆ¬æ¥è¯´éƒ½ç”¨åœ¨äº‘å‚å•†æ‰ä¼šä½¿ç”¨LoadBalancer ExternalName:å°†æœåŠ¡æ˜ å°„åˆ°externalNameå­—æ®µçš„å†…å®¹ ä¾‹å¦‚ï¼Œæ˜ å°„åˆ°ä¸»æœºå:api.foo.bar.example,è¯¥æ˜ å°„å°†é›†ç¾¤çš„ DNS æœåŠ¡å™¨é…ç½®ä¸ºè¿”å›å…·æœ‰è¯¥å¤–éƒ¨ä¸»æœºåå€¼çš„CNAMEè®°å½•ã€‚ å¸¸ç”¨çš„æš´éœ²Serviceçš„æ–¹æ³•\nkubectl expose deploy/nginx --port=80 --type=NodePort å“¦,å¯¹äº†,æ¨èä¸€ä¸ªå¼€æºçš„LBæ’ä»¶\nMetallb: Github æœåŠ¡å‘ç°å’Œæµé‡è·¯ç”± æ€»çš„æ¥è¯´æˆ‘ä»¬åˆ†ä¸ºä¸€ä¸‹å‡ ç‚¹å§\nPodä¹‹é—´çš„æµé‡é€šä¿¡: IPç›´è¿çš„æ–¹å¼: ä¸æ¨èè¿™ç§æ–¹å¼,å› ä¸ºPodçš„IPä¸¥æ ¼æ„ä¹‰ä¸Šæ¥è¯´å¯¹äºæœåŠ¡ç‰ˆæœ¬æ›´æ–°ç”Ÿå‘½å‘¨æœŸç›¸å¯¹è¾ƒçŸ­ã€‚ ClusterIPï¼š ç›¸å¯¹äºæ¯”è¾ƒæ¨èé€šè¿‡æŒ‡å®šServiceNameçš„æ–¹å¼æ¥ç»‘å®šæ´»ç€ç›‘å¬æŸç§æœåŠ¡ å¤–éƒ¨è®¿é—®Podçš„é€šä¿¡ NodePort: é€šè¿‡æš´éœ²å¤–éƒ¨ç«¯å£çš„æ–¹å¼æ¥è¿›è¡ŒPodçš„è®¿é—® LoadBalancer: é€šè¿‡å¯¹Serviceç»‘å®šLBçš„æ–¹å¼è¿›è¡ŒPodçš„è®¿é—® Podè®¿é—®å¤–éƒ¨é€šä¿¡: ExternalName: å°† Service æ˜ å°„åˆ° DNS åç§° HeadlessService: å½“ä½ ä¸éœ€è¦è´Ÿè½½å‡è¡¡ï¼Œä¹Ÿä¸éœ€è¦å•ç‹¬çš„ ServiceIP,ä½ å¯ä»¥ä½¿ç”¨HeadlessService # ä¸‹é¢æ˜¯ExternalNameçš„ç®€å•å®ä¾‹ apiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com ä¸€èˆ¬æ¥è¯´Kubernetesçš„å†…éƒ¨DNSè®°å½•æœ‰ä¸¤ç§è§„èŒƒ\nPodIP.Namespace.pod.cluster-domain.example --- ServiceName.Namespace.svc.cluster-domain.example ","date":"2023-11-07T00:00:00Z","image":"https://img13.360buyimg.com/ddimg/jfs/t1/164078/6/34165/22231/654ba81fFe1bf21d7/9d02bd791b795b3b.jpg","permalink":"http://localhost:1313/kubernetes/architectural_design/","title":"Kubernetesçš„æ¶æ„è®¾è®¡å’Œå¯¹è±¡å±æ€§åŸºæœ¬ç†è§£"},{"content":"ç®€å•åŠŸèƒ½ æ˜¯ç”·äººï¼Œå°±æ¥åˆ†äº«ä½ æ‹çš„ç…§ç‰‡ï¼ï¼ï¼ RESTFul API ä¸€ä¼šå„¿å†è¯´ã€‚\nåŸºæœ¬çš„åç«¯æŠ€æœ¯æ ˆ Gorm: æ•°æ®åº“å·¥å…· Gin: é€Ÿåº¦æå¿«çš„Goè¯­è¨€Webæ¡†æ¶ Minio: åˆ†å¸ƒå¼å­˜å‚¨ é¡¹ç›®ç›®å½• ä¸€ä¼šå„¿å†è¯´\né¡¹ç›®ç¼“å­˜è§„èŒƒ RedisKeyçš„è§„èŒƒ project:module:business:uk é¡¹ç›®å æ¨¡å—å ä¸šåŠ¡å å”¯ä¸€æ ‡è¯† ç¼“å­˜ä¿¡æ¯ è¿™éƒ¨åˆ†è¿˜æ²¡è®¾è®¡å®Œæˆ,ç­‰å¾…å®Œå–„å§ã€‚ Key ç±»å‹ è¿‡æœŸæ—¶é—´ è¯´æ˜ wecho:user:access_token:{username} string 2å¤© å­˜å‚¨ç”¨æˆ·ç”Ÿæˆçš„JWT wecho:userinfo:cache:{username} SET 3å¤© ç”¨æˆ·ä¿¡æ¯è¯¦æƒ…ç¼“å­˜ wecho:user:login_fail:{username} Incr 30Min é”™è¯¯ç™»å½•æ¬¡æ•° å¸¸ç”¨ä»£ç ç‰‡æ®µ å®ç°ç»“æ„ä½“ // UserDataService ç”¨æˆ·ç®¡ç†æœåŠ¡ var UserDataService = newUserDataService() func newUserDataService() *userDataService { return \u0026amp;userDataService{} } type userDataService struct { } Minioå¯åŠ¨å‘½ä»¤ docker run -d \\ -p 9000:9000 \\ -p 9001:9001 \\ --name minio1 \\ -v ./data:/data \\ -v ./certs:/tmp/certs \\ -e \u0026#34;MINIO_ROOT_USER=xxx\u0026#34; \\ -e \u0026#34;MINIO_ROOT_PASSWORD=xxx\u0026#34; \\ -e \u0026#34;MINIO_SERVER_URL=xxx\u0026#34; \\ quay.io/minio/minio server /data --console-address \u0026#34;:9001\u0026#34; --certs-dir /tmp/certs åç«¯æœåŠ¡ç¼–è¯‘ # Windowsç¼–è¯‘Linux # è¯·åœ¨CMDä¸­æ‰§è¡Œå‘½ä»¤ set GOARCH=amd64 set GOOS=linux go build main.go # Macç¼–è¯‘Linux CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build main.go æ¥å£ç»Ÿè®¡ ç”¨æˆ·æœåŠ¡(æ™®é€šç”¨æˆ·) è·å–ç”¨æˆ·è¯¦æƒ…ä¿¡æ¯ ä¿®æ”¹ä¸ªäººèµ„æ–™ä¿¡æ¯ ä¿®æ”¹ä¸ªäººå¯†ç  ä¿®æ”¹å¤´åƒä¿¡æ¯ ç»‘å®šæ‰‹æœºä¿¡æ¯ ç»‘å®šèº«ä»½ä¿¡æ¯ å…³æ³¨ç”¨æˆ· ç”»æ¿æœåŠ¡ è·å–ç”»æ¿è¯¦æƒ…ä¿¡æ¯ åˆ›å»ºç”»æ¿ æ›´æ–°ç”»æ¿ åˆ é™¤ç”»æ¿ è·å–ç”»æ¿åˆ—è¡¨ä¿¡æ¯ æ”¶è—ç”»æ¿ æ ¹æ®ç”»æ¿æ ‡ç­¾åˆ†ç±»ç”»æ¿ä¿¡æ¯ å®¡æ ¸æœåŠ¡ å®¡æ ¸ç”»æ¿ å®¡æ ¸ç”»æ¿å›¾ç‰‡ é€šä¿¡æœåŠ¡ å‘é€ç§ä¿¡ ","date":"2023-08-26T00:00:00Z","image":"https://ydschool-online.nosdn.127.net/tiku/f291c1c8d5dccacd2b95ef4fdc68d07cefd0e92562794e5bcdd21a9729b3455d.jpg","permalink":"http://localhost:1313/go/gin-wecho/","title":"ç»´æ‰£å›¾å…ƒ-ä¸“å±ä½ çš„æ‹ç…§æ‘„å½±ç¤¾åŒº"},{"content":"æœ¬ä¸»é¢˜å†…å®¹é¢˜æå–è‡ªbilibili-äººç”Ÿç¬¬ä¸€æ¬¡(å‘Šåˆ«)\næ¼«é•¿çš„å‘Šåˆ« å¿˜è®°æˆ‘ï¼Œä¹Ÿæ²¡äº‹â€¦ 2019å¹´12æœˆ9æ—¥\nå·¢æ–‡è‡»ï¼Œ73å²ï¼Œå››å¹´å‰ï¼Œè€ä¼´è‚çˆ±è£è¢«ç¡®è¯Šä¸ºé˜¿å°”å…¹æµ·é»˜ç—‡ã€‚\néšç€è€ä¼´çš„ç—…è¶Šæ¥è¶Šä¸¥é‡â€¦ä»ä¸€å¼€å§‹çš„å¿˜è®°äº‹æƒ…ï¼Œåˆ°åæ¥çš„å¿˜è®°äººâ€¦\nå·¢æ–‡è‡»åœ¨å¢™ä¸Šçš„è¿™å—æ¿å­ä»æ¥æ²¡æœ‰æ‘˜ä¸‹ã€‚ è€Œä»–ä¹Ÿæœ‰åœ¨ç™½æ¿ä¸Šï¼Œç»™è‡ªå·±å†™äº†ä¸€å¥å¤§å¤§çš„æç¤ºè¯­ï¼š â€œåˆ«å‘ç«ï¼â€\nå·¢æ–‡è‡»çœ‹ç€ç†Ÿç¡çš„å¦»å­ï¼Œç»ˆäºæ¾äº†å£æ°”ã€‚\nä»–èµ°è¿›å¨æˆ¿ï¼Œå¼€å§‹å‡†å¤‡åˆé¤ã€‚\næ‹¿èµ·èœåˆ€çš„ç¬é—´ï¼Œä»–è„‘æµ·ä¸­ä¾¿æµ®ç°å¦»å­æ›¾ç»åœ¨å¨æˆ¿å¿™ç¢Œçš„èº«å½±ã€‚\nçœ¼æ³ªï¼Œæ­¢ä¸ä½åœ°ä»ä¸¤é¢Šæ»‘è½ä¸‹æ¥ã€‚\nä»å®¶åˆ°å…»æŠ¤é™¢ï¼Œ11ç«™åœ°é“ï¼Œ4ç«™å…¬äº¤ï¼Œæ­¥è¡Œ800ç±³ã€‚è¿™æ ·çš„è·¯ç¨‹è€å·¢æ¯å‘¨è¦èµ°ä¸Šå››äº”æ¬¡ã€‚\nè·¯ä¸Šçš„è€å·¢ï¼Œåƒæ˜¯å†èµ´ä¸€åœºç”œèœœçš„çº¦ä¼šã€‚\nâ€œè‚çˆ±è£â€ æˆ‘å«ä»€ä¹ˆåå­—?\nä»è‚çˆ±è£å˜´é‡Œå›ç­”å‡ºçš„è¿™å¥ï¼šâ€œå·¢æ–‡è‡»â€\nå¯¹äºä»–æ¥è¯´ï¼Œå¦»å­å›ç­”çš„â€œå·¢æ–‡è‡»â€ä¸‰ä¸ªå­—ï¼Œä¾¿æ˜¯ä¸–ä¸Šæœ€ç¾çš„ä¸‰å­—æƒ…ä¹¦ã€‚\nâ€œè®¤è¯†æˆ‘ï¼è®¤è¯†æˆ‘çœŸçš„éå¸¸å¼€å¿ƒâ€\næ¯å½“å¦»å­è¯´å¯¹äº†åå­—ï¼Œå·¢æ–‡è‡»éƒ½ä¼šå°†å¥¹æŠ±ä¸Šè®¸ä¹…ï¼Œèˆä¸å¾—æ¾æ‰‹ã€‚\nå¦»å­ç”Ÿç—…åï¼Œæ›¾ç»å†…æ•›çš„å·¢æ–‡è‡»ï¼Œå˜å¾—è¶Šæ¥è¶Šä¸åå•¬äºè¡¨è¾¾å¿ƒä¸­çš„çˆ±æ„ã€‚\nåªè¦è§åˆ°å¥¹ï¼Œä¾¿å†ä¹Ÿä¸æ”¾å¼€ç´§æ¡çš„æ‰‹ã€‚\nè€ä¼´æœ‰éª¨è´¨ç–æ¾ï¼Œå·¢æ–‡è‡»æ¯å¤©éƒ½ä¼šå¸¦ç€è‚çˆ±è£æ…¢æ…¢çš„åœ¨é™¢é‡Œæ•£æ­¥ã€‚\næˆ˜å‹èšä¼šã€æœ‹å‹èšä¼šã€å·¢æ–‡è‡»ä¸€æ¦‚ä¸å»ã€‚\nä»–è¯´ï¼šâ€œæˆ‘å»äº†ä¹Ÿä¸å®šå¿ƒã€‚â€\nå¯¼æ¼”ï¼šâ€œä½ è¿™æ ·ä¸å°±æ²¡ä¸ªäººçš„ç”Ÿæ´»äº†å—ï¼Ÿâ€\nä»–è¯´ï¼šâ€œæˆ‘è¿™æ ·ä¹Ÿæ˜¯ä¸€ç§å¼€å¿ƒï¼Œæˆ‘æ²¡æœ‰è§‰å¾—çˆ±è£æ˜¯æˆ‘çš„è´Ÿæ‹…ã€‚â€\næˆ‘åº”å¦‚ä½•çˆ±ä½  ç…§é¡¾å®Œè‚çˆ±è£åï¼Œè€å·¢ä¾¿å›åˆ°äº†å®¶ã€‚\nå›åˆ°å®¶åçš„è€å·¢ï¼Œæ²‰é»˜äº†è®¸å¤šã€‚\nè€ä¼´ç¦»å¼€äº†å®¶ï¼Œè€å·¢å°±å˜æˆäº†\u0026quot;ç©ºå·¢\u0026quot;ã€‚\nåƒèˆ¬éš¾èˆåƒèˆ¬èˆï¼Œä¸‡äº‹ä¸ç”©ä¸‡äº‹ç”©ã€‚\n2019å¹´12æœˆ27æ—¥\nè€å·¢ç—…å€’äº†â€¦åŒ»ç”Ÿå‘Šè¯‰ä»–ä»–çš„å‰åˆ—è…ºä¸Šæœ‰è‚¿ç˜¤\nè€ä¼´çš„èº«ä½“æ¯ä¸€å¤©éƒ½æ¯”å‰ä¸€å¤©æ›´å·®ï¼Œè€å·¢è¯´ï¼šâ€œæœ‰ä¸€äº›äº‹ä»–è¦æƒ³åˆ°å‰é¢ã€‚â€\nè€å·¢å»äº†ä¸­åé—å˜±åº“ï¼Œä»–è¦ç«‹ä¸‹ä¸€äº›é—å˜±ï¼Œä¸ºä»¥ååšå®‰æ’ã€‚\nç«‹é—å˜±ï¼Œè¦è‡ªæ„¿ï¼Œå¤´è„‘æ¸…æ™°ï¼Œæœ‰è¡¨è¾¾èƒ½åŠ›ï¼Œä¸”ä¸èƒ½æœ‰é”™åˆ«å­—ï¼Œæ‰€ä»¥å·¢æ–‡è‡»åªèƒ½ä¸€ä¸ªäººå‰æ¥ï¼Œæ—¢ä»£è¡¨è‡ªå·±ï¼Œä¹Ÿä»£è¡¨å¦»å­ã€‚\næˆªæ­¢2019å¹´åº•ï¼Œä¸­åé—å˜±åº“å·²ç»ä¿ç®¡äº†16.5ä¸‡ä»½é—å˜±ã€‚\nè¿™äº›ç«‹é—å˜±çš„äººï¼Œå°±åƒæ˜¯å°å­¦ç”Ÿä¸€æ ·ï¼Œè¶´åœ¨è¯¾æ¡Œä¸Šå†™ç€è‡ªå·±äººç”Ÿçš„ä½œä¸šã€‚\nä¸èƒ½æœ‰ä¸€ä¸ªé”™åˆ«å­—ã€‚\nè¿™æ˜¯å·¢æ–‡è‡»å…¥ä¼äº”åå¹´ä»¥æ¥ç•™ä¸‹çš„æ„Ÿè¨€\nåŠä¸ªä¸–çºªï¼Œæ„Ÿæ…¨ä¸‡åƒã€‚\nå¦‚æœ‰æ¥ç”Ÿï¼Œå†ç»­å‰ç¼˜ï¼Ÿ\nä»–ä¸çŸ¥é“æœ‰æ²¡æœ‰æ¥ç”Ÿã€‚\nåœ¨ç«‹é—å˜±æ—¶ï¼Œåˆ«çš„è€äººéƒ½æ˜¯å†™ç»™å­å¥³ä¸€ç•ªè¯ï¼Œè€Œå·¢æ–‡è‡»æèµ·ç¬”åï¼Œåˆ™æ˜¯æ€è€ƒå†ä¸‰ï¼Œå†™ä¸‹äº†ä¸€é¦–å‘Šåˆ«è¯—ï¼š\nå¤©å ‚ä¹‹é—¨å‘æˆ‘å¼€ï¼Œä¸å°½æ€ç»ªæ»šæ»šæ¥ï¼›\nåƒèˆ¬éš¾èˆåƒèˆ¬èˆï¼Œä¸‡äº‹ä¸ç”©ä¸‡äº‹ç”©ã€‚\nå¹¸å–œå¯’é—¨å¿—ä¸è¡°ï¼Œé¢‘é‡è‰°å›°ä»°ä¼—çˆ±ï¼›\næ„¿æŠŠçš®å›ŠçŒ®ææ—ï¼Œé­‚å½’çˆ¶æ¯åº”èŠ‚å“€ã€‚\nè¯·è®°ä½æˆ‘ 2020å¹´ï¼Œè‚çˆ±è£å†æ¬¡æ‘”å€’äº†ã€‚\nè€Œä¸”ï¼Œæ˜¯ä¸¤æ¬¡ã€‚\nå·¢æ–‡è‡»æ— æ³•å¦‚å¾€å¸¸ä¸€èˆ¬å»å…»è€é™¢ï¼Œä»–æ•´æ—¥é­‚ä¸å®ˆèˆï¼Œå¼€å§‹èƒ¡æ€ä¹±æƒ³ã€‚\nä»–å®³æ€•ï¼Œå¦»å­æ‘”å€’åæ˜¯å¦èƒ½æ­£å¸¸ç”Ÿæ´»ï¼›ä»–å®³æ€•ï¼Œå¦»å­ä¸€æ—¥ä¸‰é¤é¥®é£Ÿä¸è§„å¾‹ã€‚\nä»–æ›´æ€•ï¼Œä¸‹ä¸€æ¬¡è§é¢ï¼Œå¦»å­å·²ç»ä¸è®°å¾—ä»–äº†ã€‚\n2020å¹´åˆï¼Œæ–°å† ç–«æƒ…å¤§çˆ†å‘ã€‚å·¢æ–‡è‡»å·²ç»ä¸¤ä¸ªæœˆæ²¡æœ‰è§åˆ°å¦»å­è‚çˆ±è£äº†ã€‚\nä¸ºäº†ç¼“è§£æ€å¿µï¼Œå·¢æ–‡è‡»å¼€å§‹ç»™å¦»å­å†™ä¿¡ã€‚\nå› ä¸ºå¦»å­å·²ç»ä¸è¯†å­—ï¼Œä»–åªèƒ½å¯¹ç€æ‰‹æœºå½•åˆ¶è§†é¢‘ï¼Œç„¶åå‘ç»™æŠ¤å£«ç»™å¦»å­çœ‹è¿™ä¸€å°å°é¥±å«æ€å¿µçš„â€œç”µå­ä¿¡â€ã€‚ ä¿¡ä¸Šè¯´ï¼š\nâ€œçˆ±è£å•Šâ€¦ä»å¹´åˆè‡³ä»Šå·²æœ‰ä¸¤ä¸ªæœˆæœªèƒ½ä¸ä½ è§é¢ï¼Œå¿ƒä¸­ååˆ†æƒ³å¿µã€‚\nå›æƒ³æˆ‘ä»¬ç»“å©šä»¥æ¥çš„å››åå¤šå¹´ï¼ŒåŒç”˜è‹¦å…±æ‚£éš¾ï¼Œç»å†äº†å…«æ¬¡æ¬å®¶ã€‚\nå…¶ä¸­æœ‰å…«å¹´ï¼Œä½ éƒ½æ˜¯ï¼ˆè·Ÿæˆ‘ï¼‰ç¡åœ¨åœ°æ¿ä¸Šçš„ï¼Œä½†ä½ ä»æ— æ€¨è¨€ã€‚\nåœ¨å®¶é‡Œä½ æ˜¯æ ‡å‡†çš„è´¤å¦»è‰¯æ¯ï¼Œç›¸å¤«æ•™å­ï¼Œåˆ°è€äº†æ²¡äº«å¤šå°‘ç¦ï¼Œä½ å°±ç”Ÿç—…äº†â€¦â€\nè¯´åˆ°è¿™é‡Œï¼Œå·¢æ–‡è‡»æ‹¿ä¿¡çš„æ‰‹å·²ç»å¼€å§‹é¢¤æŠ–ï¼Œä»–å¼ºå¿ç€æƒ…ç»ªè¯»å®Œäº†ä¿¡ä»¶ã€‚\nå·¢æ–‡è‡»ä¸€ç›´æœ‰ä»¶äº‹æƒ…ç’ç€å®¶äººã€‚\nè‡ªä»å¦»å­æ‚£ç—…ä¹‹åï¼Œä»–ä¹Ÿå¾—äº†æŠ‘éƒç—‡ï¼Œåœ¨è¿™æœŸé—´ï¼Œä»–å››å¤„å¯»è¯ï¼Œæƒ³å°½ä¸€åˆ‡åŠæ³•ä¸è®©è‡ªå·±å€’ä¸‹ã€‚\nå› ä¸ºä»–æ·±çŸ¥ï¼Œåªæœ‰è‡ªå·±ä¸å€’ï¼Œæ‰èƒ½å‡è½»å­©å­çš„è´Ÿæ‹…ï¼Œå¦»å­æ‰èƒ½ç”Ÿæ´»çš„æ›´å¥½ä¸€äº›ã€‚\nè¿™äº›å¹´ï¼Œæœ‰ä¹ä¸ªå­—æ¦‚æ‹¬äº†ä»–å…¨éƒ¨çš„ç”Ÿæ´»ï¼š\nâ€œç¹ç‰©æ€äººï¼Œé­‚ä¸å®ˆèˆï¼Œè‚è‚ å¯¸æ–­ã€‚â€\nä»–è¯´ï¼šâ€œè¿™çœŸçš„æ˜¯ç”Ÿç¦»æ­»åˆ«ï¼Œæˆ‘æ ¹æœ¬ä¸æƒ³æ”¾å¼€å¥¹â€\nâ€œæ²¡åŠæ³•ï¼Œæˆ‘çŸ¥é“åé¢ä¼šå‘ç”Ÿä»€ä¹ˆâ€\næˆ–è®¸ï¼Œç”Ÿçš„å¯¹ç«‹é¢å¹¶ä¸æ˜¯æ­»äº¡ï¼Œè€Œæ˜¯é—å¿˜ã€‚\nçºªå½•ç‰‡çš„æœ€åï¼Œå·¢æ–‡è‡»è¡¨ç¤ºå¸Œæœ›æœªæ¥å¦»å­æ¯”è‡ªå·±å…ˆèµ°ã€‚\nè¿™æ ·ï¼Œä»–å°±èƒ½ç…§é¡¾å¥¹ä¸€è¾ˆå­äº†ã€‚\næˆªæ­¢2022å¹´ï¼Œæˆ‘å›½é˜¿å°”èŒ¨æµ·é»˜ç—…æ‚£è€…çº¦1000ä¸‡äººï¼Œå±…å…¨çƒä¹‹é¦–ï¼\né¢„è®¡åˆ°2050å¹´å°†çªç ´4000ä¸‡äººã€‚\nè¿™äº›å†°å†·çš„æ•°å­—èƒŒåï¼Œæ˜¯ä¸€ä¸ªä¸ªæ— æ¯”ç—›è‹¦çš„å®¶åº­ã€‚\nä½œä¸ºæ™®é€šäººï¼Œæˆ‘ä»¬æ— æ³•é˜»æ­¢æ­»äº¡çš„åˆ°æ¥ï¼Œä½†æ˜¯æ­£å¦‚çš®å…‹æ–¯ç”µå½±ã€Šå¯»æ¢¦ç¯æ¸¸è®°ã€‹æ’æ›²é‡Œå”±åˆ°çš„ä¸€èˆ¬ï¼š\nè¯·è®°å¾—æˆ‘ï¼Œè™½ç„¶å†è§å¿…é¡»è¯´ã€‚\né‚£æˆ–è€…åˆæ˜¯â€¦â€œå½“ä½ è€åˆ°å¿˜äº†ä¸–ç•Œ æˆ‘ç”¨ä»€ä¹ˆæ¥çˆ±ä½ ã€‚â€\n","date":"2023-08-10T00:00:00Z","image":"https://img12.360buyimg.com/ddimg/jfs/t1/192726/25/35567/27205/64d4df78Ff9979209/ecdcae14b5eefcd1.jpg","permalink":"http://localhost:1313/documentary/valedictory/","title":"çºªå½•ç‰‡-äººç”Ÿç¬¬ä¸€æ¬¡-å‘Šåˆ«"},{"content":"äº’è”ç½‘æœåŠ¡å‘å±•è‡³ä»Šï¼Œä½œä¸ºå¼€å‘è€…é˜µè¥çš„æˆ‘ä»¬ï¼Œå·²ç»ç”¨å®è·µè¯æ˜äº†å‰åç«¯åˆ†ç¦»å¼€å‘æ¨¡å¼æ­£åœ¨é€æ¸æˆä¸ºè¶Šæ¥è¶Šå¤šäº’è”ç½‘å…¬å¸æ„å»ºæœåŠ¡å’Œåº”ç”¨çš„æ–¹å¼ã€‚\nå‰åç«¯åˆ†ç¦»ä¼˜åŠ¿å¤šå¤šï¼Œå…¶ä¸­ä¸€ä¸ªå¾ˆé‡è¦çš„ä¼˜åŠ¿æ˜¯ï¼šå¯¹äºåå°æœåŠ¡ï¼ˆç³»ç»Ÿï¼‰æ¥è®²ï¼Œåªéœ€æä¾›ä¸€å¥—ç»Ÿä¸€çš„APIæ¥å£ï¼Œå¯è¢«å¤šä¸ªå®¢æˆ·ç«¯æ‰€å¤ç”¨ï¼Œåˆ†å·¥å’Œåä½œè¢«ç»†åŒ–ï¼Œå¤§å¤§æé«˜äº†æ•ˆç‡ã€‚\nä¸æ­¤åŒæ—¶å¸¦æ¥çš„ä¸€äº›å‰¯ä½œç”¨ä¾¿æ˜¯ï¼š\næ¥å£æ–‡æ¡£ç®¡ç†æ··ä¹±ã€‚ä¹‹å‰å¾ˆå¤šå…¬å¸ç®¡ç†APIæ¥å£ï¼Œæœ‰ç”¨Wikiçš„ï¼Œæœ‰Wordæ–‡æ¡£çš„ï¼Œæœ‰Htmlçš„ï¼Œç»å¸¸é‡åˆ°é—®é¢˜æ˜¯æ¥å£å› å˜äº†ï¼Œæ¯”å¦‚å¢åŠ å‚æ•°ï¼Œå‚æ•°åå˜äº†ï¼Œå‚æ•°è¢«åˆ é™¤äº†ç­‰éƒ½æ²¡æœ‰åŠæ—¶æ›´æ–°æ–‡æ¡£çš„æƒ…å†µ æ¥å£æµ‹è¯•æ²¡æœ‰ä¿éšœã€‚æ¯•ç«Ÿå‰ç«¯å¼€å‘ä¾èµ–åç«¯æ¥å£ï¼Œå¦‚æœå‰åç«¯å¼€å‘ä¸åŒæ­¥ï¼Œæ¥å£åŠæ—¶æµ‹è¯•æˆäº†é—®é¢˜ï¼Œå› æ­¤éœ€è¦éšæ—¶æä¾›ä¸€å¥—å¯ç”¨çš„APIæ¥å£æ•°æ®æµ‹è¯•æœåŠ¡ã€‚ èµ„æºåˆ†æ•£ï¼Œéš¾ä»¥å…±äº«ã€‚æ¯ä¸ªå¼€å‘è€…ç»´æŠ¤è‡ªå·±çš„ä¸€å¥—æµ‹è¯•æ¥å£é›†åˆï¼Œæ— æ³•å…±ç”¨ä»–äººæ¥å£é›†åˆï¼Œå¼€å‘è¿‡ç¨‹ä¸­å……æ–¥ç€å¤§é‡é‡å¤é€ æ•°æ®ã€å¡«æ¥å£çš„å·¥ä½œï¼Œæ•ˆç‡ä¸é«˜ å…¶ä»–é—®é¢˜ã€‚é™¤æ­¤ä¹‹å¤–è¿˜æœ‰å¯èƒ½ç¢°åˆ°è¯¸å¦‚ æ–‡æ¡£å¯¼å‡ºã€æ¥å£åˆ†ç±»è§„åˆ’ã€æ“ä½œä¾¿åˆ©æ€§ç­‰ä¸€ç³»åˆ—é—®é¢˜ã€‚ åŸºäºæ­¤æƒ…å†µï¼Œå› æ­¤æœ¬æ–‡æ¥ä¸‹æ¥å°±æ¥æ¨èå‡ ä¸ªå¸¸ç”¨çš„ APIç®¡ç†ç³»ç»Ÿï¼Œå¸®åŠ©å‰åç«¯åˆ†ç¦»å¼€å‘æ¨¡å¼ä¸‹æå‡æ•ˆç‡å’Œå¯é æ€§ï¼Œæˆ‘æƒ³æ€»æœ‰ä¸€ä¸ªé€‚åˆé˜ä¸‹å§â˜ï¸\nSwagger Swagger æ˜¯ä¸€ç§ç”¨äºæè¿°ã€æ„å»ºå’Œå¯è§†åŒ– RESTful API çš„å¼€æºå·¥å…·é›†ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—åŠŸèƒ½ï¼ŒåŒ…æ‹¬ API æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆã€API è°ƒè¯•å’Œå¯è§†åŒ–ç­‰ã€‚ä¸‹é¢æ˜¯ä½¿ç”¨ Swagger çš„ä¸€èˆ¬æ­¥éª¤ï¼š\nå®šä¹‰ API è§„èŒƒï¼šä½¿ç”¨ Swagger è§„èŒƒï¼ˆé€šå¸¸æ˜¯ OpenAPI è§„èŒƒï¼‰ç¼–å†™ API çš„å®šä¹‰å’Œæè¿°ã€‚è¿™äº›è§„èŒƒä½¿ç”¨ YAML æˆ– JSON æ ¼å¼è¡¨ç¤ºï¼Œå¹¶æè¿°äº† API çš„è·¯å¾„ã€å‚æ•°ã€æ“ä½œã€å“åº”ç­‰ä¿¡æ¯ã€‚ ç¼–å†™ Swagger æ–‡æ¡£ï¼šæ ¹æ® API çš„å®šä¹‰å’Œæè¿°ç¼–å†™ Swagger æ–‡æ¡£ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ Swagger ç¼–è¾‘å™¨æˆ–å…¶ä»–æ–‡æ¡£å·¥å…·æ¥åˆ›å»ºå’Œç¼–è¾‘ Swagger æ–‡æ¡£ã€‚ è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ï¼šä½¿ç”¨ Swagger å·¥å…·å’Œæ’ä»¶ï¼Œå°† Swagger æ–‡æ¡£ä¸ä»£ç ï¼ˆå¦‚åç«¯æœåŠ¡ä»£ç ï¼‰é›†æˆåœ¨ä¸€èµ·ï¼Œå¹¶ç”Ÿæˆ API æ–‡æ¡£ã€‚è¿™äº›å·¥å…·å¯ä»¥æ ¹æ® Swagger æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆå¯äº¤äº’å¼çš„ API æ–‡æ¡£å’ŒUIç•Œé¢ã€‚ è°ƒè¯•å’Œæµ‹è¯•ï¼šä½¿ç”¨ Swagger æä¾›çš„å†…ç½®åŠŸèƒ½ï¼Œå¯ä»¥åœ¨ Swagger UI ä¸­ç›´æ¥è¿›è¡Œ API è°ƒè¯•å’Œæµ‹è¯•ã€‚é€šè¿‡ Swagger UIï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å‘é€è¯·æ±‚ï¼ŒæŸ¥çœ‹å“åº”å¹¶æ£€æŸ¥è¯·æ±‚å’Œå“åº”çš„è¯¦ç»†ä¿¡æ¯ã€‚ é¡¹ç›®åœ°å€ï¼š ç‚¹æˆ‘è¿›å…¥\nDemoåœ°å€ï¼š ç‚¹æˆ‘è¿›å…¥\neolinker eolinkeræ˜¯ä¸€æ¬¾ç”¨äºAPIç®¡ç†å’Œæ¥å£æµ‹è¯•çš„å·¥å…·ã€‚å®ƒæä¾›äº†ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘äººå‘˜å’Œå›¢é˜Ÿæ›´å¥½åœ°ç®¡ç†å’Œæµ‹è¯•ä»–ä»¬çš„APIæ¥å£ã€‚\neolinkerçš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\næ¥å£æ–‡æ¡£ç®¡ç†ï¼šå¯ä»¥æ–¹ä¾¿åœ°åˆ›å»ºã€ç¼–è¾‘å’Œå…±äº«æ¥å£æ–‡æ¡£ï¼ŒåŒ…æ‹¬æ¥å£çš„URLã€è¯·æ±‚æ–¹å¼ã€å‚æ•°ã€è¿”å›ç»“æœç­‰ã€‚ æ¥å£æµ‹è¯•ï¼šå¯ä»¥å¯¹æ¥å£è¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•ï¼ŒéªŒè¯æ¥å£çš„æ­£ç¡®æ€§å’Œæ€§èƒ½ã€‚ æ¥å£è”è°ƒï¼šå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œæ¥å£è”è°ƒï¼Œæ”¯æŒå¤šäººåä½œï¼Œæé«˜å¼€å‘æ•ˆç‡ã€‚ æ¥å£ç›‘æ§ï¼šå¯ä»¥å®æ—¶ç›‘æ§æ¥å£çš„çŠ¶æ€å’Œæ€§èƒ½ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜ã€‚ æ¥å£æƒé™ç®¡ç†ï¼šå¯ä»¥å¯¹æ¥å£è¿›è¡Œæƒé™æ§åˆ¶ï¼Œä¿è¯æ¥å£çš„å®‰å…¨æ€§å’Œæœºå¯†æ€§ã€‚ é¡¹ç›®åœ°å€ï¼šç‚¹æˆ‘è¿›å…¥\nShowDoc é¦–å…ˆè¿™æ˜¯æˆ‘æœ€æ¨èçš„ä¸€ä¸ªAPIç®¡ç†å·¥å…·ï¼Œå› ä¸ºç•Œé¢ç®€å•ï¼Œé£æ ¼æ¸…æ™°ï¼Œä¸ªäººè§‰å¾—å¾ˆå¥½çœ‹çš„UIç•Œé¢ï¼ï¼ï¼\nShowDocçš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\næ–‡æ¡£ç¼–å†™ï¼šæä¾›äº†å¯Œæ–‡æœ¬ç¼–è¾‘å™¨å’ŒMarkdownç¼–è¾‘å™¨ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ç¼–å†™é¡¹ç›®æ–‡æ¡£ï¼Œæ”¯æŒæ’å…¥å›¾ç‰‡ã€ä»£ç å—ç­‰åŠŸèƒ½ã€‚ æ¥å£ç®¡ç†ï¼šå¯ä»¥åˆ›å»ºå’Œç®¡ç†APIæ¥å£æ–‡æ¡£ï¼ŒåŒ…æ‹¬æ¥å£çš„URLã€è¯·æ±‚æ–¹å¼ã€å‚æ•°ã€è¿”å›ç»“æœç­‰ä¿¡æ¯ï¼Œè¿˜å¯ä»¥è¿›è¡Œæ¥å£æµ‹è¯•ã€‚ å›¢é˜Ÿåä½œï¼šæ”¯æŒå¤šäººåä½œï¼Œå¯ä»¥é‚€è¯·å›¢é˜Ÿæˆå‘˜å…±åŒç¼–è¾‘å’Œç®¡ç†æ–‡æ¡£ï¼Œæ–¹ä¾¿å›¢é˜Ÿé—´çš„æ²Ÿé€šå’Œåˆä½œã€‚ ç‰ˆæœ¬æ§åˆ¶ï¼šæ”¯æŒå¯¹æ–‡æ¡£çš„ç‰ˆæœ¬æ§åˆ¶ï¼Œå¯ä»¥æŸ¥çœ‹å†å²ç‰ˆæœ¬ï¼Œå¹¶è¿›è¡Œæ¯”è¾ƒå’Œæ¢å¤ã€‚ æƒé™ç®¡ç†ï¼šå¯ä»¥å¯¹æ–‡æ¡£è¿›è¡Œæƒé™æ§åˆ¶ï¼Œè®¾ç½®ä¸åŒçš„ç”¨æˆ·è§’è‰²å’Œæƒé™ï¼Œä¿è¯æ–‡æ¡£çš„å®‰å…¨æ€§å’Œæœºå¯†æ€§ã€‚ å¯¼å‡ºå’Œåˆ†äº«ï¼šæ”¯æŒå°†æ–‡æ¡£å¯¼å‡ºä¸ºHTMLã€PDFå’ŒMarkdownæ ¼å¼ï¼Œæ–¹ä¾¿åˆ†äº«å’Œå‘å¸ƒã€‚ é¡¹ç›®åœ°å€ï¼šç‚¹æˆ‘è¿›å…¥\nDemoï¼šç‚¹æˆ‘è¿›å…¥\n","date":"2023-08-10T00:00:00Z","image":"https://img11.360buyimg.com/ddimg/jfs/t1/120366/32/35885/27184/64a24181F7497ff67/58deb560821eb1b2.jpg","permalink":"http://localhost:1313/posts/api/","title":"æ¨èå‡ æ¬¾å¥½ç”¨çš„APIæ–‡æ¡£ç®¡ç†å·¥å…·"},{"content":"OpenEBSå­˜å‚¨ä½¿ç”¨ OpenEBS æ˜¯ä¸€ç§æ¨¡æ‹Ÿäº† AWS çš„ EBSã€é˜¿é‡Œäº‘çš„äº‘ç›˜ç­‰å—å­˜å‚¨å®ç°çš„åŸºäºå®¹å™¨çš„å­˜å‚¨å¼€æºè½¯ä»¶ã€‚OpenEBS æ˜¯ä¸€ç§åŸºäº CAS(Container Attached Storage) ç†å¿µçš„å®¹å™¨è§£å†³æ–¹æ¡ˆï¼Œå…¶æ ¸å¿ƒç†å¿µæ˜¯å­˜å‚¨å’Œåº”ç”¨ä¸€æ ·é‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œå¹¶é€šè¿‡ Kubernetes æ¥åšèµ„æºç¼–æ’ã€‚å…¶æ¶æ„å®ç°ä¸Šï¼Œæ¯ä¸ªå·çš„ Controller éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„ Podï¼Œä¸”ä¸åº”ç”¨ Pod åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ï¼Œå·çš„æ•°æ®ä½¿ç”¨å¤šä¸ª Pod è¿›è¡Œç®¡ç†ã€‚\nOpenEBS æœ‰å¾ˆå¤šç»„ä»¶ï¼Œå¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\næ§åˆ¶å¹³é¢ç»„ä»¶ - ç®¡ç† OpenEBS å·å®¹å™¨ï¼Œé€šå¸¸ä¼šç”¨åˆ°å®¹å™¨ç¼–æ’è½¯ä»¶çš„åŠŸèƒ½ æ•°æ®å¹³é¢ç»„ä»¶ - ä¸ºåº”ç”¨ç¨‹åºæä¾›æ•°æ®å­˜å‚¨ï¼ŒåŒ…å« Jiva å’Œ cStor ä¸¤ä¸ªå­˜å‚¨åç«¯ èŠ‚ç‚¹ç£ç›˜ç®¡ç†å™¨ - å‘ç°ã€ç›‘æ§å’Œç®¡ç†è¿æ¥åˆ° Kubernetes èŠ‚ç‚¹çš„åª’ä½“ ä¸äº‘åŸç”Ÿå·¥å…·çš„æ•´åˆ - ä¸ Prometheusã€Grafanaã€Fluentd å’Œ Jaeger è¿›è¡Œæ•´åˆã€‚ æ§åˆ¶å¹³é¢ OpenEBS ä¸Šä¸‹æ–‡ä¸­çš„æ§åˆ¶å¹³é¢æ˜¯æŒ‡éƒ¨ç½²åœ¨é›†ç¾¤ä¸­çš„ä¸€ç»„å·¥å…·æˆ–ç»„ä»¶ï¼Œå®ƒä»¬è´Ÿè´£ï¼š\nç®¡ç† kubernetes å·¥ä½œèŠ‚ç‚¹ä¸Šå¯ç”¨çš„å­˜å‚¨ é…ç½®å’Œç®¡ç†æ•°æ®å¼•æ“ ä¸ CSI æ¥å£ä»¥ç®¡ç†å·çš„ç”Ÿå‘½å‘¨æœŸ ä¸ CSI å’Œå…¶ä»–å·¥å…·è¿›è¡Œæ¥å£ï¼Œæ‰§è¡Œå¿«ç…§ã€å…‹éš†ã€è°ƒæ•´å¤§å°ã€å¤‡ä»½ã€æ¢å¤ç­‰æ“ä½œã€‚ é›†æˆåˆ°å…¶ä»–å·¥å…·ä¸­ï¼Œå¦‚ Prometheus/Grafana ä»¥è¿›è¡Œé¥æµ‹å’Œç›‘æ§ é›†æˆåˆ°å…¶ä»–å·¥å…·ä¸­è¿›è¡Œè°ƒè¯•ã€æ•…éšœæ’é™¤æˆ–æ—¥å¿—ç®¡ç† OpenEBS æ§åˆ¶å¹³é¢ç”±ä¸€ç»„å¾®æœåŠ¡ç»„æˆï¼Œè¿™äº›å¾®æœåŠ¡æœ¬èº«ç”± Kubernetes ç®¡ç†ï¼Œä½¿ OpenEBS çœŸæ­£æˆä¸º Kubernetes åŸç”Ÿçš„ã€‚ç”± OpenEBS æ§åˆ¶å¹³é¢ç®¡ç†çš„é…ç½®è¢«ä¿å­˜ä¸º Kubernetes è‡ªå®šä¹‰èµ„æºã€‚æ§åˆ¶å¹³é¢çš„åŠŸèƒ½å¯ä»¥åˆ†è§£ä¸ºä»¥ä¸‹å„ä¸ªé˜¶æ®µï¼š\nOpenEBS æä¾›äº†ä¸€ä¸ªåŠ¨æ€ä¾›åº”å™¨ï¼Œå®ƒæ˜¯æ ‡å‡†çš„ Kubernetes å¤–éƒ¨å­˜å‚¨æ’ä»¶ã€‚OpenEBS PV ä¾›åº”å™¨çš„ä¸»è¦ä»»åŠ¡æ˜¯å‘åº”ç”¨ Pod å‘èµ·å·ä¾›åº”ï¼Œå¹¶å®ç°Kubernetes çš„ PV è§„èŒƒã€‚\nm-apiserver æš´éœ²äº†å­˜å‚¨ REST APIï¼Œå¹¶æ‰¿æ‹…äº†å¤§éƒ¨åˆ†çš„å·ç­–ç•¥å¤„ç†å’Œç®¡ç†ã€‚\næ§åˆ¶å¹³é¢å’Œæ•°æ®å¹³é¢ä¹‹é—´çš„è¿æ¥é‡‡ç”¨ Kubernetes sidecar æ¨¡å¼ã€‚æœ‰å¦‚ä¸‹å‡ ä¸ªåœºæ™¯ï¼Œæ§åˆ¶å¹³é¢éœ€è¦ä¸æ•°æ®å¹³é¢è¿›è¡Œé€šä¿¡ã€‚\nå¯¹äº IOPSã€ååé‡ã€å»¶è¿Ÿç­‰å·ç»Ÿè®¡ - é€šè¿‡ volume-exporter sidecarå®ç° ç”¨äºé€šè¿‡å·æ§åˆ¶å™¨ Pod æ‰§è¡Œå·ç­–ç•¥ï¼Œä»¥åŠé€šè¿‡å·å¤åˆ¶ Pod è¿›è¡Œç£ç›˜/æ± ç®¡ç† - é€šè¿‡å·ç®¡ç† sidecar å®ç°ã€‚ OpenEBS Local Pv OpenEBS ä¸ºKubernetes Local Volumesæä¾›åŠ¨æ€ PV ä¾›åº”å™¨ã€‚æœ¬åœ°å·æ„å‘³ç€å­˜å‚¨åªèƒ½ä»å•ä¸ªèŠ‚ç‚¹ä½¿ç”¨ã€‚æœ¬åœ°å·è¡¨ç¤ºå·²æŒ‚è½½çš„æœ¬åœ°å­˜å‚¨è®¾å¤‡ï¼Œä¾‹å¦‚ç£ç›˜ã€åˆ†åŒºæˆ–ç›®å½•ã€‚\nç”±äº Local Volume åªèƒ½ä»å•ä¸ªèŠ‚ç‚¹è®¿é—®ï¼Œå› æ­¤æœ¬åœ°å·å—åº•å±‚èŠ‚ç‚¹å¯ç”¨æ€§çš„å½±å“ï¼Œå¹¶ä¸é€‚åˆæ‰€æœ‰åº”ç”¨ç¨‹åºã€‚å¦‚æœä¸€ä¸ªèŠ‚ç‚¹å˜å¾—ä¸å¥åº·ï¼Œé‚£ä¹ˆæœ¬åœ°å·ä¹Ÿå°†å˜å¾—ä¸å¯è®¿é—®ï¼Œä½¿ç”¨å®ƒçš„ Pod å°†æ— æ³•è¿è¡Œã€‚ä½¿ç”¨æœ¬åœ°å·çš„åº”ç”¨ç¨‹åºå¿…é¡»èƒ½å¤Ÿå®¹å¿è¿™ç§å¯ç”¨æ€§é™ä½ä»¥åŠæ½œåœ¨çš„æ•°æ®ä¸¢å¤±ï¼Œå…·ä½“å–å†³äºåº•å±‚ç£ç›˜çš„è€ç”¨æ€§ç‰¹å¾ã€‚\nå¯ä»¥ä»æœ¬åœ°å·ä¸­å—ç›Šçš„è‰¯å¥½å·¥ä½œè´Ÿè½½ç¤ºä¾‹åŒ…æ‹¬ï¼š\nå¤åˆ¶æ•°æ®åº“ï¼Œå¦‚ MongoDBã€Cassandra å¯ä»¥ä½¿ç”¨è‡ªå·±çš„é«˜å¯ç”¨æ€§é…ç½®ï¼ˆå¦‚ Elasticã€MinIOï¼‰é…ç½®çš„æœ‰çŠ¶æ€å·¥ä½œè´Ÿè½½ é€šå¸¸åœ¨å•ä¸ªèŠ‚ç‚¹æˆ–å•èŠ‚ç‚¹ Kubernetes é›†ç¾¤ä¸­è¿è¡Œçš„è¾¹ç¼˜å·¥ä½œè´Ÿè½½ã€‚ OpenEBS é€šè¿‡æä¾› Kubernetes å½“å‰ç¼ºå°‘çš„åŠŸèƒ½æ¥å¸®åŠ©ç”¨æˆ·å°†æœ¬åœ°å·æŠ•å…¥ç”Ÿäº§ï¼Œä¾‹å¦‚ï¼š\næœ¬åœ°å·çš„åŠ¨æ€ PV Provisionerã€‚ ç”± Ext3ã€XFSã€LVM æˆ– ZFS ç­‰æ–‡ä»¶ç³»ç»Ÿä¸Šçš„ä¸»æœºè·¯å¾„æ”¯æŒçš„æœ¬åœ°å·ã€‚ ç›‘æ§ç”¨äºåˆ›å»ºæœ¬åœ°å·çš„åº•å±‚è®¾å¤‡æˆ–å­˜å‚¨çš„å¥åº·çŠ¶å†µã€‚ å®¹é‡ç®¡ç†åŠŸèƒ½ï¼Œå¦‚è¿‡åº¦é…ç½®å’Œ/æˆ–é…é¢å¼ºåˆ¶æ‰§è¡Œã€‚ å½“æœ¬åœ°å·ç”± ZFS ç­‰é«˜çº§æ–‡ä»¶ç³»ç»Ÿæ”¯æŒæ—¶ï¼Œå¯ä»¥ä½¿ç”¨å¿«ç…§ã€å…‹éš†ã€å‹ç¼©ç­‰åº•å±‚å­˜å‚¨åŠŸèƒ½ã€‚ é€šè¿‡ Velero è¿›è¡Œå¤‡ä»½å’Œæ¢å¤ã€‚ é€šè¿‡ LUKS æˆ–ä½¿ç”¨åº•å±‚æ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚ ZFSï¼‰çš„å†…ç½®åŠ å¯†æ”¯æŒæ¥ä¿æŠ¤æœ¬åœ°å·ã€‚ èŠ‚ç‚¹ç£ç›˜ç®¡ç†å™¨(NDM) èŠ‚ç‚¹ç£ç›˜ç®¡ç†å™¨ï¼ˆNDMï¼‰æ˜¯OpenEBSæ¶æ„ä¸­çš„ä¸€ä¸ªé‡è¦ç»„ä»¶ã€‚NDM å°†å—è®¾å¤‡è§†ä¸ºéœ€è¦ç›‘æ§å’Œç®¡ç†çš„èµ„æºï¼Œå°±åƒå…¶ä»–èµ„æºï¼ˆå¦‚ CPUã€å†…å­˜å’Œç½‘ç»œï¼‰ä¸€æ ·ã€‚å®ƒæ˜¯ä¸€ä¸ªè¿è¡Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„å®ˆæŠ¤è¿›ç¨‹ï¼Œæ ¹æ®è¿‡æ»¤å™¨æ£€æµ‹é™„åŠ çš„å—è®¾å¤‡å¹¶å°†å®ƒä»¬ä½œä¸ºå—è®¾å¤‡è‡ªå®šä¹‰èµ„æºåŠ è½½åˆ° Kubernetes ä¸­ã€‚è¿™äº›è‡ªå®šä¹‰èµ„æºæ—¨åœ¨é€šè¿‡æä¾›ä»¥ä¸‹åŠŸèƒ½æ¥å¸®åŠ©è¶…èåˆå­˜å‚¨è¿è¥å•†ï¼š\næ˜“äºè®¿é—® Kubernetes é›†ç¾¤ä¸­å¯ç”¨çš„å—è®¾å¤‡æ¸…å•ã€‚ é¢„æµ‹ç£ç›˜æ•…éšœä»¥å¸®åŠ©é‡‡å–é¢„é˜²æªæ–½ã€‚ å…è®¸åŠ¨æ€åœ°å°†ç£ç›˜é™„åŠ /åˆ†ç¦»åˆ°å­˜å‚¨ podï¼Œè€Œæ— éœ€é‡æ–°å¯åŠ¨åœ¨ç£ç›˜é™„åŠ /åˆ†ç¦»çš„èŠ‚ç‚¹ä¸Šè¿è¡Œçš„ç›¸åº” NDM podã€‚ Node Disk Manageråœ¨Kubernetesä¸­æ˜¯ä»¥DaemonSetçš„æ–¹å¼è¿›è¡Œè¿è¡Œçš„ Node Disk Manager å°½ç®¡æ‰§è¡Œäº†ä¸Šè¿°æ‰€æœ‰æ“ä½œï¼Œä½† NDM æœ‰åŠ©äºæ•´ä½“ç®€åŒ–æŒä¹…å·çš„é…ç½®ã€‚\nNDM åœ¨å®‰è£… OpenEBS æœŸé—´éƒ¨ç½²ä¸ºå®ˆæŠ¤è¿›ç¨‹ã€‚NDM daemonset å‘ç°æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ç£ç›˜å¹¶åˆ›å»ºç§°ä¸ºå—è®¾å¤‡æˆ– BD çš„è‡ªå®šä¹‰èµ„æºã€‚\nå¯ç”¨OpenEBS ç”±äº OpenEBS é€šè¿‡ iSCSI åè®®æä¾›å­˜å‚¨æ”¯æŒï¼Œå› æ­¤ï¼Œéœ€è¦åœ¨æ‰€æœ‰ Kubernetes èŠ‚ç‚¹ä¸Šéƒ½å®‰è£… iSCSI å®¢æˆ·ç«¯ï¼ˆå¯åŠ¨å™¨ï¼‰ã€‚\næ¯”å¦‚æˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„æ˜¯Rockyçš„ç³»ç»Ÿï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤å®‰è£…å¯åŠ¨ iSCSI å¯åŠ¨å™¨ï¼š\ndnf install iscsi-initiator-utils -y # æŸ¥çœ‹iSCSIçŠ¶æ€æ˜¯å¦æ­£å¸¸ cat /etc/iscsi/initiatorname.iscsi # å¯åŠ¨iSCSI systemctl start iscsid.service systemctl status iscsid.service å®‰è£…OpenEBS ä½¿ç”¨kubectlçš„æ–¹å¼è¿›è¡Œå®‰è£… Helméƒ¨ç½²ä¹Ÿæ˜¯å¯é€‰çš„: åœ°å€ [root@Online-Beijing-master1 ~]# kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml # æ£€æŸ¥æ˜¯å¦å®‰è£…å®Œæˆ,æ­£å¸¸åº”è¯¥éƒ½æ˜¯Runningå³å¯ [root@Online-Beijing-master1 ~]# kubectl get pods -n openebs NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-846c6bdc56-vvvsv 1/1 Running 0 6m11s openebs-ndm-5sfdk 1/1 Running 0 6m11s openebs-ndm-cluster-exporter-b49987ffb-vjq87 1/1 Running 0 6m11s openebs-ndm-kthfj 1/1 Running 0 6m11s openebs-ndm-node-exporter-94zhh 1/1 Running 0 6m11s openebs-ndm-node-exporter-q9h5p 1/1 Running 0 6m11s openebs-ndm-node-exporter-x7z9t 1/1 Running 0 6m11s openebs-ndm-operator-6469f6bb4c-95kss 1/1 Running 0 6m11s openebs-ndm-wf9k2 1/1 Running 0 6m11s # æ­£å¸¸æˆ‘ä»¬ä¼šæœ‰ä¸¤ä¸ªStorageClass [root@Online-Beijing-master1 ~]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-device openebs.io/local Delete WaitForFirstConsumer false 20m openebs-hostpath openebs.io/local Delete WaitForFirstConsumer false 20m æˆ‘ä»¬è‡ªå·±åˆ›å»ºä¸€ä¸ªPvcå¯¹è±¡æ¥ç»™æˆ‘ä»¬çš„Deploymentæ¥è¿›è¡Œä½¿ç”¨ apiVersion: v1 kind: PersistentVolumeClaim metadata: name: local-hostpath-pvc spec: storageClassName: openebs-hostpath accessModes: - ReadWriteOnce resources: requests: storage: 1Gi åˆ›å»ºä¸€ä¸ªPodè¿›è¡Œæµ‹è¯• apiVersion: v1 kind: Pod metadata: name: hello-local-hostpath-pod spec: volumes: - name: local-storage persistentVolumeClaim: claimName: local-hostpath-pvc containers: - name: hello-container image: busybox command: - sh - -c - \u0026#39;while true; do echo \u0026#34;`date` [`hostname`] Hello from OpenEBS Local PV.\u0026#34; \u0026gt;\u0026gt; /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done\u0026#39; volumeMounts: - mountPath: /data name: local-storage æ³¨æ„ï¼šå¦‚æœä½ æƒ³çŸ¥é“è¿™ä¸ªPvcå…·ä½“æŒ‚è½½ä½ç½®å¯ä»¥ä½¿ç”¨kubectl describe pv pvåç§°,å…¶ä¸­çš„Ptah: /var/openebs/local/pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188å°±æ˜¯æˆ‘ä»¬æ‰€åœ¨èŠ‚ç‚¹çš„è·¯å¾„\n[root@Online-Beijing-master1 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE example-local 20Gi RWO Delete Bound default/bound-tasknginx local-storage 26d pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 1Gi RWO Delete Bound default/local-hostpath-pvc openebs-hostpath 3m53s [root@Online-Beijing-master1 ~]# kubectl describe pv pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 Name: pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 Labels: openebs.io/cas-type=local-hostpath Annotations: pv.kubernetes.io/provisioned-by: openebs.io/local Finalizers: [kubernetes.io/pv-protection] StorageClass: openebs-hostpath Status: Bound Claim: default/local-hostpath-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 1Gi Node Affinity: Required Terms: Term 0: kubernetes.io/hostname in [online-beijing-node1] Message: Source: Type: LocalVolume (a persistent volume backed by local storage on a node) Path: /var/openebs/local/pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 Events: \u0026lt;none\u0026gt; è¿›å…¥æ‰€åœ¨èŠ‚ç‚¹çš„ç›®å½•è¿›è¡ŒéªŒè¯ cd /var/openebs/local/pvc-e011f2a1-27dc-46d0-ba34-9ad44ba03188 # éšä¾¿åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ touch openebs.txt # è¿›å…¥å®¹å™¨å†…éƒ¨ kubectl exec -it nginx-67fbcff654-glklg bash # æŸ¥çœ‹æ‰€æŒ‚è½½openebsçš„è·¯å¾„æ˜¯å¦æˆåŠŸæœ‰openebs.txt root@nginx-67fbcff654-glklg:/# ls /data/ 1.txt openebs.txt ä¿®æ”¹OpenEBSçš„HostPathé»˜è®¤å­˜å‚¨ ä¿®æ”¹åå­—ä¸ºopenebs-hostpathçš„StorageClasså½“ä¸­çš„BasePathå³å¯ apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: openebs-hostpath annotations: openebs.io/cas-type: local cas.openebs.io/config: | #hostpath type will create a PV by # creating a sub-directory under the # BASEPATH provided below. - name: StorageType value: \u0026#34;hostpath\u0026#34; #Specify the location (directory) where # where PV(volume) data will be saved. # A sub-directory with pv-name will be # created. When the volume is deleted, # the PV sub-directory will be deleted. #Default value is /var/openebs/local - name: BasePath value: \u0026#34;/var/openebs/local/\u0026#34; ","date":"2023-05-14T00:00:00Z","image":"https://openebs.io/docs/assets/images/control-plane-overview-93c59878e3356a11f03029dd0fc1cd6b.svg","permalink":"http://localhost:1313/kubernetes/openebs/","title":"OpenEBSå­˜å‚¨çš„ä½¿ç”¨"},{"content":"Traekfikæ˜¯ä»€ä¹ˆ Traefik æ˜¯ä¸€ç§å¼€æº è¾¹ç¼˜è·¯ç”±å™¨ï¼Œå®ƒä½¿æ‚¨å‘å¸ƒæœåŠ¡æˆä¸ºä¸€ç§æœ‰è¶£è€Œè½»æ¾çš„ä½“éªŒã€‚å®ƒä»£è¡¨æ‚¨çš„ç³»ç»Ÿæ¥æ”¶è¯·æ±‚å¹¶æ‰¾å‡ºå“ªäº›ç»„ä»¶è´Ÿè´£å¤„ç†å®ƒä»¬ã€‚\nTraefik çš„ä¸ä¼—ä¸åŒä¹‹å¤„åœ¨äºï¼Œé™¤äº†å®ƒçš„è®¸å¤šåŠŸèƒ½ä¹‹å¤–ï¼Œå®ƒè¿˜å¯ä»¥è‡ªåŠ¨ä¸ºæ‚¨çš„æœåŠ¡å‘ç°æ­£ç¡®çš„é…ç½®ã€‚å½“ Traefik æ£€æŸ¥æ‚¨çš„åŸºç¡€æ¶æ„æ—¶ï¼Œå¥‡è¿¹å°±ä¼šå‘ç”Ÿï¼Œå®ƒä¼šåœ¨å…¶ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯å¹¶å‘ç°å“ªä¸ªæœåŠ¡æœåŠ¡äºå“ªä¸ªè¯·æ±‚ã€‚\nTraefik åŸç”Ÿå…¼å®¹æ‰€æœ‰ä¸»è¦çš„é›†ç¾¤æŠ€æœ¯ï¼Œä¾‹å¦‚ Kubernetesã€Dockerã€Docker Swarmã€AWSã€Mesosã€Marathonï¼Œç­‰ç­‰ï¼›å¹¶ä¸”å¯ä»¥åŒæ—¶å¤„ç†å¾ˆå¤šã€‚ï¼ˆå®ƒç”šè‡³é€‚ç”¨äºåœ¨è£¸æœºä¸Šè¿è¡Œçš„é—ç•™è½¯ä»¶ã€‚ï¼‰\nä½¿ç”¨ Traefikï¼Œæ— éœ€ç»´æŠ¤å’ŒåŒæ­¥å•ç‹¬çš„é…ç½®æ–‡ä»¶ï¼šä¸€åˆ‡éƒ½è‡ªåŠ¨å®æ—¶å‘ç”Ÿï¼ˆæ— éœ€é‡å¯ï¼Œæ— è¿æ¥ä¸­æ–­ï¼‰ã€‚ä½¿ç”¨ Traefikï¼Œæ‚¨å¯ä»¥èŠ±æ—¶é—´ä¸ºç³»ç»Ÿå¼€å‘å’Œéƒ¨ç½²æ–°åŠŸèƒ½ï¼Œè€Œä¸æ˜¯é…ç½®å’Œç»´æŠ¤å…¶å·¥ä½œçŠ¶æ€ã€‚\nè¾¹ç¼˜è·¯ç”±å™¨ Traefik æ˜¯ä¸€ä¸ªEdge Routerï¼Œè¿™æ„å‘³ç€å®ƒæ˜¯æ‚¨å¹³å°çš„å¤§é—¨ï¼Œå®ƒæ‹¦æˆªå¹¶è·¯ç”±æ¯ä¸ªä¼ å…¥è¯·æ±‚ï¼šå®ƒçŸ¥é“ç¡®å®šå“ªäº›æœåŠ¡å¤„ç†å“ªäº›è¯·æ±‚çš„æ‰€æœ‰é€»è¾‘å’Œæ¯æ¡è§„åˆ™ï¼ˆåŸºäºpathï¼Œhostï¼Œæ ‡å¤´ï¼Œç­‰ç­‰â€¦ï¼‰ã€‚\nè‡ªåŠ¨æœåŠ¡å‘ç° ä¼ ç»Ÿä¸Šè¾¹ç¼˜è·¯ç”±å™¨ï¼ˆæˆ–åå‘ä»£ç†ï¼‰éœ€è¦ä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«åˆ°æ‚¨çš„æœåŠ¡çš„æ¯æ¡å¯èƒ½è·¯å¾„ï¼ŒTraefik ä»æœåŠ¡æœ¬èº«è·å–å®ƒä»¬ã€‚éƒ¨ç½²æ‚¨çš„æœåŠ¡ï¼Œæ‚¨é™„åŠ ä¿¡æ¯å‘Šè¯‰ Traefik æœåŠ¡å¯ä»¥å¤„ç†çš„è¯·æ±‚çš„ç‰¹å¾ã€‚\né¦–å…ˆï¼Œå½“å¯åŠ¨ Traefik æ—¶ï¼Œéœ€è¦å®šä¹‰ entrypointsï¼ˆå…¥å£ç‚¹ï¼‰ï¼Œç„¶åï¼Œæ ¹æ®è¿æ¥åˆ°è¿™äº› entrypoints çš„è·¯ç”±æ¥åˆ†æä¼ å…¥çš„è¯·æ±‚ï¼Œæ¥æŸ¥çœ‹ä»–ä»¬æ˜¯å¦ä¸ä¸€ç»„è§„åˆ™ç›¸åŒ¹é…ï¼Œå¦‚æœåŒ¹é…ï¼Œåˆ™è·¯ç”±å¯èƒ½ä¼šå°†è¯·æ±‚é€šè¿‡ä¸€ç³»åˆ—ä¸­é—´ä»¶è½¬æ¢è¿‡åå†è½¬å‘åˆ°ä½ çš„æœåŠ¡ä¸Šå»ã€‚åœ¨äº†è§£ Traefik ä¹‹å‰æœ‰å‡ ä¸ªæ ¸å¿ƒæ¦‚å¿µæˆ‘ä»¬å¿…é¡»è¦äº†è§£ï¼š\nProviders ç”¨æ¥è‡ªåŠ¨å‘ç°å¹³å°ä¸Šçš„æœåŠ¡ï¼Œå¯ä»¥æ˜¯ç¼–æ’å·¥å…·ã€å®¹å™¨å¼•æ“æˆ–è€… key-value å­˜å‚¨ç­‰ï¼Œæ¯”å¦‚ Dockerã€Kubernetesã€File Entrypoints ç›‘å¬ä¼ å…¥çš„æµé‡ï¼ˆç«¯å£ç­‰â€¦ï¼‰ï¼Œæ˜¯ç½‘ç»œå…¥å£ç‚¹ï¼Œå®ƒä»¬å®šä¹‰äº†æ¥æ”¶è¯·æ±‚çš„ç«¯å£ï¼ˆHTTP æˆ–è€… TCPï¼‰ã€‚ Routers åˆ†æè¯·æ±‚ï¼ˆhost, path, headers, SSL, â€¦ï¼‰ï¼Œè´Ÿè´£å°†ä¼ å…¥è¯·æ±‚è¿æ¥åˆ°å¯ä»¥å¤„ç†è¿™äº›è¯·æ±‚çš„æœåŠ¡ä¸Šå»ã€‚ Services å°†è¯·æ±‚è½¬å‘ç»™ä½ çš„åº”ç”¨ï¼ˆload balancing, â€¦ï¼‰ï¼Œè´Ÿè´£é…ç½®å¦‚ä½•è·å–æœ€ç»ˆå°†å¤„ç†ä¼ å…¥è¯·æ±‚çš„å®é™…æœåŠ¡ã€‚ Middlewares ä¸­é—´ä»¶ï¼Œç”¨æ¥ä¿®æ”¹è¯·æ±‚æˆ–è€…æ ¹æ®è¯·æ±‚æ¥åšå‡ºä¸€äº›åˆ¤æ–­ï¼ˆauthentication, rate limiting, headers, â€¦ï¼‰ï¼Œä¸­é—´ä»¶è¢«é™„ä»¶åˆ°è·¯ç”±ä¸Šï¼Œæ˜¯ä¸€ç§åœ¨è¯·æ±‚å‘é€åˆ°ä½ çš„æœåŠ¡ä¹‹å‰ï¼ˆæˆ–è€…åœ¨æœåŠ¡çš„å“åº”å‘é€åˆ°å®¢æˆ·ç«¯ä¹‹å‰ï¼‰è°ƒæ•´è¯·æ±‚çš„ä¸€ç§æ–¹æ³•ã€‚ éƒ¨ç½²Traefik Traefikçš„é…ç½®å¯ä»¥ä½¿ç”¨ä¸¤ç§æ–¹å¼ï¼šé™æ€é…ç½®å’ŒåŠ¨æ€é…ç½®\né™æ€é…ç½®ï¼šåœ¨ Traefik ä¸­å®šä¹‰é™æ€é…ç½®é€‰é¡¹æœ‰ä¸‰ç§ä¸åŒçš„ã€äº’æ–¥çš„å³ä½ åªèƒ½åŒæ—¶ä½¿ç”¨ä¸€ç§ï¼‰æ–¹å¼ã€‚ åœ¨é…ç½®æ–‡ä»¶ä¸­ åœ¨å‘½ä»¤è¡Œå‚æ•°ä¸­ ä½œä¸ºç¯å¢ƒå˜é‡ åŠ¨æ€é…ç½®ï¼šTraefikä»æä¾›è€…å¤„è·å–å…¶åŠ¨æ€é…ç½®ï¼šæ— è®ºæ˜¯ç¼–æ’å™¨ã€æœåŠ¡æ³¨å†Œè¡¨è¿˜æ˜¯æ™®é€šçš„æ—§é…ç½®æ–‡ä»¶ã€‚ # ä½¿ç”¨Helmçš„æ–¹å¼è¿›è¡Œéƒ¨ç½²Traefik2.9.x [root@Online-Beijing-master1 ~]# helm repo add traefik https://traefik.github.io/charts [root@Online-Beijing-master1 ~]# helm repo update [root@Online-Beijing-master1 yaml]# helm fetch traefik/traefik [root@Online-Beijing-master1 yaml]# tar -zxf traefik-21.1.0.tgz ä¿®æ”¹ä¸€ä¸‹value.yamlä¸­çš„éƒ¨åˆ†å†…å®¹ï¼Œæ”¹åŠ¨å¤§æ¦‚å¦‚ä¸‹éƒ¨åˆ†çš„å†…å®¹ deployment: initContainers: # The \u0026#34;volume-permissions\u0026#34; init container is required if you run into permission issues. # Related issue: https://github.com/traefik/traefik/issues/6825 - name: volume-permissions image: busybox:1.35 command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;touch /data/acme.json \u0026amp;\u0026amp; chmod -Rv 600 /data/* \u0026amp;\u0026amp; chown 65532:65532 /data/acme.json\u0026#34;] volumeMounts: - name: data mountPath: /data websecure: port: 8443 hostPort: 443 expose: true exposedPort: 443 protocol: TCP web: port: 8000 hostPort: 80 expose: true exposedPort: 80 protocol: TCP service: enabled: false ingressRoute: dashboard: enabled: false nodeSelector: node.kubernetes.io/traefik-manager: \u0026#39;true\u0026#39; tolerations: - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; åˆ›å»ºä¸€ä¸ªtraefik-v2çš„åç§°ç©ºé—´ [root@Online-Beijing-master1 yaml]# kubectl create ns traefik-v2 éƒ¨ç½²Traefik [root@Online-Beijing-master1 yaml]# helm install traefik ./traefik -f ./traefik/values.yaml --namespace traefik-v2 [root@Online-Beijing-master1 yaml]# kubectl get pods traefik-67b8896675-4xdrx -n traefik-v2 -o yaml å…¶ä¸­ entryPoints å±æ€§å®šä¹‰äº† web å’Œ websecure è¿™ä¸¤ä¸ªå…¥å£ç‚¹çš„ï¼Œå¹¶å¼€å¯ kubernetesingress å’Œ kubernetescrd è¿™ä¸¤ä¸ª providerï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Kubernetes åŸæœ¬çš„ Ingress èµ„æºå¯¹è±¡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Traefik è‡ªå·±æ‰©å±•çš„ IngressRoute è¿™æ ·çš„ CRD èµ„æºå¯¹è±¡\napiVersion: v1 kind: Pod metadata: ...... spec: containers: - args: - --global.checknewversion - --global.sendanonymoususage - --entrypoints.metrics.address=:9100/tcp - --entrypoints.traefik.address=:9000/tcp - --entrypoints.web.address=:8000/tcp - --entrypoints.websecure.address=:8443/tcp - --api.dashboard=true - --ping=true - --metrics.prometheus=true - --metrics.prometheus.entrypoint=metrics - --providers.kubernetescrd - --providers.kubernetesingress - --entrypoints.websecure.http.tls=true åˆ›å»ºç”¨äº Dashboard è®¿é—®çš„ IngressRoute èµ„æº apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-dashboard namespace: traefik-v2 spec: entryPoints: - web # è¿™é‡Œå¯¹åº”çš„æ˜¯webçš„entryPoints å¦‚æœæ˜¯httpså°±éœ€è¦ä½¿ç”¨websecureçš„entryPoints routes: - match: Host(`traefik.kube.com`) # æŒ‡å®šåŸŸå kind: Rule services: - name: api@internal kind: TraefikService # å¼•ç”¨å¦å¤–çš„ Traefik Service [root@Online-Beijing-master1 yaml]# kubectl apply -f traefik.yaml ingressroute.traefik.containo.us/traefik-dashboard created\tTraefikçš„åŸºæœ¬ä½¿ç”¨ è‡ªå®šä¹‰ä¸€ä¸ªIngressRoute å‡è®¾æˆ‘ä»¬è¦è®¿é—®ä¸€ä¸ªç®€å•åœ°nginxæœåŠ¡,ä¸‹é¢æ˜¯traefikçš„åŒ¹é…è§„åˆ™\nåŒ¹é…è§„åˆ™ æè¿° HeadersRegexp(key, regexp) keyæ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦å®šä¹‰äº†é”®ï¼Œå…¶å€¼ä¸æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…regexp Host(example.com, ...) æ£€æŸ¥è¯·æ±‚åŸŸï¼ˆä¸»æœºæ ‡å¤´å€¼ï¼‰æ˜¯å¦é’ˆå¯¹ç»™å®šçš„domains. HostHeader(example.com, ...) ä¸ ç›¸åŒHostï¼Œä»…å› å†å²åŸå› è€Œå­˜åœ¨ã€‚ HostRegexp(example.com, {subdomain:[a-z]+}.example.com, ...) åŒ¹é…è¯·æ±‚åŸŸã€‚è¯·å‚é˜…ä¸‹é¢çš„â€œæ­£åˆ™è¡¨è¾¾å¼è¯­æ³•â€ã€‚ Method(GET, ...) æ£€æŸ¥è¯·æ±‚æ–¹æ³•æ˜¯å¦ä¸ºç»™å®šçš„methods( GET, POST, PUT, DELETE, PATCH, HEAD)ä¹‹ä¸€ Path(/path, /articles/{cat:[a-z]+}/{id:[0-9]+}, ...) åŒ¹é…å‡†ç¡®çš„è¯·æ±‚è·¯å¾„ã€‚è¯·å‚é˜…ä¸‹é¢çš„â€œæ­£åˆ™è¡¨è¾¾å¼è¯­æ³•â€ã€‚ PathPrefix(/products/, /articles/{cat:[a-z]+}/{id:[0-9]+}) åŒ¹é…è¯·æ±‚å‰ç¼€è·¯å¾„ã€‚è¯·å‚é˜…ä¸‹é¢çš„â€œæ­£åˆ™è¡¨è¾¾å¼è¯­æ³•â€ã€‚ Query(foo=bar, bar=baz) åŒ¹é…æŸ¥è¯¢å­—ç¬¦ä¸²å‚æ•°ã€‚å®ƒæ¥å—ä¸€ç³»åˆ—é”®=å€¼å¯¹ã€‚ ClientIP(10.0.0.0/16, ::1) å¦‚æœè¯·æ±‚å®¢æˆ·ç«¯ IP æ˜¯ç»™å®šçš„ IP/CIDR ä¹‹ä¸€ï¼Œåˆ™åŒ¹é…ã€‚å®ƒæ¥å— IPv4ã€IPv6 å’Œ CIDR apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-http namespace: default spec: entryPoints: - web # ä¾æ—§å¯¹åº”web routes: - match: Host(`traefik.qingyang.com`) # æŒ‡å®šåŸŸå kind: Rule services: - name: vue-demo # è¿™ä¸ªåœ°æ–¹å¯¹åº”kubernetesçš„svc port: 80 å¦‚æœæˆ‘ä»¬éœ€è¦å¼€å¯HTTPSæ¥è®¿é—®æˆ‘ä»¬è¿™ä¸ªåº”ç”¨çš„è¯ï¼Œå°±éœ€è¦ç›‘å¬ websecure è¿™ä¸ªå…¥å£ç‚¹ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡ 443 ç«¯å£æ¥è®¿é—®ï¼ŒåŒæ ·ç”¨ HTTPS è®¿é—®åº”ç”¨å¿…ç„¶å°±éœ€è¦è¯ä¹¦ï¼Œè¿™é‡Œæˆ‘ä»¬ç”¨ openssl æ¥åˆ›å»ºä¸€ä¸ªè‡ªç­¾åçš„è¯ä¹¦ï¼š\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=traefik.qingyang.com\u0026#34; ç„¶ååˆ›å»ºsecret\n[root@Online-Beijing-master1 yaml]# kubectl create secret tls traefik-demo-tls --cert=tls.crt --key=tls.key è¿™ä¸ªæ—¶å€™æˆ‘ä»¬å°±å¯ä»¥åˆ›å»ºä¸€ä¸ª HTTPS è®¿é—®åº”ç”¨çš„ IngressRoute å¯¹è±¡äº†\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-https spec: entryPoints: - websecure routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: vue-demo port: 80 tls: secretName: traefik-demo-tls ACMEè‡ªåŠ¨ç­¾å‘ raefik åŒæ ·ä¹Ÿæ”¯æŒä½¿ç”¨ Letâ€™s Encrypt è‡ªåŠ¨ç”Ÿæˆè¯ä¹¦ï¼Œè¦ä½¿ç”¨ Letâ€™s Encrypt æ¥è¿›è¡Œè‡ªåŠ¨åŒ– HTTPSï¼Œå°±éœ€è¦é¦–å…ˆå¼€å¯ ACMEï¼Œå¼€å¯ ACME éœ€è¦é€šè¿‡é™æ€é…ç½®çš„æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯è¯´å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ã€å¯åŠ¨å‚æ•°ç­‰æ–¹å¼æ¥æä¾›ã€‚\nACME æœ‰å¤šç§æ ¡éªŒæ–¹å¼ tlsChallengeã€httpChallenge å’Œ dnsChallenge ä¸‰ç§éªŒè¯æ–¹å¼ï¼Œä¹‹å‰æ›´å¸¸ç”¨çš„æ˜¯ http è¿™ç§éªŒè¯æ–¹å¼(å¯ä»¥ç™¾åº¦ä¸€ä¸‹)è¿™å‡ ç§çš„æ ¡éªŒæ–¹å¼ã€‚è¦ä½¿ç”¨ tls æ ¡éªŒæ–¹å¼çš„è¯éœ€è¦ä¿è¯ Traefik çš„ 443 ç«¯å£æ˜¯å¯è¾¾çš„ï¼Œdns æ ¡éªŒæ–¹å¼å¯ä»¥ç”Ÿæˆé€šé…ç¬¦çš„è¯ä¹¦ï¼Œåªéœ€è¦é…ç½®ä¸Š DNS è§£ææœåŠ¡å•†çš„ API è®¿é—®å¯†é’¥å³å¯æ ¡éªŒã€‚æˆ‘ä»¬è¿™é‡Œç”¨ DNS æ ¡éªŒçš„æ–¹å¼æ¥ä¸ºå¤§å®¶è¯´æ˜å¦‚ä½•é…ç½® ACMEã€‚\né‡æ–°ä¿®æ”¹ä¸€ä¸‹åˆšåˆšTraefikçš„å‚æ•°values.yaml additionalArguments: # ä½¿ç”¨ dns éªŒè¯æ–¹å¼ - --certificatesResolvers.ali.acme.dnsChallenge.provider=alidns # å…ˆä½¿ç”¨stagingç¯å¢ƒè¿›è¡ŒéªŒè¯ï¼ŒéªŒè¯æˆåŠŸåå†ä½¿ç”¨ç§»é™¤ä¸‹é¢ä¸€è¡Œçš„é…ç½® # - --certificatesResolvers.ali.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory # é‚®ç®±é…ç½® - --certificatesResolvers.ali.acme.email=ych_1024@163.com # ä¿å­˜ ACME è¯ä¹¦çš„ä½ç½® - --certificatesResolvers.ali.acme.storage=/data/acme.json envFrom: - secretRef: name: traefik-alidns-secret # ALICLOUD_ACCESS_KEY # ALICLOUD_SECRET_KEY # ALICLOUD_REGION_ID persistence: enabled: true # å¼€å¯æŒä¹…åŒ– accessMode: ReadWriteOnce size: 128Mi path: /data # ç”±äºä¸Šé¢æŒä¹…åŒ–äº†ACMEçš„æ•°æ®ï¼Œéœ€è¦é‡æ–°é…ç½®ä¸‹é¢çš„å®‰å…¨ä¸Šä¸‹æ–‡ securityContext: readOnlyRootFilesystem: false runAsGroup: 0 runAsUser: 0 runAsNonRoot: false ç„¶åæ›´æ–°traefik\nhelm up traefik ./traefik -f ./traefik/values.yaml --namespace traefik-v2 è¿™æ ·æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½® --certificatesresolvers.ali.acme.dnschallenge.provider=alidns å‚æ•°æ¥æŒ‡å®šæŒ‡å®šé˜¿é‡Œäº‘çš„ DNS æ ¡éªŒï¼Œè¦ä½¿ç”¨é˜¿é‡Œäº‘çš„ DNS æ ¡éªŒæˆ‘ä»¬è¿˜éœ€è¦é…ç½®3ä¸ªç¯å¢ƒå˜é‡ï¼šALICLOUD_ACCESS_KEYã€ALICLOUD_SECRET_KEYã€ALICLOUD_REGION_IDï¼Œåˆ†åˆ«å¯¹åº”æˆ‘ä»¬å¹³æ—¶å¼€å‘é˜¿é‡Œäº‘åº”ç”¨çš„æ—¶å€™çš„å¯†é’¥ï¼Œå¯ä»¥ç™»å½•é˜¿é‡Œäº‘åå°è·å–ï¼Œç”±äºè¿™æ˜¯æ¯”è¾ƒç§å¯†çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨ Secret å¯¹è±¡æ¥åˆ›å»º kubectl create secret generic traefik-alidns-secret --from-literal=ALICLOUD_ACCESS_KEY=\u0026lt;aliyun ak\u0026gt; --from-literal=ALICLOUD_SECRET_KEY=\u0026lt;aliyun sk\u0026gt; --from-literal=ALICLOUD_REGION_ID=cn-beijing -n traefik-v2 åˆ›å»ºå®Œæˆåå°†è¿™ä¸ª Secret é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®åˆ° Traefik çš„åº”ç”¨ä¸­ï¼Œè¿˜æœ‰ä¸€ä¸ªå€¼å¾—æ³¨æ„çš„æ˜¯éªŒè¯é€šè¿‡çš„è¯ä¹¦æˆ‘ä»¬è¿™é‡Œå­˜åˆ° /data/acme.json æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬ä¸€å®šè¦å°†è¿™ä¸ªæ–‡ä»¶æŒä¹…åŒ–ï¼Œå¦åˆ™æ¯æ¬¡ Traefik é‡å»ºåå°±éœ€è¦é‡æ–°è®¤è¯ï¼Œè€Œ Letâ€™s Encrypt æœ¬èº«æ ¡éªŒæ¬¡æ•°æ˜¯æœ‰é™åˆ¶çš„ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨ values ä¸­é‡æ–°å¼€å¯äº†æ•°æ®æŒä¹…åŒ–ï¼Œä¸è¿‡å¼€å¯è¿‡åéœ€è¦æˆ‘ä»¬æä¾›ä¸€ä¸ªå¯ç”¨çš„ PV å­˜å‚¨ï¼Œç”±äºæˆ‘ä»¬å°† Traefik å›ºå®šåˆ° master1 èŠ‚ç‚¹ä¸Šçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª hostpath ç±»å‹çš„ PV apiVersion: v1 kind: PersistentVolume metadata: name: traefik spec: accessModes: - ReadWriteOnce capacity: storage: 128Mi hostPath: path: /data/k8s/traefik æ›´æ–°IngressRoute apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-https spec: entryPoints: - websecure routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: vue-demo port: 80 tls: certResolver: ali domains: - main: \u0026#34;*.qingyang.com\u0026#34; åªéœ€è¦å°† tls éƒ¨åˆ†æ”¹æˆæˆ‘ä»¬å®šä¹‰çš„ ali è¿™ä¸ªè¯ä¹¦è§£æå™¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ç”Ÿæˆä¸€ä¸ªé€šé…ç¬¦çš„åŸŸåè¯ä¹¦çš„è¯å¯ä»¥å®šä¹‰ domains å‚æ•°æ¥æŒ‡å®šï¼Œç„¶åæ›´æ–° IngressRoute å¯¹è±¡ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘ä»¬å†å»ç”¨ HTTPS è®¿é—®æˆ‘ä»¬çš„åº”ç”¨ï¼ˆå½“ç„¶éœ€è¦å°†åŸŸååœ¨é˜¿é‡Œäº‘ DNS ä¸Šåšè§£æï¼‰\nä¸­é—´ä»¶ è¿æ¥åˆ°è·¯ç”±å™¨çš„ä¸­é—´ä»¶æ˜¯ä¸€ç§åœ¨å°†è¯·æ±‚å‘é€åˆ°æ‚¨çš„æœåŠ¡ä¹‹å‰ï¼ˆæˆ–åœ¨å°†æœåŠ¡çš„ç­”æ¡ˆå‘é€åˆ°å®¢æˆ·ç«¯ä¹‹å‰ï¼‰è°ƒæ•´è¯·æ±‚çš„æ–¹æ³•ã€‚\nTraefik ä¸­æœ‰å‡ ä¸ªå¯ç”¨çš„ä¸­é—´ä»¶ï¼Œæœ‰çš„å¯ä»¥ä¿®æ”¹è¯·æ±‚ï¼Œheadersï¼Œæœ‰çš„è´Ÿè´£é‡å®šå‘ï¼Œæœ‰çš„æ·»åŠ è®¤è¯ç­‰ç­‰ã€‚\nä½¿ç”¨ç›¸åŒåè®®çš„ä¸­é—´ä»¶å¯ä»¥ç»„åˆæˆé“¾ä»¥é€‚åº”å„ç§åœºæ™¯ã€‚\nHTTPä¸­é—´ä»¶åˆ—è¡¨ TCPä¸­é—´ä»¶åˆ—è¡¨ å¼ºåˆ¶è·³è½¬Https Traefik ä¸­ä¹Ÿæ˜¯å¯ä»¥é…ç½®å¼ºåˆ¶è·³è½¬çš„ï¼Œåªæ˜¯è¿™ä¸ªåŠŸèƒ½ç°åœ¨æ˜¯é€šè¿‡ä¸­é—´ä»¶æ¥æä¾›çš„äº†ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨ redirectScheme ä¸­é—´ä»¶æ¥åˆ›å»ºæä¾›å¼ºåˆ¶è·³è½¬æœåŠ¡\n# Redirect to https apiVersion: traefik.containo.us/v1alpha1 kind: Middleware # åˆ›å»ºä¸€ä¸ªä¸­é—´ä»¶ metadata: name: test-redirectscheme spec: redirectScheme: scheme: https permanent: true --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-http namespace: default spec: entryPoints: - web routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: vue-demo port: 80 middlewares: - name: test-redirectscheme # æŒ‡å®šæ·»åŠ ä¸­é—´ä»¶çš„åç§° å¦‚æœä½ æœ‰å…·ä½“éœ€æ±‚çš„è¯è¯·å‰å¾€å®˜ç½‘æ–‡æ¡£æŸ¥çœ‹æ›´å¤šä¸­é—´ä»¶çš„é€‚ç”¨æ–¹æ³•ã€‚\nTraefik Pilot è™½ç„¶ Traefik å·²ç»é»˜è®¤å®ç°äº†å¾ˆå¤šä¸­é—´ä»¶ï¼Œå¯ä»¥æ»¡è¶³å¤§éƒ¨åˆ†æˆ‘ä»¬æ—¥å¸¸çš„éœ€æ±‚ï¼Œä½†æ˜¯åœ¨å®é™…å·¥ä½œä¸­ï¼Œç”¨æˆ·ä»ç„¶è¿˜æ˜¯æœ‰è‡ªå®šä¹‰ä¸­é—´ä»¶çš„éœ€æ±‚ï¼Œè¿™å°± Traefik Pilot çš„åŠŸèƒ½äº†ã€‚\nTraefik Pilot æ˜¯ä¸€ä¸ª SaaS å¹³å°ï¼Œå’Œ Traefik è¿›è¡Œé“¾æ¥æ¥æ‰©å±•å…¶åŠŸèƒ½ï¼Œå®ƒæä¾›äº†å¾ˆå¤šåŠŸèƒ½ï¼Œé€šè¿‡ä¸€ä¸ªå…¨å±€æ§åˆ¶é¢æ¿å’Œ Dashboard æ¥å¢å¼ºå¯¹ Traefik çš„è§‚æµ‹å’Œæ§åˆ¶ï¼š\nTraefik ä»£ç†å’Œä»£ç†ç»„çš„ç½‘ç»œæ´»åŠ¨çš„æŒ‡æ ‡ æœåŠ¡å¥åº·é—®é¢˜å’Œå®‰å…¨æ¼æ´è­¦æŠ¥ æ‰©å±• Traefik åŠŸèƒ½çš„æ’ä»¶ åœ¨ Traefik å¯ä»¥ä½¿ç”¨ Traefik Pilot çš„åŠŸèƒ½ä¹‹å‰ï¼Œå¿…é¡»å…ˆè¿æ¥å®ƒä»¬ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹ Traefik çš„é™æ€é…ç½®è¿›è¡Œå°‘é‡æ›´æ”¹å³å¯ã€‚\næ³¨æ„: Traefik ä»£ç†å¿…é¡»è¦èƒ½è®¿é—®äº’è”ç½‘æ‰èƒ½è¿æ¥åˆ° Traefik Pilotï¼Œé€šè¿‡ HTTPS åœ¨ 443 ç«¯å£ä¸Šå»ºç«‹è¿æ¥ã€‚ è¿™ä¸ªæˆ‘å°±ä¸æ¼”ç¤ºäº†ï¼Œæˆ‘çš„è™šæ‹Ÿæœºæœ¨æœ‰å¤–ç½‘ã€‚\nç°åº¦å‘å¸ƒ è·ŸIngress-nginxæ˜¯ä¸€æ ·çš„å°±ä¸å¤šä»‹ç»ç°åº¦å‘å¸ƒæ˜¯å•¥äº†\nåŸºäºæƒé‡çš„è½®è¯¢ è¿™æ¬¡ä½¿ç”¨çš„Deploymentå’Œä¸Šæ¬¡Ingress-nginxä¸€æ ·ï¼Œå¤§å®¶å»Ingressé‚£ç¯‡æ–‡ç« å»æ‰¾ä¸€ä¸‹å’¯\næˆ‘ä»¬å¯ä»¥ç›´æ¥åˆ©ç”¨TraefikServiceè¿™ä¸ªå¯¹è±¡æ¥é…ç½®åŸºäºæƒé‡çš„è½®è¯¢\napiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: auy-cat-wrr spec: weighted: services: - name: production weight: 3 # å®šä¹‰æƒé‡ port: 80 kind: Service - name: canary-service weight: 1 port: 80 ç„¶åä¿®æ”¹æˆ‘ä»¬IngressRouteä¸­ä½¿ç”¨çš„TraefikService\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-http namespace: default spec: entryPoints: - web routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: auy-cat-wrr kind: TraefikService # ä½¿ç”¨å£°æ˜çš„ TraefikService æœåŠ¡ï¼Œè€Œä¸æ˜¯ K8S çš„ Service æµé‡å¤åˆ¶ é™¤äº†ç°åº¦å‘å¸ƒä¹‹å¤–ï¼ŒTraefikè¿˜å¼•å…¥äº†æµé‡é•œåƒæœåŠ¡ï¼Œæ˜¯ä¸€ç§å¯ä»¥å°†æµå…¥æµé‡å¤åˆ¶å¹¶åŒæ—¶å°†å…¶å‘é€ç»™å…¶ä»–æœåŠ¡çš„æ–¹æ³•ï¼Œé•œåƒæœåŠ¡å¯ä»¥è·å¾—ç»™å®šç™¾åˆ†æ¯”çš„è¯·æ±‚åŒæ—¶ä¹Ÿä¼šå¿½ç•¥è¿™éƒ¨åˆ†è¯·æ±‚çš„å“åº”ã€‚\nå‡è®¾æˆ‘ä»¬åˆšåˆšçš„productionä¸ºçº¿ä¸ŠæœåŠ¡canaryä¸ºé¢„è§ˆæœåŠ¡,ç°åœ¨å¸Œæœ›è¯·æ±‚productionçš„æµé‡åŒæ—¶å¤åˆ¶ä¸€ä»½ä¹Ÿè¯·æ±‚åˆ°canaryç‰ˆæœ¬ä¸­\napiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: mirror-replication spec: mirroring: name: production port: 80 mirrors: - name: canary-service percent: 50 port: 80 --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-qingyang-http namespace: default spec: entryPoints: - web routes: - match: Host(`traefik.qingyang.com`) kind: Rule services: - name: mirror-replication kind: TraefikService # ä½¿ç”¨å£°æ˜çš„ TraefikService æœåŠ¡ï¼Œè€Œä¸æ˜¯ K8S çš„ Service ä»£ç†Tcp/Udp å¦å¤– Traefik2.X å·²ç»æ”¯æŒäº† TCP æœåŠ¡çš„ï¼Œä¸‹é¢æˆ‘ä»¬ä»¥ mongo ä¸ºä¾‹æ¥äº†è§£ä¸‹ Traefik æ˜¯å¦‚ä½•æ”¯æŒ TCP æœåŠ¡å¾—ã€‚\napiVersion: apps/v1 kind: Deployment metadata: name: mongo-traefik labels: app: mongo-traefik spec: selector: matchLabels: app: mongo-traefik template: metadata: labels: app: mongo-traefik spec: containers: - name: mongo image: mongo ports: - containerPort: 27017 --- apiVersion: v1 kind: Service metadata: name: mongo-traefik spec: selector: app: mongo-traefik ports: - port: 27017 æ–°å¢Traefikçš„å…¥å£ç‚¹ ports: web: port: 8000 hostPort: 80 websecure: port: 8443 hostPort: 443 mongo: port: 27017 hostPort: 27017 è¿™é‡Œç»™å…¥å£ç‚¹æ·»åŠ  hostPort æ˜¯ä¸ºäº†èƒ½å¤Ÿé€šè¿‡èŠ‚ç‚¹çš„ç«¯å£è®¿é—®åˆ°æœåŠ¡ï¼Œå…³äº entryPoints å…¥å£ç‚¹çš„æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹æ–‡æ¡£ entrypoints äº†è§£æ›´å¤šä¿¡æ¯ã€‚\nhelm upgrade --install traefik ./traefik -f ./traefik/values.yaml -n traefik-v2 ç”±äº Traefik ä¸­ä½¿ç”¨ TCP è·¯ç”±é…ç½®éœ€è¦ SNIï¼Œè€Œ SNI åˆæ˜¯ä¾èµ– TLS çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦é…ç½®è¯ä¹¦æ‰è¡Œï¼Œå¦‚æœæ²¡æœ‰è¯ä¹¦çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€šé…ç¬¦ * è¿›è¡Œé…ç½®ï¼Œæˆ‘ä»¬è¿™é‡Œåˆ›å»ºä¸€ä¸ª IngressRouteTCP ç±»å‹çš„ CRD apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: mongo-traefik-tcp spec: entryPoints: - mongo routes: - match: HostSNI(`*`) services: - name: mongo-traefik port: 27017 æˆ‘è¿™é‡Œæ²¡æœ‰mongoçš„å®¢æˆ·ç«¯,ç›´æ¥æ ¡éªŒä¸€ä¸‹ç«¯å£å°±è¡Œäº†\ntelnet traefik.qingyang.com 27017 ä½¿ç”¨ç‰¹å®šçš„åŸŸåè¿›è¡Œä»£ç†è®¿é—® å‡è®¾æˆ‘ç°åœ¨æœ‰ä¸€ä¸ªmysqlæœåŠ¡ï¼Œæƒ³é€šè¿‡traefik.mysql.prodè¿›è¡Œè¿æ¥è®¿é—®.\næˆ‘ä»¬åœ¨åŠ ä¸€ä¸ªproxy-mysqlçš„entryPoints\nports: web: port: 8000 hostPort: 80 websecure: port: 8443 hostPort: 443 mongo: port: 27017 hostPort: 27017 proxy-mysql: port: 3306 hostPort: 3306 ç”Ÿæˆä¸€ä¸ªtraefik.mysql.prodçš„è‡ªç­¾åè¯ä¹¦ è£…ä¸ªGolangdnf install -y go git clone https://github.com/jsha/minica.git cd minica go build [root@Online-Beijing-master1 minica]# ./minica --domains \u0026#39;traefik.mysql.prod\u0026#39; [root@Online-Beijing-master1 minica]# cd traefik.mysql.prod/ ç”Ÿæˆsecret,è¯·ç¡®ä¿ä½ å¤„äºå½“å‰çš„cert.keyå’Œkey.pemçš„ç›®å½•ä¸‹ kubectl create secret tls tcp-demo-mysql --cert=cert.pem --key=key.pem åˆ›å»ºIngressRouteTCPå¯¹è±¡ apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: tcp-inner-mysql namespace: default spec: entryPoints: - proxy-mysql routes: - match: HostSNI(`traefik.mysql.prod`) services: - name: env-prod-mysql-svc port: 3306 tls: # ç»‘å®šTls secretName: tcp-demo-mysql ä»£ç†ä¸€ä¸ªUdpæœåŠ¡ éƒ¨ç½²ä¸€ä¸ªUDPæœåŠ¡ kind: Deployment apiVersion: apps/v1 metadata: name: whoami labels: app: whoami spec: replicas: 1 selector: matchLabels: app: whoami template: metadata: labels: app: whoami spec: containers: - name: whoami image: containous/whoamiudp ports: - name: web containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: whoamiudp spec: ports: - protocol: UDP name: udp port: 8080 selector: app: whoami åœ¨Traefikå½“ä¸­æ·»åŠ UDPçš„å…¥å£ç‚¹ï¼Œè€æ ·å­ä¿®æ”¹values.yaml udpend: port: 18080 hostPort: 18080 protocol: UDP UDP çš„å…¥å£ç‚¹å¢åŠ æˆåŠŸåï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª IngressRouteUDP ç±»å‹çš„èµ„æºå¯¹è±¡ï¼Œç”¨æ¥ä»£ç† UDP è¯·æ±‚ï¼š apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteUDP metadata: name: whoamiudp spec: entryPoints: - udpend routes: - services: - name: whoamiudp port: 80 ","date":"2023-04-07T00:00:00Z","image":"https://doc.traefik.io/traefik/assets/img/traefik-architecture.png","permalink":"http://localhost:1313/kubernetes/traefik/","title":"TraekfikåŸºç¡€ä½¿ç”¨æŒ‡å—"},{"content":"æœ¬åœ°å­˜å‚¨ å‰é¢æˆ‘ä»¬æœ‰é€šè¿‡ hostPath æˆ–è€… emptyDir çš„æ–¹å¼æ¥æŒä¹…åŒ–æˆ‘ä»¬çš„æ•°æ®ï¼Œä½†æ˜¯æ˜¾ç„¶æˆ‘ä»¬è¿˜éœ€è¦æ›´åŠ å¯é çš„å­˜å‚¨æ¥ä¿å­˜åº”ç”¨çš„æŒä¹…åŒ–æ•°æ®ï¼Œè¿™æ ·å®¹å™¨åœ¨é‡å»ºåï¼Œä¾ç„¶å¯ä»¥ä½¿ç”¨ä¹‹å‰çš„æ•°æ®ã€‚ä½†æ˜¯å­˜å‚¨èµ„æºå’Œ CPU èµ„æºä»¥åŠå†…å­˜èµ„æºæœ‰å¾ˆå¤§ä¸åŒï¼Œä¸ºäº†å±è”½åº•å±‚çš„æŠ€æœ¯å®ç°ç»†èŠ‚ï¼Œè®©ç”¨æˆ·æ›´åŠ æ–¹ä¾¿çš„ä½¿ç”¨ï¼ŒKubernetes ä¾¿å¼•å…¥äº† PV å’Œ PVC ä¸¤ä¸ªé‡è¦çš„èµ„æºå¯¹è±¡æ¥å®ç°å¯¹å­˜å‚¨çš„ç®¡ç†ã€‚\nPersistentVolume PV çš„å…¨ç§°æ˜¯ï¼šPersistentVolumeï¼ˆæŒä¹…åŒ–å·ï¼‰ï¼Œæ˜¯å¯¹åº•å±‚å…±äº«å­˜å‚¨çš„ä¸€ç§æŠ½è±¡ï¼ŒPV ç”±ç®¡ç†å‘˜è¿›è¡Œåˆ›å»ºå’Œé…ç½®ï¼Œå®ƒå’Œå…·ä½“çš„åº•å±‚çš„å…±äº«å­˜å‚¨æŠ€æœ¯çš„å®ç°æ–¹å¼æœ‰å…³ï¼Œæ¯”å¦‚ Cephã€GlusterFSã€NFSã€hostPath ç­‰ï¼Œéƒ½æ˜¯é€šè¿‡æ’ä»¶æœºåˆ¶å®Œæˆä¸å…±äº«å­˜å‚¨çš„å¯¹æ¥ã€‚\nPersistentVolumeClaim PVC çš„å…¨ç§°æ˜¯ï¼šPersistentVolumeClaimï¼ˆæŒä¹…åŒ–å·å£°æ˜ï¼‰ï¼ŒPVC æ˜¯ç”¨æˆ·å­˜å‚¨çš„ä¸€ç§å£°æ˜ï¼ŒPVC å’Œ Pod æ¯”è¾ƒç±»ä¼¼ï¼ŒPod æ¶ˆè€—çš„æ˜¯èŠ‚ç‚¹ï¼ŒPVC æ¶ˆè€—çš„æ˜¯ PV èµ„æºï¼ŒPod å¯ä»¥è¯·æ±‚ CPU å’Œå†…å­˜ï¼Œè€Œ PVC å¯ä»¥è¯·æ±‚ç‰¹å®šçš„å­˜å‚¨ç©ºé—´å’Œè®¿é—®æ¨¡å¼ã€‚å¯¹äºçœŸæ­£ä½¿ç”¨å­˜å‚¨çš„ç”¨æˆ·ä¸éœ€è¦å…³å¿ƒåº•å±‚çš„å­˜å‚¨å®ç°ç»†èŠ‚ï¼Œåªéœ€è¦ç›´æ¥ä½¿ç”¨ PVC å³å¯ã€‚\nä½†æ˜¯é€šè¿‡ PVC è¯·æ±‚åˆ°ä¸€å®šçš„å­˜å‚¨ç©ºé—´ä¹Ÿå¾ˆæœ‰å¯èƒ½ä¸è¶³ä»¥æ»¡è¶³åº”ç”¨å¯¹äºå­˜å‚¨è®¾å¤‡çš„å„ç§éœ€æ±‚ï¼Œè€Œä¸”ä¸åŒçš„åº”ç”¨ç¨‹åºå¯¹äºå­˜å‚¨æ€§èƒ½çš„è¦æ±‚å¯èƒ½ä¹Ÿä¸å°½ç›¸åŒï¼Œæ¯”å¦‚è¯»å†™é€Ÿåº¦ã€å¹¶å‘æ€§èƒ½ç­‰ï¼Œä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒKubernetes åˆä¸ºæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„èµ„æºå¯¹è±¡ï¼šStorageClassï¼Œé€šè¿‡ StorageClass çš„å®šä¹‰ï¼Œç®¡ç†å‘˜å¯ä»¥å°†å­˜å‚¨èµ„æºå®šä¹‰ä¸ºæŸç§ç±»å‹çš„èµ„æºï¼Œæ¯”å¦‚å¿«é€Ÿå­˜å‚¨ã€æ…¢é€Ÿå­˜å‚¨ç­‰ï¼Œç”¨æˆ·æ ¹æ® StorageClass çš„æè¿°å°±å¯ä»¥éå¸¸ç›´è§‚çš„çŸ¥é“å„ç§å­˜å‚¨èµ„æºçš„å…·ä½“ç‰¹æ€§äº†ï¼Œè¿™æ ·å°±å¯ä»¥æ ¹æ®åº”ç”¨çš„ç‰¹æ€§å»ç”³è¯·åˆé€‚çš„å­˜å‚¨èµ„æºäº†ï¼Œæ­¤å¤– StorageClass è¿˜å¯ä»¥ä¸ºæˆ‘ä»¬è‡ªåŠ¨ç”Ÿæˆ PVï¼Œå…å»äº†æ¯æ¬¡æ‰‹åŠ¨åˆ›å»ºçš„éº»çƒ¦ã€‚\nHostPath æˆ‘ä»¬ä¸Šé¢æåˆ°äº† PV æ˜¯å¯¹åº•å±‚å­˜å‚¨æŠ€æœ¯çš„ä¸€ç§æŠ½è±¡ï¼ŒPV ä¸€èˆ¬éƒ½æ˜¯ç”±ç®¡ç†å‘˜æ¥åˆ›å»ºå’Œé…ç½®çš„ï¼Œæˆ‘ä»¬é¦–å…ˆæ¥åˆ›å»ºä¸€ä¸ª hostPath ç±»å‹çš„ PersistentVolumeã€‚Kubernetes æ”¯æŒ hostPath ç±»å‹çš„ PersistentVolume ä½¿ç”¨èŠ‚ç‚¹ä¸Šçš„æ–‡ä»¶æˆ–ç›®å½•æ¥æ¨¡æ‹Ÿé™„å¸¦ç½‘ç»œçš„å­˜å‚¨ï¼Œä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯åœ¨ç”Ÿäº§é›†ç¾¤ä¸­ï¼Œæˆ‘ä»¬ä¸ä¼šä½¿ç”¨ hostPathï¼Œé›†ç¾¤ç®¡ç†å‘˜ä¼šæä¾›ç½‘ç»œå­˜å‚¨èµ„æºï¼Œæ¯”å¦‚ NFS å…±äº«å·æˆ– Ceph å­˜å‚¨å·ï¼Œé›†ç¾¤ç®¡ç†å‘˜è¿˜å¯ä»¥ä½¿ç”¨ StorageClasses æ¥è®¾ç½®åŠ¨æ€æä¾›å­˜å‚¨ã€‚å› ä¸º Pod å¹¶ä¸æ˜¯å§‹ç»ˆå›ºå®šåœ¨æŸä¸ªèŠ‚ç‚¹ä¸Šé¢çš„ï¼Œæ‰€ä»¥è¦ä½¿ç”¨ hostPath çš„è¯æˆ‘ä»¬å°±éœ€è¦å°† Pod å›ºå®šåœ¨æŸä¸ªèŠ‚ç‚¹ä¸Šï¼Œè¿™æ ·æ˜¾ç„¶å°±å¤§å¤§é™ä½äº†åº”ç”¨çš„å®¹é”™æ€§ã€‚\nå½“ç„¶äº†ï¼Œç”Ÿäº§ç¯å¢ƒä¸­ç”¨çš„è¿˜æ˜¯ç›¸å¯¹è¾ƒå°‘å› ä¸ºæœ‰è¾ƒå°‘çš„éœ€æ±‚éœ€è¦å°†Podæ¥å›ºå®šåˆ°æŸäº›èŠ‚ç‚¹ä¸Šã€‚\nåˆ›å»ºPersistentVolume å‡è®¾æˆ‘ä»¬ç°åœ¨åœ¨èŠ‚ç‚¹1ä¸Šæ–°å»ºä¸€ä¸ª/data/hostPath/index.html [root@Online-Beijing-node1 ~]# echo \u0026#34;Hello This is new hostPath message.\u0026#34; \u0026gt;\u0026gt; /data/hostPath/index.html æ¥ä¸‹æ¥åˆ›å»ºä¸€ä¸ªPvå¯¹è±¡ apiVersion: v1 kind: PersistentVolume metadata: name: demo-hostpath labels: type: local spec: capacity: # å®šä¹‰è¯¥Pvçš„å®¹é‡ä¸º10Gb storage: 10Gi accessModes: # å®šä¹‰è¯¥Pvçš„è®¿é—®æ¨¡å¼ - ReadWriteOnce hostPath: path: \u0026#34;/data/hostPath\u0026#34; storageClassName: type-ssd-sc Capacityï¼ˆå­˜å‚¨èƒ½åŠ›ï¼‰ï¼šä¸€èˆ¬æ¥è¯´ï¼Œä¸€ä¸ª PV å¯¹è±¡éƒ½è¦æŒ‡å®šä¸€ä¸ªå­˜å‚¨èƒ½åŠ›ï¼Œé€šè¿‡ PV çš„ capacity å±æ€§æ¥è®¾ç½®çš„ï¼Œç›®å‰åªæ”¯æŒå­˜å‚¨ç©ºé—´çš„è®¾ç½®ï¼Œå°±æ˜¯æˆ‘ä»¬è¿™é‡Œçš„ storage=10Giï¼Œä¸è¿‡æœªæ¥å¯èƒ½ä¼šåŠ å…¥ IOPSã€ååé‡ç­‰æŒ‡æ ‡çš„é…ç½®ã€‚ AccessModesï¼ˆè®¿é—®æ¨¡å¼ï¼‰ï¼šç”¨æ¥å¯¹ PV è¿›è¡Œè®¿é—®æ¨¡å¼çš„è®¾ç½®ï¼Œç”¨äºæè¿°ç”¨æˆ·åº”ç”¨å¯¹å­˜å‚¨èµ„æºçš„è®¿é—®æƒé™ï¼Œè®¿é—®æƒé™åŒ…æ‹¬ä¸‹é¢å‡ ç§æ–¹å¼ï¼š ReadWriteOnceï¼ˆRWOï¼‰ï¼šè¯»å†™æƒé™ï¼Œä½†æ˜¯åªèƒ½è¢«å•ä¸ªèŠ‚ç‚¹æŒ‚è½½ ReadOnlyManyï¼ˆROXï¼‰ï¼šåªè¯»æƒé™ï¼Œå¯ä»¥è¢«å¤šä¸ªèŠ‚ç‚¹æŒ‚è½½ ReadWriteManyï¼ˆRWXï¼‰ï¼šè¯»å†™æƒé™ï¼Œå¯ä»¥è¢«å¤šä¸ªèŠ‚ç‚¹æŒ‚è½½ åˆ›å»ºå®ŒæˆåæŸ¥çœ‹ PersistentVolume çš„ä¿¡æ¯ï¼Œè¾“å‡ºç»“æœæ˜¾ç¤ºè¯¥ PersistentVolume çš„çŠ¶æ€ï¼ˆSTATUSï¼‰ ä¸º Availableã€‚ è¿™æ„å‘³ç€å®ƒè¿˜æ²¡æœ‰è¢«ç»‘å®šç»™ PersistentVolumeClaim\n[root@Online-Beijing-master1 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE demo-hostpath 10Gi RWO Retain Available type-ssd-sc 13m å…¶ä¸­æœ‰ä¸€é¡¹ RECLAIM POLICY çš„é…ç½®ï¼ŒåŒæ ·æˆ‘ä»¬å¯ä»¥é€šè¿‡ PV çš„ persistentVolumeReclaimPolicyï¼ˆå›æ”¶ç­–ç•¥ï¼‰å±æ€§æ¥è¿›è¡Œé…ç½®ï¼Œç›®å‰ PV æ”¯æŒçš„ç­–ç•¥æœ‰ä¸‰ç§ï¼š\nRetainï¼ˆä¿ç•™ï¼‰ï¼šå›æ”¶ç­–ç•¥ Retain ä½¿å¾—ç”¨æˆ·å¯ä»¥æ‰‹åŠ¨å›æ”¶èµ„æºã€‚å½“ PersistentVolumeClaim å¯¹è±¡è¢«åˆ é™¤æ—¶ï¼ŒPersistentVolume å·ä»ç„¶å­˜åœ¨ï¼Œå¯¹åº”çš„æ•°æ®å·è¢«è§†ä¸º\u0026quot;å·²é‡Šæ”¾ï¼ˆreleasedï¼‰\u0026quot;ã€‚ ç”±äºå·ä¸Šä»ç„¶å­˜åœ¨è¿™å‰ä¸€ç”³é¢†äººçš„æ•°æ®ï¼Œè¯¥å·è¿˜ä¸èƒ½ç”¨äºå…¶ä»–ç”³é¢†ã€‚ ç®¡ç†å‘˜å¯ä»¥é€šè¿‡ä¸‹é¢çš„æ­¥éª¤æ¥æ‰‹åŠ¨å›æ”¶è¯¥å·ï¼š åˆ é™¤ PersistentVolume å¯¹è±¡ã€‚ä¸ä¹‹ç›¸å…³çš„ã€ä½äºå¤–éƒ¨åŸºç¡€è®¾æ–½ä¸­çš„å­˜å‚¨èµ„äº§ ï¼ˆä¾‹å¦‚ AWS EBSã€GCE PDã€Azure Disk æˆ– Cinder å·ï¼‰åœ¨ PV åˆ é™¤ä¹‹åä»ç„¶å­˜åœ¨ã€‚ æ ¹æ®æƒ…å†µï¼Œæ‰‹åŠ¨æ¸…é™¤æ‰€å…³è”çš„å­˜å‚¨èµ„äº§ä¸Šçš„æ•°æ®ã€‚ æ‰‹åŠ¨åˆ é™¤æ‰€å…³è”çš„å­˜å‚¨èµ„äº§ã€‚ Recycleï¼ˆå›æ”¶ï¼‰ï¼šå›æ”¶ç­–ç•¥ Recycle å·²è¢«åºŸå¼ƒã€‚å–è€Œä»£ä¹‹çš„å»ºè®®æ–¹æ¡ˆæ˜¯ä½¿ç”¨åŠ¨æ€åˆ¶å¤‡ã€‚å¦‚æœä¸‹å±‚çš„å·æ’ä»¶æ”¯æŒï¼Œå›æ”¶ç­–ç•¥ Recycle ä¼šåœ¨å·ä¸Šæ‰§è¡Œä¸€äº›åŸºæœ¬çš„æ“¦é™¤ ï¼ˆrm -rf /thevolume/*ï¼‰æ“ä½œï¼Œä¹‹åå…è®¸è¯¥å·ç”¨äºæ–°çš„ PVC ç”³é¢†ã€‚ Deleteï¼ˆåˆ é™¤ï¼‰ï¼šå¯¹äºæ”¯æŒ Delete å›æ”¶ç­–ç•¥çš„å·æ’ä»¶ï¼Œåˆ é™¤åŠ¨ä½œä¼šå°† PersistentVolume å¯¹è±¡ä» Kubernetes ä¸­ç§»é™¤ï¼ŒåŒæ—¶ä¹Ÿä¼šä»å¤–éƒ¨åŸºç¡€è®¾æ–½ï¼ˆå¦‚ AWS EBSã€GCE PDã€Azure Disk æˆ– Cinder å·ï¼‰ä¸­ç§»é™¤æ‰€å…³è”çš„å­˜å‚¨èµ„äº§ã€‚ ç›®å‰ï¼Œä»… NFS å’Œ HostPath æ”¯æŒå›æ”¶ï¼ˆRecycleï¼‰ã€‚ AWS EBSã€GCE PDã€Azure Disk å’Œ Cinder å·éƒ½æ”¯æŒåˆ é™¤ï¼ˆDeleteï¼‰ã€‚\nä¸è¿‡éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç›®å‰åªæœ‰ NFS å’Œ HostPath ä¸¤ç§ç±»å‹æ”¯æŒå›æ”¶ç­–ç•¥ï¼Œå½“ç„¶ä¸€èˆ¬æ¥è¯´è¿˜æ˜¯è®¾ç½®ä¸º Retain è¿™ç§ç­–ç•¥ä¿é™©ä¸€ç‚¹ã€‚\nå…³äº PV çš„çŠ¶æ€ï¼Œå®é™…ä¸Šæè¿°çš„æ˜¯ PV çš„ç”Ÿå‘½å‘¨æœŸçš„æŸä¸ªé˜¶æ®µï¼Œä¸€ä¸ª PV çš„ç”Ÿå‘½å‘¨æœŸä¸­ï¼Œå¯èƒ½ä¼šå¤„äº4ç§ä¸åŒçš„é˜¶æ®µï¼š\nAvailableï¼ˆå¯ç”¨ï¼‰ï¼šè¡¨ç¤ºå¯ç”¨çŠ¶æ€ï¼Œè¿˜æœªè¢«ä»»ä½• PVC ç»‘å®š Boundï¼ˆå·²ç»‘å®šï¼‰ï¼šè¡¨ç¤º PVC å·²ç»è¢« PVC ç»‘å®š Releasedï¼ˆå·²é‡Šæ”¾ï¼‰ï¼šPVC è¢«åˆ é™¤ï¼Œä½†æ˜¯èµ„æºè¿˜æœªè¢«é›†ç¾¤é‡æ–°å£°æ˜ Failedï¼ˆå¤±è´¥ï¼‰ï¼š è¡¨ç¤ºè¯¥ PV çš„è‡ªåŠ¨å›æ”¶å¤±è´¥ åˆ›å»ºPersistentVolumeClaim å¦‚æœæˆ‘ä»¬éœ€è¦ä½¿ç”¨è¿™ä¸ª PV çš„è¯ï¼Œå°±éœ€è¦åˆ›å»ºä¸€ä¸ªå¯¹åº”çš„ PVC æ¥å’Œä»–è¿›è¡Œç»‘å®šäº†ï¼Œå°±ç±»ä¼¼äºæˆ‘ä»¬çš„æœåŠ¡æ˜¯é€šè¿‡ Pod æ¥è¿è¡Œçš„ï¼Œè€Œä¸æ˜¯ Nodeï¼Œåªæ˜¯ Pod è·‘åœ¨ Node ä¸Šè€Œå·²ã€‚\nè®©æˆ‘ä»¬ç”³è¯·ä¸€ä¸ªä½¿ç”¨3Gç©ºé—´çš„PersistentVolumeClaim\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi storageClassName: type-ssd-sc åˆ›å»º PersistentVolumeClaim ä¹‹åï¼ŒKubernetes æ§åˆ¶å¹³é¢å°†æŸ¥æ‰¾æ»¡è¶³ç”³é¢†è¦æ±‚çš„ PersistentVolumeã€‚ å¦‚æœæ§åˆ¶å¹³é¢æ‰¾åˆ°å…·æœ‰ç›¸åŒ StorageClass çš„é€‚å½“çš„ PersistentVolumeï¼Œ åˆ™å°† PersistentVolumeClaim ç»‘å®šåˆ°è¯¥ PersistentVolume ä¸Šã€‚æ‰€ä»¥å†æ¬¡kubectl get pvçš„PersistentVolumeçŠ¶æ€åº”è¯¥å±äºBoundçŠ¶æ€ã€‚\n[root@Online-Beijing-master1 yaml]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE demo-hostpath 10Gi RWO Retain Bound default/task-pv-claim type-ssd-sc 47m [root@Online-Beijing-master1 yaml]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound demo-hostpath 10Gi RWO type-ssd-sc 18m å¯ä»¥çœ‹åˆ°å·²ç»ç»‘å®šåˆ°äº†ä¸€ä¸ªVolumeå«åšdemo-hostpathçš„PersistentVolume\néœ€è¦æ³¨æ„çš„æ˜¯ç›®å‰PersistentVolumeå’ŒPersistentVolumeClaimä¹‹é—´æ˜¯ä¸€å¯¹ä¸€ç»‘å®šçš„å…³ç³»ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªPersistentVolumeåªèƒ½è¢«ä¸€ä¸ªPersistentVolumeClaimç»‘å®šã€‚\nåˆ›å»ºä¸€ä¸ªDeployment åˆ›å»ºä¸€ä¸ªdeploymentç„¶åç»‘å®šPersistentVolumeClaimç´§æ¥ç€å›ºå®šèŠ‚ç‚¹åˆ°online-beijing-node1\n--- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s.kuboard.cn/name: task-nginx-demo name: task-nginx-demo namespace: default spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s.kuboard.cn/name: task-nginx-demo strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s.kuboard.cn/name: task-nginx-demo nodeSelector: kubernetes.io/hostname: online-beijing-node1 spec: containers: - image: \u0026#39;nginx:latest\u0026#39; imagePullPolicy: Always name: task-nginx-demo ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: task-hostpath-volume dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: task-hostpath-volume persistentVolumeClaim: claimName: task-pv-claim å½“è¿™ä¸ªdeploymentåˆ›å»ºå®Œæˆä»¥åæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡è®¿é—®serviceæµ‹è¯•ä¸€ä¸‹.\næ­£å¸¸æƒ…å†µä¸‹ä½ å¯ä»¥çœ‹åˆ°Hello This is new hostPath message.è¿™æ¡ä¿¡æ¯\n[root@Online-Beijing-master1 yaml]# curl -v 10.10.56.102 * Rebuilt URL to: 10.10.56.102/ * Trying 10.10.56.102... * TCP_NODELAY set * Connected to 10.10.56.102 (10.10.56.102) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: 10.10.56.102 \u0026gt; User-Agent: curl/7.61.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.23.3 \u0026lt; Date: Wed, 22 Mar 2023 09:54:49 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 36 \u0026lt; Last-Modified: Wed, 22 Mar 2023 07:53:15 GMT \u0026lt; Connection: keep-alive \u0026lt; ETag: \u0026#34;641ab3eb-24\u0026#34; \u0026lt; Accept-Ranges: bytes Hello This is new hostPath message. è¿™ä¸ªå°±æ˜¯æˆ‘ä»¬ä¸€ä¸ªå¾ˆç®€å•çš„åŸºäºhostPathæ¥æŒä¹…åŒ–æ•°æ®ä½¿ç”¨PersistentVolumeå’ŒPersistentVolumeClaimç®€å•æ•™å­¦ã€‚\nLocal PersistentVolume ä¸Šé¢æˆ‘ä»¬åˆ›å»ºäº†åç«¯æ˜¯ hostPath ç±»å‹çš„ PV èµ„æºå¯¹è±¡,é‚£ä¹ˆä¸ªäººè®¤ä¸ºhostPathçš„ç¼ºç‚¹åœ¨äº\nPodä¸èƒ½è¿›è¡Œéšæ—¶éšåœ°çš„èŠ‚ç‚¹æ›´æ¢,å¦‚æœæ›´æ¢åˆ™ä¼šå‡ºç°ä¸¢å¤±æ•°æ®çš„ç°è±¡ã€‚\néœ€è¦æ¯æ¬¡éƒ½æ­é…nodeSelectorè¿›è¡Œä½¿ç”¨ã€‚\nå…¶ä¼˜ç‚¹ä¹Ÿæ˜¯ç›¸å¯¹äºæ¯”è¾ƒæ˜æ˜¾\nå› ä¸ºhostPathä½¿ç”¨çš„æ˜¯æœ¬åœ°ç£ç›˜,å¯ä»¥å……åˆ†çš„åˆ©ç”¨ç£ç›˜çš„è¯»å†™æ€§èƒ½ã€‚ æ‰€ä»¥åœ¨ hostPath çš„åŸºç¡€ä¸Šï¼ŒKubernetes ä¾é  PVã€PVC å®ç°äº†ä¸€ä¸ªæ–°çš„ç‰¹æ€§ï¼Œè¿™ä¸ªç‰¹æ€§çš„åå­—å«ä½œï¼šLocal Persistent Volumeï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬è¯´çš„ Local PVã€‚\nlocal å·åªèƒ½ç”¨ä½œé™æ€åˆ›å»ºçš„æŒä¹…å·ã€‚ä¸æ”¯æŒåŠ¨æ€é…ç½®ã€‚\nç„¶è€Œï¼Œlocal å·ä»ç„¶å–å†³äºåº•å±‚èŠ‚ç‚¹çš„å¯ç”¨æ€§ï¼Œå¹¶ä¸é€‚åˆæ‰€æœ‰åº”ç”¨ç¨‹åºã€‚ å¦‚æœèŠ‚ç‚¹å˜å¾—ä¸å¥åº·ï¼Œé‚£ä¹ˆ local å·ä¹Ÿå°†å˜å¾—ä¸å¯è¢« Pod è®¿é—®ã€‚ä½¿ç”¨å®ƒçš„ Pod å°†ä¸èƒ½è¿è¡Œã€‚ ä½¿ç”¨ local å·çš„åº”ç”¨ç¨‹åºå¿…é¡»èƒ½å¤Ÿå®¹å¿è¿™ç§å¯ç”¨æ€§çš„é™ä½ï¼Œä»¥åŠå› åº•å±‚ç£ç›˜çš„è€ç”¨æ€§ç‰¹å¾è€Œå¸¦æ¥çš„æ½œåœ¨çš„æ•°æ®ä¸¢å¤±é£é™©ã€‚\nå®ƒä¸HostPathæœ‰ä½•ä¸åŒï¼Ÿ ä¸ºäº†æ›´å¥½åœ°ç†è§£æœ¬åœ°æŒä¹…å·çš„ä¼˜åŠ¿ï¼Œå°†å…¶ä¸HostPath å·è¿›è¡Œæ¯”è¾ƒå¾ˆæœ‰ç”¨ã€‚HostPath å·å°†ä¸»æœºèŠ‚ç‚¹æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶æˆ–ç›®å½•æŒ‚è½½åˆ° Pod ä¸­ã€‚ç±»ä¼¼åœ°ï¼ŒLocal Persistent Volume å°†æœ¬åœ°ç£ç›˜æˆ–åˆ†åŒºæŒ‚è½½åˆ° Pod ä¸­\næœ€å¤§çš„åŒºåˆ«æ˜¯ Kubernetes è°ƒåº¦ç¨‹åºäº†è§£æœ¬åœ°æŒä¹…å·å±äºå“ªä¸ªèŠ‚ç‚¹ã€‚å¯¹äº HostPath å·ï¼Œå¼•ç”¨ HostPath å·çš„ pod å¯èƒ½ä¼šè¢«è°ƒåº¦ç¨‹åºç§»åŠ¨åˆ°ä¸åŒçš„èŠ‚ç‚¹ï¼Œä»è€Œå¯¼è‡´æ•°æ®ä¸¢å¤±ã€‚ä½†æ˜¯å¯¹äº Local Persistent Volumesï¼ŒKubernetes è°ƒåº¦å™¨ç¡®ä¿ä½¿ç”¨ Local Persistent Volume çš„ pod æ€»æ˜¯è¢«è°ƒåº¦åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ã€‚\nè™½ç„¶ HostPath å·å¯ä»¥é€šè¿‡ Persistent Volume Claim (PVC) å¼•ç”¨æˆ–ç›´æ¥å†…åµŒåœ¨ pod å®šä¹‰ä¸­ï¼Œä½† Local Persistent Volumes åªèƒ½é€šè¿‡ PVC å¼•ç”¨ã€‚è¿™æä¾›äº†é¢å¤–çš„å®‰å…¨ä¼˜åŠ¿ï¼Œå› ä¸º Persistent Volume å¯¹è±¡ç”±ç®¡ç†å‘˜ç®¡ç†ï¼Œé˜²æ­¢ Pod èƒ½å¤Ÿè®¿é—®ä¸»æœºä¸Šçš„ä»»ä½•è·¯å¾„ã€‚\næ‰€ä»¥ï¼Œä¸€èˆ¬æ¥è¯´ Local PV å¯¹åº”çš„å­˜å‚¨ä»‹è´¨æ˜¯ä¸€å—é¢å¤–æŒ‚è½½åœ¨å®¿ä¸»æœºçš„ç£ç›˜æˆ–è€…å—è®¾å¤‡ã€‚\nåˆ›å»ºä¸€ä¸ªLocalæŒä¹…å·å®ä¾‹ ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ local å·å’Œ nodeAffinity çš„æŒä¹…å·ç¤ºä¾‹ï¼š\napiVersion: v1 kind: PersistentVolume metadata: name: example-local spec: capacity: storage: 20Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/ssd1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - online-beijing-node1 ä½¿ç”¨ local å·æ—¶ï¼Œä½ éœ€è¦è®¾ç½® PersistentVolume å¯¹è±¡çš„ nodeAffinity å­—æ®µã€‚ Kubernetes è°ƒåº¦ï¨¸ä½¿ç”¨ PersistentVolume çš„ nodeAffinity ä¿¡æ¯æ¥å°†ä½¿ç”¨ local å·çš„ Pod è°ƒåº¦åˆ°æ­£ç¡®çš„èŠ‚ç‚¹ã€‚\nå½“ç„¶äº†,è¿™ä¹Ÿå°±æ„å‘³ç€å¦‚æœä½ çš„Podæƒ³ä½¿ç”¨è¿™ä¸ªPVçš„è¯,é‚£ä¹ˆå°±åªèƒ½è¿è¡Œåœ¨online-beijing-node1è¿™ä¸ªèŠ‚ç‚¹ä¸Šã€‚è¿™æ ·ï¼Œè°ƒåº¦å™¨åœ¨è°ƒåº¦ Pod çš„æ—¶å€™ï¼Œå°±èƒ½å¤ŸçŸ¥é“ä¸€ä¸ª PV ä¸èŠ‚ç‚¹çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œåšå‡ºæ­£ç¡®çš„é€‰æ‹©ã€‚\nç»‘å®šPersistentVolumeClaim\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: bound-tasknginx spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local-storage æ¥ä¸‹æ¥åˆ›å»ºä¸€ä¸ªPodæ¥ç»‘å®šè¿™ä¸ªPvc,ç„¶åå¯ä»¥é€šè¿‡è®¿é—®Podçš„IPåœ°å€è¿›è¡ŒéªŒè¯ã€‚\napiVersion: v1 kind: Pod metadata: name: pv-local-pod spec: volumes: - name: example-pv-local persistentVolumeClaim: claimName: bound-tasknginx containers: - name: example-pv-local image: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /usr/share/nginx/html name: example-pv-local [root@Online-Beijing-master1 yaml]# curl -v 10.10.38.225 * Rebuilt URL to: 10.10.38.225/ * Trying 10.10.38.225... * TCP_NODELAY set * Connected to 10.10.38.225 (10.10.38.225) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: 10.10.38.225 \u0026gt; User-Agent: curl/7.61.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.23.3 \u0026lt; Date: Thu, 23 Mar 2023 08:45:18 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 25 \u0026lt; Last-Modified: Thu, 23 Mar 2023 08:43:41 GMT \u0026lt; Connection: keep-alive \u0026lt; ETag: \u0026#34;641c113d-19\u0026#34; \u0026lt; Accept-Ranges: bytes \u0026lt; Date: 2023-03-23 LocalPv * Connection #0 to host 10.10.38.225 left intact å½“ç„¶äº†ä½ ä¹Ÿå¯ä»¥è¿›å…¥åˆ°Podå½“ä¸­æŸ¥çœ‹æ˜¯å¦æˆåŠŸ\n[root@Online-Beijing-master1 yaml]# kubectl exec -it pv-local-pod /bin/bash root@pv-local-pod:/usr/share/nginx/html# cd /usr/share/nginx/html/ root@pv-local-pod:/usr/share/nginx/html# cat index.html Date: 2023-03-23 LocalPv åˆ é™¤é™æ€ç®¡ç†çš„æŒä¹…åŒ–å­˜å‚¨ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸Šé¢æ‰‹åŠ¨åˆ›å»ºPersistentVolumeçš„æ–¹å¼ï¼Œå³é™æ€çš„PersistentVolumeç®¡ç†æ–¹å¼ï¼Œåœ¨åˆ é™¤PersistentVolumeæ—¶éœ€è¦æŒ‰å¦‚ä¸‹æµç¨‹æ‰§è¡Œæ“ä½œã€‚\nåˆ é™¤ä½¿ç”¨è¿™ä¸ªPersistentVolumeçš„ Pod ä»å®¿ä¸»æœºç§»é™¤æœ¬åœ°ç£ç›˜ åˆ é™¤PersistentVolumeClaim åˆ é™¤PersistentVolume ","date":"2023-03-22T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/localstorage/","title":"Kubernetes-æœ¬åœ°å­˜å‚¨"},{"content":" ","date":"2023-03-10T00:00:00Z","image":"https://img11.360buyimg.com/ddimg/jfs/t1/170852/25/36092/117643/640ad741Fe576192d/3f596893cbcf93a4.jpg","permalink":"http://localhost:1313/shoot/yiheyuan/","title":"æ‘„å½±æ—¥è®°-é¢å’Œå›­"},{"content":"ä»€ä¹ˆæ˜¯Ingress Ingress æ˜¯å¯¹é›†ç¾¤ä¸­æœåŠ¡çš„å¤–éƒ¨è®¿é—®è¿›è¡Œç®¡ç†çš„ API å¯¹è±¡ï¼Œå…¸å‹çš„è®¿é—®æ–¹å¼æ˜¯ HTTPã€‚\nIngress å¯ä»¥æä¾›è´Ÿè½½å‡è¡¡ã€SSL ç»ˆç»“å’ŒåŸºäºåç§°çš„è™šæ‹Ÿæ‰˜ç®¡ã€‚\nIngress å…¬å¼€ä»é›†ç¾¤å¤–éƒ¨åˆ°é›†ç¾¤å†…æœåŠ¡çš„ HTTP å’Œ HTTPS è·¯ç”±ã€‚ æµé‡è·¯ç”±ç”± Ingress èµ„æºä¸Šå®šä¹‰çš„è§„åˆ™æ§åˆ¶ã€‚\nä¸‹é¢æ˜¯ä¸€ä¸ªå°†æ‰€æœ‰æµé‡éƒ½å‘é€åˆ°åŒä¸€ Service çš„ç®€å• Ingress ç¤ºä¾‹ï¼š\nIngress å…¶å®å°±æ˜¯ä» Kuberenets é›†ç¾¤å¤–éƒ¨è®¿é—®é›†ç¾¤çš„ä¸€ä¸ªå…¥å£ï¼Œå°†å¤–éƒ¨çš„è¯·æ±‚è½¬å‘åˆ°é›†ç¾¤å†…ä¸åŒçš„ Service ä¸Šï¼Œå…¶å®å°±ç›¸å½“äº nginxã€haproxy ç­‰è´Ÿè½½å‡è¡¡ä»£ç†æœåŠ¡å™¨ï¼Œå¯èƒ½ä½ ä¼šè§‰å¾—æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ nginx å°±å®ç°äº†ï¼Œä½†æ˜¯åªä½¿ç”¨ nginx è¿™ç§æ–¹å¼æœ‰å¾ˆå¤§ç¼ºé™·ï¼Œæ¯æ¬¡æœ‰æ–°æœåŠ¡åŠ å…¥çš„æ—¶å€™æ€ä¹ˆæ”¹ Nginx é…ç½®ï¼Ÿä¸å¯èƒ½è®©æˆ‘ä»¬å»æ‰‹åŠ¨æ›´æ”¹æˆ–è€…æ»šåŠ¨æ›´æ–°å‰ç«¯çš„ Nginx Pod å§ï¼Ÿé‚£æˆ‘ä»¬å†åŠ ä¸Šä¸€ä¸ªæœåŠ¡å‘ç°çš„å·¥å…·æ¯”å¦‚ consul å¦‚ä½•ï¼Ÿè²Œä¼¼æ˜¯å¯ä»¥ï¼Œå¯¹å§ï¼ŸIngress å®é™…ä¸Šå°±æ˜¯è¿™æ ·å®ç°çš„ï¼Œåªæ˜¯æœåŠ¡å‘ç°çš„åŠŸèƒ½è‡ªå·±å®ç°äº†ï¼Œä¸éœ€è¦ä½¿ç”¨ç¬¬ä¸‰æ–¹çš„æœåŠ¡äº†ï¼Œç„¶åå†åŠ ä¸Šä¸€ä¸ªåŸŸåè§„åˆ™å®šä¹‰ï¼Œè·¯ç”±ä¿¡æ¯çš„åˆ·æ–°ä¾é  Ingress Controller æ¥æä¾›ã€‚\nIngress Controller å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªç›‘å¬å™¨ï¼Œé€šè¿‡ä¸æ–­åœ°ç›‘å¬ kube-apiserverï¼Œå®æ—¶çš„æ„ŸçŸ¥åç«¯ Serviceã€Pod çš„å˜åŒ–ï¼Œå½“å¾—åˆ°è¿™äº›ä¿¡æ¯å˜åŒ–åï¼ŒIngress Controller å†ç»“åˆ Ingress çš„é…ç½®ï¼Œæ›´æ–°åå‘ä»£ç†è´Ÿè½½å‡è¡¡å™¨ï¼Œè¾¾åˆ°æœåŠ¡å‘ç°çš„ä½œç”¨ã€‚å…¶å®è¿™ç‚¹å’ŒæœåŠ¡å‘ç°å·¥å…· consulã€ consul-template éå¸¸ç±»ä¼¼ã€‚\nç°åœ¨å¯ä»¥ä¾›å¤§å®¶ä½¿ç”¨çš„ Ingress Controller æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚ traefikã€nginx-controllerã€Kubernetes Ingress Controller for Kongã€HAProxy Ingress controllerï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥è‡ªå·±å®ç°ä¸€ä¸ª Ingress Controllerï¼Œç°åœ¨æ™®éç”¨å¾—è¾ƒå¤šçš„æ˜¯ traefik å’Œ nginx-controllerï¼Œtraefik çš„æ€§èƒ½è¾ƒ nginx-controller å·®ï¼Œä½†æ˜¯é…ç½®ä½¿ç”¨è¦ç®€å•è®¸å¤šï¼Œæˆ‘ä»¬è¿™é‡Œä¼šé‡ç‚¹ç»™å¤§å®¶ä»‹ç» nginx-controller ä»¥åŠ traefik çš„ä½¿ç”¨ã€‚\nå®‰è£…NGINX Ingress Controller å®˜æ–¹æ–‡æ¡£ï¼šNGINX Ingress Controller NGINX Ingress Controller æ˜¯ä½¿ç”¨ Kubernetes Ingress èµ„æºå¯¹è±¡æ„å»ºçš„ï¼Œç”¨ ConfigMap æ¥å­˜å‚¨ Nginx é…ç½®çš„ä¸€ç§ Ingress Controller å®ç°ã€‚\nç”±äº nginx-ingress æ‰€åœ¨çš„èŠ‚ç‚¹éœ€è¦èƒ½å¤Ÿè®¿é—®å¤–ç½‘ï¼Œè¿™æ ·åŸŸåå¯ä»¥è§£æåˆ°è¿™äº›èŠ‚ç‚¹ä¸Šç›´æ¥ä½¿ç”¨ï¼Œæ‰€ä»¥éœ€è¦è®© nginx-ingress ç»‘å®šèŠ‚ç‚¹çš„ 80 å’Œ 443 ç«¯å£ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ hostPort æ¥è¿›è¡Œè®¿é—®ã€‚\næŸ¥çœ‹å½“å‰Ingress-Nginxé€‚ç”¨çš„kubernetesç‰ˆæœ¬\nIngress-NGINX version k8s supported version Alpine Version Nginx Version v1.6.4 1.26, 1.25, 1.24, 1.23 3.17.0 1.21.6 v1.5.1 1.25, 1.24, 1.23 3.16.2 1.21.6 v1.4.0 1.25, 1.24, 1.23, 1.22 3.16.2 1.19.10â€  v1.3.1 1.24, 1.23, 1.22, 1.21, 1.20 3.16.2 1.19.10â€  v1.3.0 1.24, 1.23, 1.22, 1.21, 1.20 3.16.0 1.19.10â€  v1.2.1 1.23, 1.22, 1.21, 1.20, 1.19 3.14.6 1.19.10â€  v1.1.3 1.23, 1.22, 1.21, 1.20, 1.19 3.14.4 1.19.10â€  v1.1.2 1.23, 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9â€  v1.1.1 1.23, 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9â€  v1.1.0 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9â€  v1.0.5 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9â€  v1.0.4 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9â€  v1.0.3 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9â€  v1.0.2 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9â€  v1.0.1 1.22, 1.21, 1.20, 1.19 3.14.2 1.19.9â€  v1.0.0 1.22, 1.21, 1.20, 1.19 3.13.5 1.20.1 ä½¿ç”¨Helmè¿›è¡Œéƒ¨ç½²nginx-ingress-controller helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm fetch ingress-nginx/ingress-nginx tar -xvf ingress-nginx-4.5.2.tgz æ–°å»ºä¸€ä¸ªvalue-test.yamlçš„é…ç½®æ–‡ä»¶ dnsPolicyï¼šå› ä¸ºå¤„äºhostNetwork:trueçš„çŠ¶æ€ä¸‹,Podé»˜è®¤ä½¿ç”¨å®¿ä¸»æœºçš„DNSè§£æ,è¿™æ ·ä¼šå¯¼è‡´å¦‚æœä½ ä½¿ç”¨ServiceNameçš„æ–¹å¼æ¥è®¿é—®Podçš„è¯ä¼šå‡ºç°æ— æ³•è§£æçš„æƒ…å†µã€‚æ‰€ä»¥ä¿®æ”¹ä¸ºClusterFirstWithHostNet è¯·å°†webhookçš„é•œåƒä¿®æ”¹ä¸ºregistry.cn-beijing.aliyuncs.com/polymerization/kube-webhook-certgen:v20220916-gd32f8c343 controller: name: controller image: repository: registry.cn-beijing.aliyuncs.com/polymerization/nginx-controller tag: \u0026#34;v1.6.4\u0026#34; digest: sha256:e727015a639975f4fc0808b91f9e88a83c60938b640ee6c2f5606ddd779c858d dnsPolicy: ClusterFirstWithHostNet hostNetwork: true publishService: # hostNetwork æ¨¡å¼ä¸‹è®¾ç½®ä¸ºfalseï¼Œé€šè¿‡èŠ‚ç‚¹IPåœ°å€ä¸ŠæŠ¥ingress statusæ•°æ® enabled: false kind: DaemonSet tolerations: # æ³¨æ„,å¦‚æœä½ çš„Kubernetesé›†ç¾¤ä¸­å­˜åœ¨å¤šä¸ªTaintéœ€è¦å…¨éƒ¨è¿›è¡Œå®¹å¿ã€‚ - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; nodeSelector: # å›ºå®šèŠ‚ç‚¹-\u0026gt;è¯·ç»™3å°masterå…¨éƒ¨æ‰“ä¸Šè¿™ä¸ªæ ‡ç­¾-\u0026gt;ä¸ªäººå»ºè®®å°†ingress-managerè¾¹ç¼˜åŒ– node.kubernetes.io/ingress-manager: \u0026#39;true\u0026#39; service: # HostNetwork æ¨¡å¼ä¸éœ€è¦åˆ›å»ºservice enabled: false admissionWebhooks: enable: true patch: enable: true image: registry: registry.cn-beijing.aliyuncs.com image: polymerization/kube-webhook-certgen tag: v20220916-gd32f8c343 digest: sha256:c0e3bef270e179a5e4ab373f8ba6d57f596f3683d9d40c33ea900b19ec182ba2 pullPolicy: IfNotPresent defaultBackend: enabled: false éƒ¨ç½²ingress-controller # å®‰è£… helm install --namespace ingress-nginx ingress-nginx ./ingress-nginx -f value-test.yaml # å¸è½½ helm uninstall ingress-nginx --namespace ingress-nginx Ingressçš„åŸºæœ¬ä½¿ç”¨ åˆ›å»ºä¸€ä¸ªingressèµ„æºå¯¹è±¡ ä¸€ä¸ªæœ€å°çš„ Ingress èµ„æºç¤ºä¾‹\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: simple-qingyang-ingress namespace: out-apps spec: ingressClassName: nginx rules: - host: nginx.qingyang.com # å°†åŸŸåæ˜ å°„åˆ°åç«¯æœåŠ¡ http: paths: - path: / pathType: Prefix backend: service: name: os-qingyang port: number: 80 Ingress éœ€è¦æŒ‡å®š apiVersionã€kindã€ metadataå’Œ spec å­—æ®µã€‚ Ingress å¯¹è±¡çš„å‘½åå¿…é¡»æ˜¯åˆæ³•çš„ DNS å­åŸŸååç§°ã€‚ å…³äºå¦‚ä½•ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œè¯·å‚è§éƒ¨ç½²åº”ç”¨ã€ é…ç½®å®¹å™¨ã€ ç®¡ç†èµ„æºã€‚ Ingress ç»å¸¸ä½¿ç”¨æ³¨è§£ï¼ˆannotationsï¼‰æ¥é…ç½®ä¸€äº›é€‰é¡¹ï¼Œå…·ä½“å–å†³äº Ingress æ§åˆ¶å™¨ï¼Œä¾‹å¦‚é‡å†™ç›®æ ‡æ³¨è§£ã€‚ ä¸åŒçš„ Ingress æ§åˆ¶å™¨æ”¯æŒä¸åŒçš„æ³¨è§£ã€‚ æŸ¥çœ‹ä½ æ‰€é€‰çš„ Ingress æ§åˆ¶å™¨çš„æ–‡æ¡£ï¼Œä»¥äº†è§£å…¶æ”¯æŒå“ªäº›æ³¨è§£ã€‚\nå¦‚æœ ingressClassName è¢«çœç•¥ï¼Œé‚£ä¹ˆä½ åº”è¯¥å®šä¹‰ä¸€ä¸ªé»˜è®¤ Ingress ç±»,å¦åˆ™æ— æ³•è½¬å‘æœåŠ¡\nåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ingressClass\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller name: default-nginx annotations: ingressclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; spec: controller: k8s.io/ingress-nginx æœ‰ä¸€äº› Ingress æ§åˆ¶å™¨ä¸éœ€è¦å®šä¹‰é»˜è®¤çš„ IngressClassã€‚æ¯”å¦‚ï¼šIngress-NGINX æ§åˆ¶å™¨å¯ä»¥é€šè¿‡å‚æ•° --watch-ingress-without-class æ¥é…ç½®ã€‚ ä¸è¿‡ä»ç„¶æ¨èåˆ›å»ºé»˜è®¤çš„ingressClass\nå¯ä»¥çœ‹ä¸€ä¸‹ç®€å•åœ°ingress-controllerè¯·æ±‚æµç¨‹\nå®¢æˆ·ç«¯é¦–å…ˆå¯¹ ngdemo.qikqiak.com æ‰§è¡Œ DNS è§£æï¼Œå¾—åˆ° Ingress Controller æ‰€åœ¨èŠ‚ç‚¹çš„ IP åå®¢æˆ·ç«¯å‘ Ingress Controller å‘é€ HTTP è¯·æ±‚ æ ¹æ® Ingress å¯¹è±¡é‡Œé¢çš„æè¿°åŒ¹é…åŸŸåï¼Œæ‰¾åˆ°å¯¹åº”çš„ Service å¯¹è±¡ï¼Œå¹¶è·å–å…³è”çš„ Endpoints åˆ—è¡¨ï¼Œå°†å®¢æˆ·ç«¯çš„è¯·æ±‚è½¬å‘ç»™å…¶ä¸­ä¸€ä¸ª Pod åˆ›å»ºTodo-appæµ‹è¯•(æš‚æ—¶åºŸå¼ƒ) é¦–å…ˆéƒ¨ç½²MongoDB apiVersion: apps/v1 kind: Deployment metadata: name: mongo spec: selector: matchLabels: app: mongo template: metadata: labels: app: mongo spec: volumes: - name: data emptyDir: {} - name: resolv-conf configMap: name: cache-dns items: - key: resolv.conf path: resolv.conf containers: - name: mongo image: mongo ports: - containerPort: 27017 volumeMounts: - name: data mountPath: /data/db - name: resolv-conf mountPath: /etc/resolv.conf subPath: resolv.conf --- apiVersion: v1 kind: Service metadata: name: mongo spec: selector: app: mongo type: ClusterIP ports: - name: db port: 27017 targetPort: 27017 åˆ›å»ºTodo apiVersion: apps/v1 kind: Deployment metadata: name: todo spec: selector: matchLabels: app: todo template: metadata: labels: app: todo spec: containers: - name: web image: cnych/todo:v1.1 env: - name: \u0026#34;DBHOST\u0026#34; value: \u0026#34;mongodb://mongo.default.svc.cluster.local:27017\u0026#34; ports: - containerPort: 3000 --- apiVersion: v1 kind: Service metadata: name: todo spec: selector: app: todo type: ClusterIP ports: - name: web port: 3000 targetPort: 3000 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: todo spec: ingressClassName: nginx rules: - host: nginx.qingyang.com # å°†åŸŸåæ˜ å°„åˆ°åç«¯æœåŠ¡ http: paths: - path: / pathType: Prefix backend: service: name: todo port: number: 3000 URL Rewrite Rewriteçš„Ingressæ³¨è§£\nnginx.ingress.kubernetes.io/rewrite-target Target URI where the traffic must be redirected string nginx.ingress.kubernetes.io/ssl-redirect Indicates if the location section is only accessible via SSL (defaults to True when Ingress contains a Certificate) bool nginx.ingress.kubernetes.io/force-ssl-redirect Forces the redirection to HTTPS even if the Ingress is not TLS Enabled bool nginx.ingress.kubernetes.io/app-root Defines the Application Root that the Controller must redirect if itâ€™s in / context string nginx.ingress.kubernetes.io/use-regex Indicates if the paths defined on an Ingress use regular expressions bool ç°åœ¨æˆ‘ä»¬éœ€è¦å¯¹è®¿é—®çš„ URL è·¯å¾„åšä¸€ä¸ª Rewriteï¼Œæ¯”å¦‚åœ¨ PATH ä¸­æ·»åŠ ä¸€ä¸ª app çš„å‰ç¼€ï¼Œå…³äº Rewrite çš„æ“ä½œåœ¨ ingress-nginx å®˜æ–¹æ–‡æ¡£ä¸­ä¹Ÿç»™å‡ºå¯¹åº”çš„è¯´æ˜ã€‚\nnginx.ingress.kubernetes.io/rewrite-target: æµé‡å¿…é¡»é‡å®šå‘çš„ç›®æ ‡URI(Target URI where the traffic must be redirected) apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: todo annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 spec: ingressClassName: nginx rules: - host: nginx.qingyang.com http: paths: - path: /something(/|$)(.*) # åŒ¹é…/somethingå’Œ/something/* pathType: Prefix backend: service: name: todo port: number: 3000 åœ¨æ­¤å…¥å£å®šä¹‰ä¸­ï¼Œæ•è·çš„ä»»ä½•å­—ç¬¦éƒ½(.*)å°†åˆ†é…ç»™å ä½ç¬¦$2ï¼Œç„¶åå°†å…¶ç”¨ä½œæ³¨é‡Šä¸­çš„å‚æ•°rewrite-targetã€‚\nä¾‹å¦‚ï¼Œä¸Šé¢çš„å…¥å£å®šä¹‰å°†å¯¼è‡´ä»¥ä¸‹é‡å†™ï¼š\nnginx.qingyang.com/somethingæ”¹å†™ä¸ºnginx.qingyang.com/ nginx.qingyang.com/something/æ”¹å†™ä¸ºnginx.qingyang.com/ nginx.qingyang.com/something/newæ”¹å†™ä¸ºnginx.qingyang.com/new ä½¿ç”¨æ­¤æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´éƒ¨åˆ†cssã€jsç­‰å†…å®¹æ— æ³•æ‰¾åˆ°,å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•å®ç°\né€šè¿‡configuration-snippetæ³¨è§£ apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: todo annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 nginx.ingress.kubernetes.io/configuration-snippet: | rewrite ^/css/(.*)$ /something/css/$1 redirect; # ä¸ºcssæ ·å¼æ·»åŠ /somethingå‰ç¼€ rewrite ^/js/(.*)$ /something/js/$1 Basic Auth ä»¬è¿˜å¯ä»¥åœ¨ Ingress Controller ä¸Šé¢é…ç½®ä¸€äº›åŸºæœ¬çš„ Auth è®¤è¯ï¼Œæ¯”å¦‚ Basic Authï¼Œå¯ä»¥ç”¨ htpasswd ç”Ÿæˆä¸€ä¸ªå¯†ç æ–‡ä»¶æ¥éªŒè¯èº«ä»½éªŒè¯ã€‚\n[root@Online-Beijing-master1 ~]# htpasswd -c auth admin åˆ›å»ºä¸€ä¸ªsecret\n[root@Online-Beijing-master1 ~]# kubectl create secret generic basic-auth --from-file=authBasic Auth çš„ Ingress å¯¹è±¡ï¼š åˆ›å»ºä¸€ä¸ªå…·æœ‰ Basic Auth çš„ Ingress å¯¹è±¡\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: basic-auth-demo annotations: nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-realm: \u0026#39;Authentication Required - admin\u0026#39; spec: ingressClassName: nginx rules: - host: nginx.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: os-vue-comment port: number: 80 æ­£å¸¸ä¼šå¼¹å‡ºæ¥è®¤è¯çª—å£ï¼Œè¿›è¡Œè®¤è¯å°±è¡Œã€‚\nç°åº¦åº”ç”¨ åœ¨æ—¥å¸¸å·¥ä½œä¸­æˆ‘ä»¬ç»å¸¸éœ€è¦å¯¹æœåŠ¡è¿›è¡Œç‰ˆæœ¬æ›´æ–°å‡çº§ï¼Œæ‰€ä»¥æˆ‘ä»¬ç»å¸¸ä¼šä½¿ç”¨åˆ°æ»šåŠ¨å‡çº§ã€è“ç»¿å‘å¸ƒã€ç°åº¦å‘å¸ƒç­‰ä¸åŒçš„å‘å¸ƒæ“ä½œã€‚è€Œ ingress-nginx æ”¯æŒé€šè¿‡ Annotations é…ç½®æ¥å®ç°ä¸åŒåœºæ™¯ä¸‹çš„ç°åº¦å‘å¸ƒå’Œæµ‹è¯•ï¼Œå¯ä»¥æ»¡è¶³é‡‘ä¸é›€å‘å¸ƒã€è“ç»¿éƒ¨ç½²ä¸ A/B æµ‹è¯•ç­‰ä¸šåŠ¡åœºæ™¯ã€‚\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›é€šè¿‡å‘ä¸ç”Ÿäº§æœåŠ¡ä¸åŒçš„æœåŠ¡å‘é€å°‘é‡è¯·æ±‚æ¥é‡‘ä¸é›€ä¸€ç»„æ–°çš„æ›´æ”¹ã€‚Canary æ³¨é‡Šä½¿ Ingress è§„èŒƒå¯ä»¥å……å½“è¯·æ±‚è·¯ç”±åˆ°çš„æ›¿ä»£æœåŠ¡ï¼Œå…·ä½“å–å†³äºåº”ç”¨çš„è§„åˆ™ã€‚\ningress-nginx çš„ Annotations æ”¯æŒä»¥ä¸‹ 4 ç§ Canary è§„åˆ™ï¼š\nnginx.ingress.kubernetes.io/canary-by-header: åŸºäº Request Header çš„æµé‡åˆ‡åˆ†ï¼Œé€‚ç”¨äºç°åº¦å‘å¸ƒä»¥åŠ A/B æµ‹è¯•ã€‚å½“ Request Header è®¾ç½®ä¸º always æ—¶ï¼Œè¯·æ±‚å°†ä¼šè¢«ä¸€ç›´å‘é€åˆ° Canary ç‰ˆæœ¬ï¼›å½“ Request Header è®¾ç½®ä¸º neveræ—¶ï¼Œè¯·æ±‚ä¸ä¼šè¢«å‘é€åˆ° Canary å…¥å£ï¼›å¯¹äºä»»ä½•å…¶ä»– Header å€¼ï¼Œå°†å¿½ç•¥ Headerï¼Œå¹¶é€šè¿‡ä¼˜å…ˆçº§å°†è¯·æ±‚ä¸å…¶ä»–é‡‘ä¸é›€è§„åˆ™è¿›è¡Œä¼˜å…ˆçº§çš„æ¯”è¾ƒã€‚ nginx.ingress.kubernetes.io/canary-by-header-value: è¦åŒ¹é…çš„ Request Header çš„å€¼ï¼Œç”¨äºé€šçŸ¥ Ingress å°†è¯·æ±‚è·¯ç”±åˆ° Canary Ingress ä¸­æŒ‡å®šçš„æœåŠ¡ã€‚å½“ Request Header è®¾ç½®ä¸ºæ­¤å€¼æ—¶ï¼Œå®ƒå°†è¢«è·¯ç”±åˆ° Canary å…¥å£ã€‚è¯¥è§„åˆ™å…è®¸ç”¨æˆ·è‡ªå®šä¹‰ Request Header çš„å€¼ã€‚æ­¤æ³¨é‡Šå¿…é¡»ä¸ ä¸€èµ·ä½¿ç”¨nginx.ingress.kubernetes.io/canary-by-headerã€‚å¦‚æœæœªå®šä¹‰canary-by-header,é‚£ä¹ˆè¯¥æ³¨è§£æ²¡æœ‰ä»»ä½•æ•ˆæœã€‚ nginx.ingress.kubernetes.io/canary-weight: åŸºäºæœåŠ¡æƒé‡çš„æµé‡åˆ‡åˆ†ï¼Œé€‚ç”¨äºè“ç»¿éƒ¨ç½²ï¼Œæƒé‡èŒƒå›´ 0 - 100 æŒ‰ç™¾åˆ†æ¯”å°†è¯·æ±‚è·¯ç”±åˆ° Canary Ingress ä¸­æŒ‡å®šçš„æœåŠ¡ã€‚æƒé‡ä¸º 0 æ„å‘³ç€è¯¥é‡‘ä¸é›€è§„åˆ™ä¸ä¼šå‘ Canary å…¥å£çš„æœåŠ¡å‘é€ä»»ä½•è¯·æ±‚ï¼Œæƒé‡ä¸º 100 æ„å‘³ç€æ‰€æœ‰è¯·æ±‚éƒ½å°†è¢«å‘é€åˆ° Canary å…¥å£ã€‚ nginx.ingress.kubernetes.io/canary-by-cookie: åŸºäº cookie çš„æµé‡åˆ‡åˆ†ï¼Œé€‚ç”¨äºç°åº¦å‘å¸ƒä¸ A/B æµ‹è¯•ã€‚ç”¨äºé€šçŸ¥ Ingress å°†è¯·æ±‚è·¯ç”±åˆ° Canary Ingress ä¸­æŒ‡å®šçš„æœåŠ¡çš„cookieã€‚å½“ cookie å€¼è®¾ç½®ä¸º always æ—¶ï¼Œå®ƒå°†è¢«è·¯ç”±åˆ° Canary å…¥å£ï¼›å½“ cookie å€¼è®¾ç½®ä¸º never æ—¶ï¼Œè¯·æ±‚ä¸ä¼šè¢«å‘é€åˆ° Canary å…¥å£ï¼›å¯¹äºä»»ä½•å…¶ä»–å€¼ï¼Œå°†å¿½ç•¥ cookie å¹¶å°†è¯·æ±‚ä¸å…¶ä»–é‡‘ä¸é›€è§„åˆ™è¿›è¡Œä¼˜å…ˆçº§çš„æ¯”è¾ƒã€‚ Canary è§„åˆ™æŒ‰ä¼˜å…ˆé¡ºåºè¿›è¡Œè¯„ä¼°ã€‚ä¼˜å…ˆçº§å¦‚ä¸‹ï¼šcanary-by-header -\u0026gt; canary-by-cookie -\u0026gt; canary-weight\næŠŠä»¥ä¸Šçš„å››ä¸ª annotation è§„åˆ™å¯ä»¥æ€»ä½“åˆ’åˆ†ä¸ºä»¥ä¸‹ä¸¤ç±»ï¼š\nåŸºäºæƒé‡çš„çš„Canaryè§„åˆ™ åŸºäºç”¨æˆ·è¯·æ±‚çš„Canaryè§„åˆ™ ç°åº¦éªŒè¯ é¦–å…ˆæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªåŸºäºProducationç‰ˆæœ¬çš„åº”ç”¨ apiVersion: apps/v1 kind: Deployment metadata: name: production-app labels: version: production spec: replicas: 1 selector: matchLabels: version: production template: metadata: labels: version: production spec: containers: - name: production-demo image: mirrorgooglecontainers/echoserver:1.10 ports: - containerPort: 8080 env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP --- apiVersion: v1 kind: Service metadata: name: production-service labels: version: production spec: ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: version: production åˆ›å»ºProduction ç‰ˆæœ¬çš„åº”ç”¨è·¯ç”± (Ingress)ã€‚ apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: production-ingress spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: production-service port: number: 80 åˆ›å»ºCanaryç‰ˆæœ¬çš„åº”ç”¨ä¸Šçº¿ apiVersion: apps/v1 kind: Deployment metadata: name: canary-demo labels: app: canary-app version: canary spec: replicas: 1 selector: matchLabels: app: canary-app template: metadata: labels: app: canary-app spec: containers: - name: canary-demo image: mirrorgooglecontainers/echoserver:1.10 ports: - containerPort: 8080 env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP --- apiVersion: v1 kind: Service metadata: name: canary-service labels: version: canary spec: ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: app: canary-app åŸºäºæƒé‡çš„Canaryè§„åˆ™ åŸºäºæƒé‡çš„æµé‡åˆ‡åˆ†çš„å…¸å‹åº”ç”¨åœºæ™¯å°±æ˜¯è“ç»¿éƒ¨ç½²ï¼Œå¯é€šè¿‡å°†æƒé‡è®¾ç½®ä¸º 0 æˆ– 100 æ¥å®ç°ã€‚ä¾‹å¦‚ï¼Œå¯å°† Green ç‰ˆæœ¬è®¾ç½®ä¸ºä¸»è¦éƒ¨åˆ†ï¼Œå¹¶å°† Blue ç‰ˆæœ¬çš„å…¥å£é…ç½®ä¸º Canaryã€‚æœ€åˆï¼Œå°†æƒé‡è®¾ç½®ä¸º 0ï¼Œå› æ­¤ä¸ä¼šå°†æµé‡ä»£ç†åˆ° Blue ç‰ˆæœ¬ã€‚ä¸€æ—¦æ–°ç‰ˆæœ¬æµ‹è¯•å’ŒéªŒè¯éƒ½æˆåŠŸåï¼Œå³å¯å°† Blue ç‰ˆæœ¬çš„æƒé‡è®¾ç½®ä¸º 100ï¼Œå³æ‰€æœ‰æµé‡ä» Green ç‰ˆæœ¬è½¬å‘ Blueã€‚\nä»¥ä¸‹ Ingress ç¤ºä¾‹çš„ Canary ç‰ˆæœ¬ä½¿ç”¨äº†åŸºäºæƒé‡è¿›è¡Œæµé‡åˆ‡åˆ†çš„ annotation è§„åˆ™ï¼Œå°†åˆ†é… 30% çš„æµé‡è¯·æ±‚å‘é€è‡³ Canary ç‰ˆæœ¬ã€‚\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # å¼€å¯canaryæœºåˆ¶ nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # åˆ‡åˆ†30çš„æµé‡åˆ°canaryç‰ˆæœ¬ä¸­ spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: canary-service port: number: 80 åº”ç”¨çš„ Canary ç‰ˆæœ¬åŸºäºæƒé‡ (30%) è¿›è¡Œæµé‡åˆ‡åˆ†åï¼Œè®¿é—®åˆ° Canary ç‰ˆæœ¬çš„æ¦‚ç‡æ¥è¿‘ 30%ï¼Œæµé‡æ¯”ä¾‹å¯èƒ½ä¼šæœ‰å°èŒƒå›´çš„æµ®åŠ¨ã€‚\nåŸºäº Request Header åŸºäº Request Header è¿›è¡Œæµé‡åˆ‡åˆ†çš„å…¸å‹åº”ç”¨åœºæ™¯å³ç°åº¦å‘å¸ƒæˆ– A/B æµ‹è¯•åœºæ™¯ã€‚å‚è€ƒä»¥ä¸‹æˆªå›¾ï¼Œåœ¨ KubeSphere ç»™ Canary ç‰ˆæœ¬çš„åº”ç”¨è·¯ç”± (Ingress) æ–°å¢ä¸€æ¡ annotation nginx.ingress.kubernetes.io/canary-by-header: canary(è¿™é‡Œçš„ annotation çš„ value å¯ä»¥æ˜¯ä»»æ„å€¼)ï¼Œä½¿å½“å‰çš„ Ingress å®ç°åŸºäº Request Header è¿›è¡Œæµé‡åˆ‡åˆ†ã€‚\né‡‘ä¸é›€è§„åˆ™æŒ‰ä¼˜å…ˆé¡ºåº canary-by-header - \u0026gt; canary-by-cookie - \u0026gt; canary-weightè¿›è¡Œå¦‚ä¸‹æ’åºï¼Œå› æ­¤ä»¥ä¸‹æƒ…å†µå°†å¿½ç•¥åŸæœ‰ canary-weight çš„è§„åˆ™ã€‚\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary-by-header: \u0026#34;canary-header\u0026#34; # æ·»åŠ header nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # å¼€å¯canaryæœºåˆ¶ nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # åˆ‡åˆ†30çš„æµé‡åˆ°canaryç‰ˆæœ¬ä¸­ spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: canary-service port: number: 80 æˆ‘ä»¬å°è¯•è®¿é—®ä¸€ä¸‹\n[root@Online-Beijing-master1 ~]# for i in $(seq 1 10); do curl -s prod.qingyang.com | grep \u0026#34;Hostname\u0026#34;; done Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: canary-demo-54dfb9bd-n2zlg Hostname: production-app-678488687f-s4sbd å°è¯•åŠ å…¥è¯·æ±‚å¤´è®¿é—®\nå¦‚æœä½ çš„canary-headerçš„å€¼ä¸ºneveråˆ™è¡¨ç¤ºè¯·æ±‚æ°¸è¿œä¸ä¼šè¯·æ±‚åˆ°å½“å‰ç‰ˆæœ¬,å¦‚æœä½ çš„canary-headerçš„å€¼è®¾ç½®ä¸ºalwaysçš„è¯åˆ™è¡¨ç¤ºæ°¸è¿œè¯·æ±‚å½“å‰ç‰ˆæœ¬ [root@Online-Beijing-master1 ~]# for i in $(seq 1 10); do curl -s -H \u0026#34;canary-header: never\u0026#34; prod.qingyang.com | grep \u0026#34;Hostname\u0026#34;; done Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd Hostname: production-app-678488687f-s4sbd ------------------------------------------------ [root@Online-Beijing-master1 ~]# for i in $(seq 1 10); do curl -s -H \u0026#34;canary-header: always\u0026#34; prod.qingyang.com | grep \u0026#34;Hostname\u0026#34;; done Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg Hostname: canary-demo-54dfb9bd-n2zlg å¦‚æœä½ æƒ³è®©ç”¨æˆ·è¯·æ±‚åˆ°æŒ‡å®šçš„æœåŠ¡ä¸Šå¯ä»¥æ·»åŠ ginx.ingress.kubernetes.io/canary-by-header-value: user-value,å½“è¯·æ±‚è®¿é—®æºå¸¦canary-header: user-valueçš„æ—¶å€™,é‚£ä¹ˆè¯¥è¯·æ±‚ä¼šè¢«è½¬å‘åˆ°canaryç‰ˆæœ¬ã€‚\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary-by-header-value: \u0026#34;user-value\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: \u0026#34;canary-header\u0026#34; # æ·»åŠ header nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # å¼€å¯canaryæœºåˆ¶ nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # åˆ‡åˆ†30çš„æµé‡åˆ°canaryç‰ˆæœ¬ä¸­ spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: canary-service port: number: 80 åŸºäº Cookieçš„canary ä¸åŸºäº Request Header çš„ annotation ç”¨æ³•è§„åˆ™ç±»ä¼¼ã€‚ä¾‹å¦‚åœ¨ A/B æµ‹è¯•åœºæ™¯ä¸‹ï¼Œéœ€è¦è®©åœ°åŸŸä¸ºåŒ—äº¬çš„ç”¨æˆ·è®¿é—® Canary ç‰ˆæœ¬ã€‚é‚£ä¹ˆå½“ cookie çš„ annotation è®¾ç½®ä¸º nginx.ingress.kubernetes.io/canary-by-cookie: \u0026quot;users_from_beijing\u0026quot;ï¼Œæ­¤æ—¶åå°å¯å¯¹ç™»å½•çš„ç”¨æˆ·è¯·æ±‚è¿›è¡Œæ£€æŸ¥ï¼Œå¦‚æœè¯¥ç”¨æˆ·è®¿é—®æºæ¥è‡ªåŒ—äº¬åˆ™è®¾ç½® cookie users_from_beijingçš„å€¼ä¸º alwaysï¼Œè¿™æ ·å°±å¯ä»¥ç¡®ä¿åŒ—äº¬çš„ç”¨æˆ·ä»…è®¿é—® Canary ç‰ˆæœ¬\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary-by-cookie: \u0026#34;user_from_beijing\u0026#34; # æ·»åŠ cookie nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # å¼€å¯canaryæœºåˆ¶ nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # åˆ‡åˆ†30çš„æµé‡åˆ°canaryç‰ˆæœ¬ä¸­ spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: canary-service port: number: 80 è‡ªç­¾HTTPS å¦‚æœæˆ‘ä»¬éœ€è¦ç”¨ HTTPS æ¥è®¿é—®æˆ‘ä»¬è¿™ä¸ªåº”ç”¨çš„è¯ï¼Œå°±éœ€è¦ç›‘å¬ 443 ç«¯å£äº†ï¼ŒåŒæ ·ç”¨ HTTPS è®¿é—®åº”ç”¨å¿…ç„¶å°±éœ€è¦è¯ä¹¦ï¼Œè¿™é‡Œæˆ‘ä»¬ç”¨ openssl æ¥åˆ›å»ºä¸€ä¸ªè‡ªç­¾åçš„è¯ä¹¦ï¼š\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=prod.qingyang.com\u0026#34; åˆ›å»ºtlsç±»å‹çš„secret\nkubectl create secret tls self-sign-nginx --cert=tls.crt --key=tls.key åˆ›å»ºå¸¦tlsçš„ingress\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: canary-ingress annotations: nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; # å¼€å¯canaryæœºåˆ¶ nginx.ingress.kubernetes.io/canary-weight: \u0026#34;30\u0026#34; # åˆ‡åˆ†30çš„æµé‡åˆ°canaryç‰ˆæœ¬ä¸­ spec: ingressClassName: nginx rules: - host: prod.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: nginx-demo port: number: 80 tls: - hosts: - prod.qingyang.com secretName: self-sign-nginx CertManager è‡ªåŠ¨ HTTPS cert-manager å°†è¯ä¹¦å’Œè¯ä¹¦é¢å‘è€…ä½œä¸ºèµ„æºç±»å‹æ·»åŠ åˆ° Kubernetes é›†ç¾¤ä¸­ï¼Œå¹¶ç®€åŒ–äº†è¿™äº›è¯ä¹¦çš„è·å–ã€æ›´æ–°å’Œä½¿ç”¨è¿‡ç¨‹ã€‚\nå®ƒå¯ä»¥ä»å„ç§å—æ”¯æŒçš„æ¥æºé¢å‘è¯ä¹¦ï¼ŒåŒ…æ‹¬ Letâ€™s Encryptã€HashiCorp Vaultå’ŒVenafiä»¥åŠç§æœ‰ PKIã€‚\nå®ƒå°†ç¡®ä¿è¯ä¹¦æœ‰æ•ˆä¸”æœ€æ–°ï¼Œå¹¶å°è¯•åœ¨åˆ°æœŸå‰çš„é…ç½®æ—¶é—´æ›´æ–°è¯ä¹¦ã€‚\nå®ƒå¤§è‡´åŸºäº kube-legoçš„å·¥ä½œï¼Œå¹¶å€Ÿé‰´äº†å…¶ä»–ç±»ä¼¼é¡¹ç›®ï¼ˆä¾‹å¦‚ kube-cert-managerï¼‰çš„ä¸€äº›æ™ºæ…§ã€‚\nIssuers: ä»£è¡¨çš„æ˜¯è¯ä¹¦é¢å‘è€…ï¼Œå¯ä»¥å®šä¹‰å„ç§æä¾›è€…çš„è¯ä¹¦é¢å‘è€…ï¼Œå½“å‰æ”¯æŒåŸºäº Let's Encrypt/HashiCorp/Vault å’Œ CA çš„è¯ä¹¦é¢å‘è€…ï¼Œè¿˜å¯ä»¥å®šä¹‰ä¸åŒç¯å¢ƒä¸‹çš„è¯ä¹¦é¢å‘è€…ã€‚ Certificates: ä»£è¡¨çš„æ˜¯ç”Ÿæˆè¯ä¹¦çš„è¯·æ±‚. éƒ¨ç½²cert-manager å¥½åƒè¿™ä¸ªquay.ioèƒ½æ‹‰ä¸‹æ¥äº†â€¦\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml æ­£å¸¸éƒ¨ç½²å®Œæˆå¯ä»¥çœ‹åˆ°Podæ­£åœ¨è¿è¡Œ\n[root@Online-Beijing-master1 ~]# kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-6499989f7-m6vdj 1/1 Running 0 2m30s cert-manager-cainjector-645b688547-xjcb8 1/1 Running 0 2m30s cert-manager-webhook-6b7f49999f-mcnf7 1/1 Running 0 2m30s æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢çš„æµ‹è¯•æ¥éªŒè¯ä¸‹æ˜¯å¦å¯ä»¥ç­¾å‘åŸºæœ¬çš„è¯ä¹¦ç±»å‹ï¼Œåˆ›å»ºä¸€ä¸ª Issuer èµ„æºå¯¹è±¡æ¥æµ‹è¯• webhook å·¥ä½œæ˜¯å¦æ­£å¸¸(åœ¨å¼€å§‹ç­¾å‘è¯ä¹¦ä¹‹å‰ï¼Œå¿…é¡»åœ¨ç¾¤é›†ä¸­è‡³å°‘é…ç½®ä¸€ä¸ª Issuer æˆ– ClusterIssuer èµ„æº)\napiVersion: v1 kind: Namespace metadata: name: cert-manager-test --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} # é…ç½®è‡ªç­¾åçš„è¯ä¹¦æœºæ„ç±»å‹ --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-cert namespace: cert-manager-test spec: dnsNames: - example.com secretName: selfsigned-cert-tls issuerRef: name: test-selfsigned è‡ªåŠ¨HTTPS Let's Encrypt ä½¿ç”¨ ACME åè®®æ¥æ ¡éªŒåŸŸåæ˜¯å¦çœŸçš„å±äºä½ ï¼Œæ ¡éªŒæˆåŠŸåå°±å¯ä»¥è‡ªåŠ¨é¢å‘å…è´¹è¯ä¹¦ï¼Œè¯ä¹¦æœ‰æ•ˆæœŸåªæœ‰ 90 å¤©ï¼Œåœ¨åˆ°æœŸå‰éœ€è¦å†æ ¡éªŒä¸€æ¬¡æ¥å®ç°ç»­æœŸï¼Œè€Œ cert-manager æ˜¯å¯ä»¥è‡ªåŠ¨ç»­æœŸçš„ï¼Œæ‰€ä»¥äº‹å®ä¸Šå¹¶ä¸ç”¨æ‹…å¿ƒè¯ä¹¦è¿‡æœŸçš„é—®é¢˜ã€‚ç›®å‰ä¸»è¦æœ‰ HTTP å’Œ DNS ä¸¤ç§æ ¡éªŒæ–¹å¼ã€‚\nHTTP-01 æ ¡éªŒ HTTP-01 çš„æ ¡éªŒæ˜¯é€šè¿‡ç»™ä½ åŸŸåæŒ‡å‘çš„ HTTP æœåŠ¡å¢åŠ ä¸€ä¸ªä¸´æ—¶ locationï¼Œåœ¨æ ¡éªŒçš„æ—¶å€™ Let's Encrypt ä¼šå‘é€ http è¯·æ±‚åˆ° http://\u0026lt;YOUR_DOMAIN\u0026gt;/.well-known/acme-challenge/\u0026lt;TOKEN\u0026gt;ï¼Œå…¶ä¸­ YOUR_DOMAIN å°±æ˜¯è¢«æ ¡éªŒçš„åŸŸåï¼ŒTOKEN æ˜¯ cert-manager ç”Ÿæˆçš„ä¸€ä¸ªè·¯å¾„ï¼Œå®ƒé€šè¿‡ä¿®æ”¹ Ingress è§„åˆ™æ¥å¢åŠ è¿™ä¸ªä¸´æ—¶æ ¡éªŒè·¯å¾„å¹¶æŒ‡å‘æä¾› TOKEN çš„æœåŠ¡ã€‚Let's Encrypt ä¼šå¯¹æ¯” TOKEN æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Œæ ¡éªŒæˆåŠŸåå°±ä¼šé¢å‘è¯ä¹¦äº†ï¼Œä¸è¿‡è¿™ç§æ–¹æ³•ä¸æ”¯æŒæ³›åŸŸåè¯ä¹¦ã€‚\nä½¿ç”¨ HTTP æ ¡éªŒè¿™ç§æ–¹å¼ï¼Œé¦–å…ˆéœ€è¦å°†åŸŸåè§£æé…ç½®å¥½ï¼Œä¹Ÿå°±æ˜¯éœ€è¦ä¿è¯ ACME æœåŠ¡ç«¯å¯ä»¥æ­£å¸¸è®¿é—®åˆ°ä½ çš„ HTTP æœåŠ¡ã€‚è¿™é‡Œæˆ‘ä»¬ä»¥ä¸Šé¢çš„ TODO åº”ç”¨ä¸ºä¾‹ï¼Œæˆ‘ä»¬å·²ç»å°† demo.qingyang.com åŸŸååšå¥½äº†æ­£ç¡®çš„è§£æã€‚\nç”±äº Letâ€™s Encrypt çš„ç”Ÿäº§ç¯å¢ƒæœ‰ç€ä¸¥æ ¼çš„æ¥å£è°ƒç”¨é™åˆ¶ï¼Œæ‰€ä»¥ä¸€èˆ¬æˆ‘ä»¬éœ€è¦å…ˆåœ¨ staging ç¯å¢ƒæµ‹è¯•é€šè¿‡åï¼Œå†åˆ‡æ¢åˆ°ç”Ÿäº§ç¯å¢ƒã€‚é¦–å…ˆæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå…¨å±€èŒƒå›´ staging ç¯å¢ƒä½¿ç”¨çš„ HTTP-01 æ ¡éªŒæ–¹å¼çš„è¯ä¹¦é¢å‘æœºæ„ï¼š\napiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # ACMEæœåŠ¡ç«¯åœ°å€ server: https://acme-staging-v02.api.letsencrypt.org/directory # æ³¨å†ŒACMEçš„é‚®ç®± email: ailunbolinkenasi@gmail.com # ç”¨äºå­˜æ”¾ACMEå¸å·privateKeyçš„secret privateKeySecretRef: name: example-issuer-account-key solvers: - http01: # ACMEçš„ç±»å‹ ingress: class: nginx # æŒ‡å®šingressçš„åç§° æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥ç”Ÿæˆå…è´¹è¯ä¹¦äº†ï¼Œcert-manager ç»™æˆ‘ä»¬æä¾›äº† Certificate è¿™ä¸ªç”¨äºç”Ÿæˆè¯ä¹¦çš„è‡ªå®šä¹‰èµ„æºå¯¹è±¡ï¼Œä¸è¿‡è¿™ä¸ªå¯¹è±¡éœ€è¦åœ¨ä¸€ä¸ªå…·ä½“çš„å‘½åç©ºé—´ä¸‹ä½¿ç”¨ï¼Œè¯ä¹¦æœ€ç»ˆä¼šåœ¨è¿™ä¸ªå‘½åç©ºé—´ä¸‹ä»¥ Secret çš„èµ„æºå¯¹è±¡å­˜å‚¨ã€‚æˆ‘ä»¬è¿™é‡Œæ˜¯è¦ç»“åˆ ingress-nginx ä¸€èµ·ä½¿ç”¨ï¼Œå®é™…ä¸Šæˆ‘ä»¬åªéœ€è¦ä¿®æ”¹ Ingress å¯¹è±¡ï¼Œæ·»åŠ ä¸Š cert-manager çš„ç›¸å…³æ³¨è§£å³å¯ï¼Œä¸éœ€è¦æ‰‹åŠ¨åˆ›å»º Certificate å¯¹è±¡äº†ã€‚\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: https-ingress annotations: cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-staging\u0026#34; # ä½¿ç”¨å“ªä¸ªissuer spec: ingressClassName: nginx tls: - hosts: - demo.qingyang.com secretName: demo-qingyang-com-tls # ç”¨äºå­˜å‚¨è¯ä¹¦çš„Secretå¯¹è±¡åå­— rules: - host: demo.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: vue-demo port: number: 80 åˆ›å»ºå®Œæˆåä¼šå¤šå‡ºä¸€ä¸ªingresså¯¹è±¡,ä¸»è¦æ˜¯ä¸ºäº†è®©acmeå¯ä»¥è®¿é—®åˆ°å½“å‰çš„çš„token\n[root@Online-Beijing-master1 ~]# kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE cm-acme-http-solver-xpxm7 \u0026lt;none\u0026gt; demo.qingyang.com 10.1.6.24,10.1.6.45,10.1.6.48 80 7m51s https-ingress nginx demo.qingyang.com 10.1.6.24,10.1.6.45,10.1.6.48 80, 443 7m55s å¯ä»¥æŸ¥çœ‹å½“å‰çš„acmeè®¤è¯,å…¶ä¸­/.well-known/acme-challenge/pVY-ihomZPdlWDMt44cV9qZUwMQScjHvd3Zkf_FDLRIæ˜¯è¢«éªŒè¯å¯¹è±¡\n[root@Online-Beijing-master1 ~]# kubectl describe ingress cm-acme-http-solver-xpxm7 Name: cm-acme-http-solver-xpxm7 Labels: acme.cert-manager.io/http-domain=1002178207 acme.cert-manager.io/http-token=1266355919 acme.cert-manager.io/http01-solver=true Namespace: default Address: 10.1.6.24,10.1.6.45,10.1.6.48 Ingress Class: \u0026lt;none\u0026gt; Default backend: \u0026lt;default\u0026gt; Rules: Host Path Backends ---- ---- -------- demo.qingyang.com /.well-known/acme-challenge/pVY-ihomZPdlWDMt44cV9qZUwMQScjHvd3Zkf_FDLRI cm-acme-http-solver-f4ntj:8089 (10.10.180.124:8089) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/whitelist-source-range: 0.0.0.0/0,::/0 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 7m46s (x2 over 8m13s) nginx-ingress-controller Scheduled for sync Normal Sync 7m46s (x2 over 8m13s) nginx-ingress-controller Scheduled for sync Normal Sync 7m46s (x2 over 8m13s) nginx-ingress-controller Scheduled for sync ä½ å¯ä»¥å°è¯•è®¿é—®https://demo.qingyang.com/.well-known/acme-challenge/pVY-ihomZPdlWDMt44cV9qZUwMQScjHvd3Zkf_FDLRIã€‚æ­£å¸¸ä¼šå‡ºç°å…·ä½“çš„éªŒè¯å¯†é’¥å³æˆåŠŸ.\nç”±äºæˆ‘çš„æ˜¯æœ¬åœ°è‡ªå·±æ­å»ºçš„kubernetesé›†ç¾¤,æ²¡æœ‰å¤–éƒ¨è§£æçš„è®¿é—®æƒé™æ‰€ä»¥è¿™ä¸ªåœ°æ–¹å°±æ²¡åŠæ³•ç»™å¤§å®¶æ¼”ç¤ºäº†ã€‚\nDNS-01 æ ¡éªŒ NS-01 çš„æ ¡éªŒæ˜¯é€šè¿‡ DNS æä¾›å•†çš„ API æ‹¿åˆ°ä½ çš„ DNS æ§åˆ¶æƒé™ï¼Œ åœ¨ Let's Encrypt ä¸º cert-manager æä¾› TOKEN åï¼Œcert-manager å°†åˆ›å»ºä»è¯¥ TOKEN å’Œä½ çš„å¸æˆ·å¯†é’¥æ´¾ç”Ÿçš„ TXT è®°å½•ï¼Œå¹¶å°†è¯¥è®°å½•æ”¾åœ¨ _acme-challenge.\u0026lt;YOUR_DOMAIN\u0026gt;ã€‚ç„¶å Let's Encrypt å°†å‘ DNS ç³»ç»ŸæŸ¥è¯¢è¯¥è®°å½•ï¼Œå¦‚æœæ‰¾åˆ°åŒ¹é…é¡¹ï¼Œå°±å¯ä»¥é¢å‘è¯ä¹¦ï¼Œè¿™ç§æ–¹æ³•æ˜¯æ”¯æŒæ³›åŸŸåè¯ä¹¦çš„ã€‚\nDNS-01 æ”¯æŒå¤šç§ä¸åŒçš„æœåŠ¡æä¾›å•†ï¼Œç›´æ¥åœ¨ Issuer æˆ–è€… ClusterIssuer ä¸­å¯ä»¥ç›´æ¥é…ç½®ï¼Œå¯¹äºä¸€äº›ä¸æ”¯æŒçš„ DNS æœåŠ¡æä¾›å•†å¯ä»¥ä½¿ç”¨å¤–éƒ¨ webhook æ¥æä¾›æ”¯æŒï¼Œæ¯”å¦‚é˜¿é‡Œäº‘çš„ DNS è§£æé»˜è®¤æƒ…å†µä¸‹æ˜¯ä¸æ”¯æŒçš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é˜¿é‡Œäº‘è¿™ä¸ª webhook æ¥æä¾›æ”¯æŒã€‚\nalidns-webhook å®‰è£…alidns-webhook kubectl apply -f https://raw.githubusercontent.com/pragkent/alidns-webhook/master/deploy/bundle.yaml æ¥ç€åˆ›å»ºä¸€ä¸ªåŒ…å«è®¿é—®é˜¿é‡Œäº‘ DNS è®¤è¯å¯†é’¥ä¿¡æ¯çš„ Secret å¯¹è±¡ï¼Œå¯¹åº”çš„ accessk-key å’Œ secret-key kubectl create secret generic alidns-secret --from-literal=access-key=YOUR_ACCESS_KEY --from-literal=secret-key=YOUR_SECRET_KEY -n cert-manager æ¥ä¸‹æ¥åŒæ ·é¦–å…ˆåˆ›å»ºä¸€ä¸ª staging ç¯å¢ƒçš„ DNS ç±»å‹çš„è¯ä¹¦æœºæ„èµ„æºå¯¹è±¡ apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns01 spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: beilanzhisen@163.com privateKeySecretRef: name: letsencrypt-staging-dns01 solvers: - dns01: # ACME DNS-01 ç±»å‹ webhook: groupName: acme.yourcompany.com solverName: alidns config: region: \u0026#34;\u0026#34; accessKeySecretRef: # å¼•ç”¨ ak name: alidns-secret key: access-key secretKeySecretRef: # å¼•ç”¨ sk name: alidns-secret key: secret-key æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„ ClusterIssuer å¯¹è±¡æ¥æˆ–è€…è¯ä¹¦æ•°æ®äº†ï¼Œåˆ›å»ºå¦‚ä¸‹æ‰€ç¤ºçš„ Certificate èµ„æºå¯¹è±¡\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: qingyang-com-cert spec: secretName: qingyang-com-tls commonName: \u0026#34;*.qingyang.com\u0026#34; dnsNames: - qingyang.com - \u0026#34;*.qingyang.com\u0026#34; issuerRef: name: letsencrypt-staging-dns01 kind: ClusterIssuer åæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥åœ¨ Ingress èµ„æºå¯¹è±¡ä¸­ä½¿ç”¨ä¸Šé¢çš„ Secret å¯¹è±¡äº†\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: https-ingress annotations: cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-staging\u0026#34; # ä½¿ç”¨å“ªä¸ªissuer spec: ingressClassName: nginx tls: - hosts: - \u0026#34;*.qingyang.com\u0026#34; secretName: qingyang-com-tls # ç”¨äºå­˜å‚¨è¯ä¹¦çš„Secretå¯¹è±¡åå­— rules: - host: demo.qingyang.com http: paths: - path: / pathType: Prefix backend: service: name: vue-demo port: number: 80 ","date":"2023-03-08T00:00:00Z","image":"https://d33wubrfki0l68.cloudfront.net/4f01eaec32889ff16ee255e97822b6d165b633f0/a54b4/zh-cn/docs/images/ingress.svg","permalink":"http://localhost:1313/kubernetes/ingress/","title":"Ingressçš„ç®€å•ä½¿ç”¨"},{"content":"å¦‚æœåœ¨é›†ç¾¤è§„æ¨¡è¾ƒå¤§å¹¶å‘è¾ƒé«˜çš„æƒ…å†µä¸‹æˆ‘ä»¬ä»ç„¶éœ€è¦å¯¹ DNS è¿›è¡Œä¼˜åŒ–ï¼Œå…¸å‹çš„å°±æ˜¯å¤§å®¶æ¯”è¾ƒç†Ÿæ‚‰çš„ CoreDNS ä¼šå‡ºç°è¶…æ—¶5sçš„æƒ…å†µã€‚\nè¶…æ—¶åŸå›  åœ¨ iptables æ¨¡å¼ä¸‹ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰ï¼Œæ¯ä¸ªæœåŠ¡çš„ kube-proxy åœ¨ä¸»æœºç½‘ç»œåç§°ç©ºé—´çš„ nat è¡¨ä¸­åˆ›å»ºä¸€äº› iptables è§„åˆ™ã€‚ æ¯”å¦‚åœ¨é›†ç¾¤ä¸­å…·æœ‰ä¸¤ä¸ª DNS æœåŠ¡å™¨å®ä¾‹çš„ kube-dns æœåŠ¡ï¼Œå…¶ç›¸å…³è§„åˆ™å¤§è‡´å¦‚ä¸‹æ‰€ç¤ºï¼š\n(1) -A PREROUTING -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES \u0026lt;...\u0026gt; (2) -A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment \u0026#34;kube-system/kube-dns:dns cluster IP\u0026#34; -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU \u0026lt;...\u0026gt; (3) -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment \u0026#34;kube-system/kube-dns:dns\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-LLLB6FGXBLX6PZF7 (4) -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment \u0026#34;kube-system/kube-dns:dns\u0026#34; -j KUBE-SEP-LRVEW52VMYCOUSMZ \u0026lt;...\u0026gt; (5) -A KUBE-SEP-LLLB6FGXBLX6PZF7 -p udp -m comment --comment \u0026#34;kube-system/kube-dns:dns\u0026#34; -m udp -j DNAT --to-destination 10.32.0.6:53 \u0026lt;...\u0026gt; (6) -A KUBE-SEP-LRVEW52VMYCOUSMZ -p udp -m comment --comment \u0026#34;kube-system/kube-dns:dns\u0026#34; -m udp -j DNAT --to-destination 10.32.0.7:53 æˆ‘ä»¬çŸ¥é“æ¯ä¸ª Pod çš„ /etc/resolv.conf æ–‡ä»¶ä¸­éƒ½æœ‰å¡«å……çš„ nameserver 10.96.0.10 è¿™ä¸ªæ¡ç›®ã€‚æ‰€ä»¥æ¥è‡ª Pod çš„ DNS æŸ¥æ‰¾è¯·æ±‚å°†å‘é€åˆ° 10.96.0.10ï¼Œè¿™æ˜¯ kube-dns æœåŠ¡çš„ ClusterIP åœ°å€ã€‚ ç”±äº (1) è¯·æ±‚è¿›å…¥ KUBE-SERVICE é“¾ï¼Œç„¶ååŒ¹é…è§„åˆ™ (2)ï¼Œæœ€åæ ¹æ® (3) çš„ random éšæœºæ¨¡å¼ï¼Œè·³è½¬åˆ° (5) æˆ– (6) æ¡ç›®ï¼Œå°†è¯·æ±‚ UDP æ•°æ®åŒ…çš„ç›®æ ‡ IP åœ°å€ä¿®æ”¹ä¸º DNS æœåŠ¡å™¨çš„å®é™… IP åœ°å€ï¼Œè¿™æ˜¯é€šè¿‡ DNAT å®Œæˆçš„ã€‚å…¶ä¸­ 10.32.0.6 å’Œ 10.32.0.7 æ˜¯æˆ‘ä»¬é›†ç¾¤ä¸­ CoreDNS çš„ä¸¤ä¸ª Pod å‰¯æœ¬çš„ IP åœ°å€ã€‚\nå†…æ ¸ä¸­çš„DNAT DNAT çš„ä¸»è¦èŒè´£æ˜¯åŒæ—¶æ›´æ”¹ä¼ å‡ºæ•°æ®åŒ…çš„ç›®çš„åœ°ï¼Œå“åº”æ•°æ®åŒ…çš„æºï¼Œå¹¶ç¡®ä¿å¯¹æ‰€æœ‰åç»­æ•°æ®åŒ…è¿›è¡Œç›¸åŒçš„ä¿®æ”¹ã€‚åè€…ä¸¥é‡ä¾èµ–äºè¿æ¥è·Ÿè¸ªæœºåˆ¶ï¼Œä¹Ÿç§°ä¸º conntrackï¼Œå®ƒè¢«å®ç°ä¸ºå†…æ ¸æ¨¡å—ã€‚conntrack ä¼šè·Ÿè¸ªç³»ç»Ÿä¸­æ­£åœ¨è¿›è¡Œçš„ç½‘ç»œè¿æ¥ã€‚\nconntrack ä¸­çš„æ¯ä¸ªè¿æ¥éƒ½ç”±ä¸¤ä¸ªå…ƒç»„è¡¨ç¤ºï¼Œä¸€ä¸ªå…ƒç»„ç”¨äºåŸå§‹è¯·æ±‚ï¼ˆIP_CT_DIR_ORIGINALï¼‰ï¼Œå¦ä¸€ä¸ªå…ƒç»„ç”¨äºç­”å¤ï¼ˆIP_CT_DIR_REPLYï¼‰ã€‚å¯¹äº UDPï¼Œæ¯ä¸ªå…ƒç»„éƒ½ç”±æº IP åœ°å€ï¼Œæºç«¯å£ä»¥åŠç›®æ ‡ IP åœ°å€å’Œç›®æ ‡ç«¯å£ç»„æˆï¼Œç­”å¤å…ƒç»„åŒ…å«å­˜å‚¨åœ¨src å­—æ®µä¸­çš„ç›®æ ‡çš„çœŸå®åœ°å€ã€‚\nä¾‹å¦‚ï¼Œå¦‚æœ IP åœ°å€ä¸º 10.40.0.17 çš„ Pod å‘ kube-dns çš„ ClusterIP å‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œè¯¥è¯·æ±‚è¢«è½¬æ¢ä¸º 10.32.0.6ï¼Œåˆ™å°†åˆ›å»ºä»¥ä¸‹å…ƒç»„ï¼š\nåŸå§‹ï¼šsrc = 10.40.0.17 dst = 10.96.0.10 sport = 53378 dport = 53 å›å¤ï¼šsrc = 10.32.0.6 dst = 10.40.0.17 sport = 53 dport = 53378 é€šè¿‡è¿™äº›æ¡ç›®å†…æ ¸å¯ä»¥ç›¸åº”åœ°ä¿®æ”¹ä»»ä½•ç›¸å…³æ•°æ®åŒ…çš„ç›®çš„åœ°å’Œæºåœ°å€ï¼Œè€Œæ— éœ€å†æ¬¡éå† DNAT è§„åˆ™ï¼Œæ­¤å¤–ï¼Œå®ƒå°†çŸ¥é“å¦‚ä½•ä¿®æ”¹å›å¤ä»¥åŠåº”å°†å›å¤å‘é€ç»™è°ã€‚åˆ›å»º conntrack æ¡ç›®åï¼Œå°†é¦–å…ˆå¯¹å…¶è¿›è¡Œç¡®è®¤ï¼Œç„¶åå¦‚æœæ²¡æœ‰å·²ç¡®è®¤çš„ conntrack æ¡ç›®å…·æœ‰ç›¸åŒçš„åŸå§‹å…ƒç»„æˆ–å›å¤å…ƒç»„ï¼Œåˆ™å†…æ ¸å°†å°è¯•ç¡®è®¤è¯¥æ¡ç›®ã€‚\nå…·ä½“åŸå› å¯ä»¥å‚è€ƒ weave works æ€»ç»“çš„æ–‡ç«  Racy conntrack and DNS lookup timeouts\nåªæœ‰å¤šä¸ªçº¿ç¨‹æˆ–è¿›ç¨‹ï¼Œå¹¶å‘ä»åŒä¸€ä¸ª socket å‘é€ç›¸åŒäº”å…ƒç»„çš„ UDP æŠ¥æ–‡æ—¶ï¼Œæ‰æœ‰ä¸€å®šæ¦‚ç‡ä¼šå‘ç”Ÿ glibcã€muslï¼ˆalpine linux çš„ libc åº“ï¼‰éƒ½ä½¿ç”¨ parallel query, å°±æ˜¯å¹¶å‘å‘å‡ºå¤šä¸ªæŸ¥è¯¢è¯·æ±‚ï¼Œå› æ­¤å¾ˆå®¹æ˜“ç¢°åˆ°è¿™æ ·çš„å†²çªï¼Œé€ æˆæŸ¥è¯¢è¯·æ±‚è¢«ä¸¢å¼ƒ ç”±äº ipvs ä¹Ÿä½¿ç”¨äº† conntrack, ä½¿ç”¨ kube-proxy çš„ ipvs æ¨¡å¼ï¼Œå¹¶ä¸èƒ½é¿å…è¿™ä¸ªé—®é¢˜ è§£å†³æ–¹æ³• è¦å½»åº•è§£å†³è¿™ä¸ªé—®é¢˜æœ€å¥½å½“ç„¶æ˜¯å†…æ ¸ä¸Šå» FIX æ‰è¿™ä¸ª BUGï¼Œé™¤äº†è¿™ç§æ–¹æ³•ä¹‹å¤–æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨å…¶ä»–æ–¹æ³•æ¥è¿›è¡Œè§„é¿ï¼Œæˆ‘ä»¬å¯ä»¥é¿å…ç›¸åŒäº”å…ƒç»„ DNSè¯·æ±‚çš„å¹¶å‘ã€‚\nåœ¨ resolv.conf ä¸­å°±æœ‰ä¸¤ä¸ªç›¸å…³çš„å‚æ•°å¯ä»¥è¿›è¡Œé…ç½®ï¼š\nsingle-request-reopenï¼šå‘é€ A ç±»å‹è¯·æ±‚å’Œ AAAA ç±»å‹è¯·æ±‚ä½¿ç”¨ä¸åŒçš„æºç«¯å£ï¼Œè¿™æ ·ä¸¤ä¸ªè¯·æ±‚åœ¨ conntrack è¡¨ä¸­ä¸å ç”¨åŒä¸€ä¸ªè¡¨é¡¹ï¼Œä»è€Œé¿å…å†²çªã€‚ single-requestï¼šé¿å…å¹¶å‘ï¼Œæ”¹ä¸ºä¸²è¡Œå‘é€ A ç±»å‹å’Œ AAAA ç±»å‹è¯·æ±‚ã€‚æ²¡æœ‰äº†å¹¶å‘ï¼Œä»è€Œä¹Ÿé¿å…äº†å†²çªã€‚ Pod çš„ postStart hook ä¸­æ·»åŠ  lifecycle: postStart: exec: command: - /bin/sh - -c - \u0026#34;/bin/echo \u0026#39;options single-request-reopen\u0026#39; \u0026gt;\u0026gt; /etc/resolv.conf\u0026#34; ä½¿ç”¨ template.spec.dnsConfig é…ç½® template: spec: dnsConfig: options: - name: single-request-reopen\tä½¿ç”¨ ConfigMap è¦†ç›– Pod é‡Œé¢çš„ /etc/resolv.conf # configmap apiVersion: v1 data: resolv.conf: | nameserver 1.2.3.4 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 single-request-reopen timeout:1 kind: ConfigMap metadata: name: resolvconf --- # Pod Spec spec: volumeMounts: - name: resolv-conf mountPath: /etc/resolv.conf subPath: resolv.conf # åœ¨æŸä¸ªç›®å½•ä¸‹é¢æŒ‚è½½ä¸€ä¸ªæ–‡ä»¶ï¼ˆä¿è¯ä¸è¦†ç›–å½“å‰ç›®å½•ï¼‰éœ€è¦ä½¿ç”¨subPath -\u0026gt; ä¸æ”¯æŒçƒ­æ›´æ–° ... volumes: - name: resolv-conf configMap: name: resolvconf items: - key: resolv.conf path: resolv.conf NodeLocal DNSCache NodeLocal DNSCache é€šè¿‡åœ¨é›†ç¾¤èŠ‚ç‚¹ä¸Šä½œä¸º DaemonSet è¿è¡Œ DNS ç¼“å­˜ä»£ç†æ¥æé«˜é›†ç¾¤ DNS æ€§èƒ½ã€‚ åœ¨å½“ä»Šçš„ä½“ç³»ç»“æ„ä¸­ï¼Œè¿è¡Œåœ¨ ClusterFirst DNS æ¨¡å¼ä¸‹çš„ Pod å¯ä»¥è¿æ¥åˆ° kube-dns serviceIP è¿›è¡Œ DNS æŸ¥è¯¢ã€‚ é€šè¿‡ kube-proxy æ·»åŠ çš„ iptables è§„åˆ™å°†å…¶è½¬æ¢ä¸º kube-dns/CoreDNS ç«¯ç‚¹ã€‚ å€ŸåŠ©è¿™ç§æ–°æ¶æ„ï¼ŒPod å°†å¯ä»¥è®¿é—®åœ¨åŒä¸€èŠ‚ç‚¹ä¸Šè¿è¡Œçš„ DNS ç¼“å­˜ä»£ç†ï¼Œä»è€Œé¿å… iptables DNAT è§„åˆ™å’Œè¿æ¥è·Ÿè¸ªã€‚ æœ¬åœ°ç¼“å­˜ä»£ç†å°†æŸ¥è¯¢ kube-dns æœåŠ¡ä»¥è·å–é›†ç¾¤ä¸»æœºåçš„ç¼“å­˜ç¼ºå¤±ï¼ˆé»˜è®¤ä¸º â€œcluster.localâ€ åç¼€ï¼‰ã€‚\nåŠ¨æœº ä½¿ç”¨å½“å‰çš„ DNS ä½“ç³»ç»“æ„ï¼Œå¦‚æœæ²¡æœ‰æœ¬åœ° kube-dns/CoreDNS å®ä¾‹ï¼Œåˆ™å…·æœ‰æœ€é«˜ DNS QPS çš„ Pod å¯èƒ½å¿…é¡»å»¶ä¼¸åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹ã€‚ åœ¨è¿™ç§åœºæ™¯ä¸‹ï¼Œæ‹¥æœ‰æœ¬åœ°ç¼“å­˜å°†æœ‰åŠ©äºæ”¹å–„å»¶è¿Ÿã€‚ è·³è¿‡ iptables DNAT å’Œè¿æ¥è·Ÿè¸ªå°†æœ‰åŠ©äºå‡å°‘ conntrack ç«äº‰å¹¶é¿å… UDP DNS æ¡ç›®å¡«æ»¡ conntrack è¡¨ã€‚ ä»æœ¬åœ°ç¼“å­˜ä»£ç†åˆ° kube-dns æœåŠ¡çš„è¿æ¥å¯ä»¥å‡çº§ä¸º TCPã€‚ TCP conntrack æ¡ç›®å°†åœ¨è¿æ¥å…³é—­æ—¶è¢«åˆ é™¤ï¼Œç›¸å UDP æ¡ç›®å¿…é¡»è¶…æ—¶ ï¼ˆé»˜è®¤ nf_conntrack_udp_timeout æ˜¯ 30 ç§’ï¼‰ã€‚ å°† DNS æŸ¥è¯¢ä» UDP å‡çº§åˆ° TCP å°†å‡å°‘ç”±äºè¢«ä¸¢å¼ƒçš„ UDP åŒ…å’Œ DNS è¶…æ—¶è€Œå¸¦æ¥çš„å°¾éƒ¨ç­‰å¾…æ—¶é—´ï¼› è¿™ç±»å»¶æ—¶é€šå¸¸é•¿è¾¾ 30 ç§’ï¼ˆ3 æ¬¡é‡è¯• + 10 ç§’è¶…æ—¶ï¼‰ã€‚ ç”±äº nodelocal ç¼“å­˜ç›‘å¬ UDP DNS æŸ¥è¯¢ï¼Œåº”ç”¨ä¸éœ€è¦å˜æ›´ã€‚ åœ¨èŠ‚ç‚¹çº§åˆ«å¯¹ DNS è¯·æ±‚çš„åº¦é‡å’Œå¯è§æ€§ã€‚ å¯ä»¥é‡æ–°å¯ç”¨è´Ÿç¼“å­˜ï¼Œä»è€Œå‡å°‘å¯¹ kube-dns æœåŠ¡çš„æŸ¥è¯¢æ•°é‡ã€‚ å·¥ä½œåŸç†å¦‚ä¸‹\næ­¤å›¾æ˜¾ç¤ºäº† NodeLocal DNSCache å¦‚ä½•å¤„ç† DNS æŸ¥è¯¢\nå®‰è£…NodeLocalDNS ç›´æ¥ä»å®˜æ–¹çš„èµ„æºæ¸…å•å½“ä¸­è·å–å³å¯\nimageï¼šé»˜è®¤é•œåƒå›½å†…æ˜¯ä¸‹è½½ä¸äº†çš„è¯·æ›´æ¢åœ°å€ wget https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # ä¸‹è½½å®Œæˆè¯·æ›´æ¢imageåœ°å€ï¼š registry.cn-beijing.aliyuncs.com/custom_img/k8s-dns-node-cache:1.22.18 æ³¨æ„èµ„æºæ¸…å•ä¸­çš„å‡ ä¸ªå˜é‡ä¿¡æ¯\n__PILLAR__DNS__SERVER__ï¼šè¡¨ç¤º kube-dns è¿™ä¸ª Service çš„ ClusterIPã€‚ __PILLAR__LOCAL__DNS__: è¡¨ç¤º DNSCache æœ¬åœ°çš„ IPï¼Œé»˜è®¤ä¸º 169.254.20.10 __PILLAR__DNS__DOMAIN__: è¡¨ç¤ºé›†ç¾¤åŸŸï¼Œé»˜è®¤å°±æ˜¯ cluster.local # é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œè·å– kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP} # ä¿®æ”¹éƒ¨åˆ†å˜é‡ä¿¡æ¯ sed -i \u0026#39;s/__PILLAR__DNS__SERVER__/10.10.0.10/g s/__PILLAR__LOCAL__DNS__/169.254.20.10/g s/__PILLAR__DNS__DOMAIN__/cluster.local/g\u0026#39; nodelocaldns.yaml # åˆ›å»ºèµ„æºé…ç½®æ¸…å• kubectl apply -f nodelocaldns.yaml å¦‚æœ kube-proxy è¿è¡Œåœ¨ IPVS æ¨¡å¼(å› ä¸ºæˆ‘æ˜¯ipvsçš„æ¨¡å¼) sed -i \u0026#34;s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/$kubedns/g\u0026#34; nodelocaldns.yaml åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œnode-local-dns Pod åªä¼šä¾¦å¬ \u0026lt;node-local-address\u0026gt; çš„åœ°å€ã€‚ node-local-dns æ¥å£ä¸èƒ½ç»‘å®š kube-dns çš„é›†ç¾¤ IP åœ°å€ï¼Œå› ä¸º IPVS è´Ÿè½½å‡è¡¡ä½¿ç”¨çš„æ¥å£å·²ç»å ç”¨äº†è¯¥åœ°å€ã€‚ node-local-dns Pod ä¼šè®¾ç½® __PILLAR__UPSTREAM__SERVERS__ã€‚\næŸ¥çœ‹Podæ˜¯å¦è¿è¡ŒæˆåŠŸ\n[root@Online-Beijing-master1 ~]# kubectl get pods -n kube-system | grep node-local-dns node-local-dns-578vf 1/1 Running 0 5m23s node-local-dns-5jhcl 1/1 Running 0 5m23s node-local-dns-8hz5j 1/1 Running 0 5m23s node-local-dns-ch44w 1/1 Running 0 5m23s node-local-dns-jbg2p 1/1 Running 0 5m23s node-local-dns-t92ww 1/1 Running 0 5m23s å¦‚æœ kube-proxy ç»„ä»¶ä½¿ç”¨çš„æ˜¯ ipvs æ¨¡å¼çš„è¯æˆ‘ä»¬è¿˜éœ€è¦ä¿®æ”¹ kubelet çš„ --cluster-dns å‚æ•°ï¼Œå°†å…¶æŒ‡å‘ 169.254.20.10ï¼ŒDaemonset ä¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºä¸€ä¸ªç½‘å¡æ¥ç»‘è¿™ä¸ª IPï¼ŒPod å‘æœ¬èŠ‚ç‚¹è¿™ä¸ª IP å‘ DNS è¯·æ±‚ï¼Œç¼“å­˜æ²¡æœ‰å‘½ä¸­çš„æ—¶å€™æ‰ä¼šå†ä»£ç†åˆ°ä¸Šæ¸¸é›†ç¾¤ DNS è¿›è¡ŒæŸ¥è¯¢ã€‚\nå¦‚æœæ‹…å¿ƒçº¿ä¸Šç¯å¢ƒä¿®æ”¹ --cluster-dns å‚æ•°ä¼šäº§ç”Ÿå½±å“ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥åœ¨æ–°éƒ¨ç½²çš„ Pod ä¸­é€šè¿‡ dnsConfig é…ç½®ä½¿ç”¨æ–°çš„ localdns çš„åœ°å€æ¥è¿›è¡Œè§£æã€‚\né€šè¿‡ä¿®æ”¹--cluster-dnså®ç° # 1. é¦–å…ˆæŸ¥çœ‹å½“å‰çš„proxyæ¨¡å¼ [root@Online-Beijing-master1 ~]# kubectl get cm kube-proxy -n kube-system -o yaml | grep mode mode: \u0026#34;ipvs\u0026#34; sed -i \u0026#39;s/10.10.0.10/169.254.20.10/g\u0026#39; /var/lib/kubelet/config.yaml systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet Podä¸­é€šè¿‡dnsConfigé…ç½®ä½¿ç”¨localdns dnsConfig: nameservers: - 169.254.20.10 searches: - default.svc.cluster.local - svc.cluster.local - cluster.local options: - name: ndots value: \u0026#39;3\u0026#39; dnsPolicy: None ç”±äºæŒ‡å®šnameserverså±äºappendæ“ä½œï¼Œå¦‚æœéœ€è¦å¿½ç•¥åŸæ¥çš„dnsåœ°å€è¯·ä½¿ç”¨dnsPolicy: None\nCoreDnsçš„æ€§èƒ½ä¼˜åŒ– åˆç†æ§åˆ¶CoreDNSçš„å‰¯æœ¬æ•°é‡ kubectl -n kube-system scale --replicas=10 deployment/coredns ä¸º coredns å®šä¹‰ HPA è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚ å®‰è£… cluster-proportional-autoscaler ä»¥å®ç°æ›´ç²¾ç¡®çš„æ‰©ç¼©å®¹(æ¨è)ã€‚ ç¦ç”¨IPv6 å¦‚æœ K8S èŠ‚ç‚¹æ²¡æœ‰ç¦ç”¨ IPV6 çš„è¯ï¼Œå®¹å™¨å†…è¿›ç¨‹è¯·æ±‚ coredns æ—¶çš„é»˜è®¤è¡Œä¸ºæ˜¯åŒæ—¶å‘èµ· IPV4 å’Œ IPV6 è§£æï¼Œè€Œé€šå¸¸æˆ‘ä»¬åªéœ€è¦ç”¨åˆ° IPV4ï¼Œå½“å®¹å™¨è¯·æ±‚æŸä¸ªåŸŸåæ—¶ï¼Œcoredns è§£æä¸åˆ° IPV6 è®°å½•ï¼Œå°±ä¼š forward åˆ° upstream å»è§£æï¼Œå¦‚æœåˆ° upstream éœ€è¦ç»è¿‡è¾ƒé•¿æ—¶é—´(æ¯”å¦‚è·¨å…¬ç½‘ï¼Œè·¨æœºæˆ¿ä¸“çº¿)ï¼Œå°±ä¼šæ‹–æ…¢æ•´ä¸ªè§£ææµç¨‹çš„é€Ÿåº¦ï¼Œä¸šåŠ¡å±‚é¢å°±ä¼šæ„ŸçŸ¥ DNS è§£ææ…¢ã€‚\nkubectl edit cm coredns -n kube-system Corefileä¸­æ·»åŠ ç¦ç”¨IPv6\napiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } # æ·»åŠ æ­¤å†…å®¹ template ANY AAAA { rcode NXDOMAIN } ... } ä¼˜åŒ–ndots é»˜è®¤æƒ…å†µä¸‹ï¼ŒKubernetes é›†ç¾¤ä¸­çš„åŸŸåè§£æå¾€å¾€éœ€è¦ç»è¿‡å¤šæ¬¡è¯·æ±‚æ‰èƒ½è§£æåˆ°ã€‚æŸ¥çœ‹ pod å†… çš„ /etc/resolv.conf å¯ä»¥çŸ¥é“ ndots é€‰é¡¹é»˜è®¤ä¸º 5\nroot@nginxv1-56f77cbc67-4v4fp:/# cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local nameserver 10.10.0.10 options ndots:5 æ„æ€æ˜¯: å¦‚æœåŸŸåä¸­ . çš„æ•°é‡å°äº 5ï¼Œå°±ä¾æ¬¡éå† search ä¸­çš„åç¼€å¹¶æ‹¼æ¥ä¸Šè¿›è¡Œ DNS æŸ¥è¯¢ã€‚\nä¸¾ä¸ªä¾‹å­ï¼Œåœ¨ debug å‘½åç©ºé—´æŸ¥è¯¢ kubernetes.default.svc.cluster.local è¿™ä¸ª service:\nåŸŸåä¸­æœ‰4ä¸ª.å°äº5å°è¯•æ‹¼æ¥ä¸Šç¬¬ä¸€ä¸ª search è¿›è¡ŒæŸ¥è¯¢,ä¹Ÿå°±æ˜¯æŸ¥è¯¢å³kubernetes.default.svc.cluster.local.debug.svc.cluster.localæŸ¥ä¸åˆ°è¯¥åŸŸåã€‚ ç»§ç»­å°è¯• kubernetes.default.svc.cluster.local.svc.cluster.localï¼ŒæŸ¥ä¸åˆ°è¯¥åŸŸåã€‚ ç»§ç»­å°è¯• kubernetes.default.svc.cluster.local.cluster.localï¼Œä»ç„¶æŸ¥ä¸åˆ°è¯¥åŸŸåã€‚ å°è¯•ä¸åŠ åç¼€ï¼Œå³ kubernetes.default.svc.cluster.localï¼ŒæŸ¥è¯¢æˆåŠŸï¼Œè¿”å›å“åº”çš„ ClusterIPã€‚ å¯ä»¥çœ‹åˆ°ä¸€ä¸ªç®€å•çš„ service åŸŸåè§£æéœ€è¦ç»è¿‡ 4 è½®è§£ææ‰èƒ½æˆåŠŸï¼Œé›†ç¾¤ä¸­å……æ–¥ç€å¤§é‡æ— ç”¨çš„ DNS è¯·æ±‚ã€‚\næˆ‘ä»¬å¯ä»¥è®¾ç½®è¾ƒå°çš„ ndotsï¼Œåœ¨ Pod çš„ dnsConfig ä¸­å¯ä»¥è®¾ç½®\nspec: containers: - name: nginxv1 image: nginx:latest resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: Always securityContext: privileged: false # åŠ å…¥dnsConfigè¿›è¡Œè®¾ç½® dnsConfig: options: - name: ndots value: \u0026#34;2\u0026#34; ç„¶åä¸šåŠ¡å‘è¯·æ±‚æ—¶å°½é‡å°† service åŸŸåæ‹¼å®Œæ•´ï¼Œè¿™æ ·å°±ä¸ä¼šç»è¿‡ search æ‹¼æ¥é€ æˆå¤§é‡å¤šä½™çš„ DNS è¯·æ±‚ã€‚\nä¸è¿‡è¿™æ ·ä¼šæ¯”è¾ƒéº»çƒ¦ï¼Œæœ‰æ²¡æœ‰æ›´å¥½çš„åŠæ³•å‘¢ï¼Ÿæœ‰çš„ï¼è¯·çœ‹ä¸‹é¢çš„ autopath æ–¹å¼ã€‚\nå¯ç”¨autopath å¯ç”¨ CoreDNS çš„ autopath æ’ä»¶å¯ä»¥é¿å…æ¯æ¬¡åŸŸåè§£æç»è¿‡å¤šæ¬¡è¯·æ±‚æ‰èƒ½è§£æåˆ°ï¼ŒåŸç†æ˜¯ CoreDNS æ™ºèƒ½è¯†åˆ«æ‹¼æ¥è¿‡ search çš„ DNS è§£æï¼Œç›´æ¥å“åº” CNAME å¹¶é™„ä¸Šç›¸åº”çš„ ClusterIPï¼Œä¸€æ­¥åˆ°ä½ï¼Œå¯ä»¥æå¤§å‡å°‘é›†ç¾¤å†… DNS è¯·æ±‚æ•°é‡ã€‚\nkubectl -n kube-system edit configmap coredns { \u0026#34;Corefile\u0026#34;: \u0026#34;.:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure # ä¿®æ”¹ä¸º pods verified fallthrough in-addr.arpa ip6.arpa ttl 30 } autopath @kubernetes # æ·»åŠ autopath @kubernetes prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } template ANY AAAA { rcode NXDOMAIN } cache 30 loop reload loadbalance } } éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯ç”¨ autopath åï¼Œç”±äº coredns éœ€è¦ watch æ‰€æœ‰çš„ podï¼Œä¼šå¢åŠ  coredns çš„å†…å­˜æ¶ˆè€—ï¼Œæ ¹æ®æƒ…å†µé€‚å½“è°ƒèŠ‚ coredns çš„ memory request å’Œ limitã€‚\næœ‰å…´è¶£çš„å¯ä»¥å»çœ‹çœ‹è¿™ç¯‡æ–‡ç« ï¼šè¯¦è§£DNSå’ŒCoreDNS ","date":"2023-02-26T00:00:00Z","image":"https://d33wubrfki0l68.cloudfront.net/bf8e5eaac697bac89c5b36a0edb8855c860bfb45/6944f/images/docs/nodelocaldns.svg","permalink":"http://localhost:1313/kubernetes/nodelocaldns/","title":"CacheDNSå’ŒDNSç¼“å­˜"},{"content":"ä½¿ç”¨Kubeadmåˆ›å»ºä¸€ä¸ªé«˜å¯ç”¨çš„Etcdé›†ç¾¤ é»˜è®¤æƒ…å†µä¸‹ï¼Œkubeadm åœ¨æ¯ä¸ªæ§åˆ¶å¹³é¢èŠ‚ç‚¹ä¸Šè¿è¡Œä¸€ä¸ªæœ¬åœ° etcd å®ä¾‹ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨å¤–éƒ¨çš„ etcd é›†ç¾¤ï¼Œå¹¶åœ¨ä¸åŒçš„ä¸»æœºä¸Šæä¾› etcd å®ä¾‹ã€‚ è¿™ä¸¤ç§æ–¹æ³•çš„åŒºåˆ«åœ¨ é«˜å¯ç”¨æ‹“æ‰‘çš„é€‰é¡¹ é¡µé¢ä¸­é˜è¿°ã€‚\nè¿™ä¸ªä»»åŠ¡å°†æŒ‡å¯¼ä½ åˆ›å»ºä¸€ä¸ªç”±ä¸‰ä¸ªæˆå‘˜ç»„æˆçš„é«˜å¯ç”¨å¤–éƒ¨ etcd é›†ç¾¤ï¼Œè¯¥é›†ç¾¤åœ¨åˆ›å»ºè¿‡ç¨‹ä¸­å¯è¢« kubeadm ä½¿ç”¨ã€‚\nå‡†å¤‡å¼€å§‹ ä¸‰ä¸ªå¯ä»¥é€šè¿‡ 2379 å’Œ 2380 ç«¯å£ç›¸äº’é€šä¿¡çš„ä¸»æœºã€‚æœ¬æ–‡æ¡£ä½¿ç”¨è¿™äº›ä½œä¸ºé»˜è®¤ç«¯å£ã€‚ä¸è¿‡ï¼Œå®ƒä»¬å¯ä»¥é€šè¿‡ kubeadm çš„é…ç½®æ–‡ä»¶è¿›è¡Œè‡ªå®šä¹‰ã€‚ æ¯ä¸ªä¸»æœºå¿…é¡»å®‰è£… systemd å’Œ bash å…¼å®¹çš„ shellã€‚ æ¯å°ä¸»æœºå¿…é¡»å®‰è£…æœ‰å®¹å™¨è¿è¡Œæ—¶ã€kubelet å’Œ kubeadm æ¯ä¸ªä¸»æœºéƒ½åº”è¯¥èƒ½å¤Ÿè®¿é—® Kubernetes å®¹å™¨é•œåƒä»“åº“ (registry.k8s.io)ï¼Œ æˆ–è€…ä½¿ç”¨ kubeadm config images list/pull åˆ—å‡º/æ‹‰å–æ‰€éœ€çš„ etcd é•œåƒã€‚ æœ¬æŒ‡å—å°†æŠŠ etcd å®ä¾‹è®¾ç½®ä¸ºç”± kubelet ç®¡ç†çš„é™æ€ Podã€‚ ä¸€äº›å¯ä»¥ç”¨æ¥åœ¨ä¸»æœºé—´å¤åˆ¶æ–‡ä»¶çš„åŸºç¡€è®¾æ–½ã€‚ä¾‹å¦‚ ssh å’Œ scp å°±å¯ä»¥æ»¡è¶³éœ€æ±‚ã€‚ æœ¬æ¬¡å®¹å™¨è¿è¡Œæ—¶é‡‡ç”¨Containerdä½œä¸ºRuntime\nå°†Kubeleté…ç½®ä¸ºEtcdçš„æœåŠ¡å¯åŠ¨ç®¡ç†å™¨ ä½ å¿…é¡»åœ¨è¦è¿è¡Œ etcd çš„æ‰€æœ‰ä¸»æœºä¸Šæ‰§è¡Œæ­¤æ“ä½œã€‚\ncat \u0026lt;\u0026lt; EOF \u0026gt; /usr/lib/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock Restart=always EOF å¯åŠ¨kubelet\nsystemctl daemon-reload systemctl restart kubelet æ³¨æ„: è¯·æ‰§è¡Œå®Œæ¯•ååŠ¡å¿…ç¡®ä¿kubeletå¤„äºrunningçŠ¶æ€ã€‚\nä¸ºKubeadmåˆ›å»ºé…ç½®æ–‡ä»¶ # ä½¿ç”¨ä½ çš„ä¸»æœº IP æ›¿æ¢ HOST0ã€HOST1 å’Œ HOST2 çš„ IP åœ°å€ export HOST0=10.1.6.48 export HOST1=10.1.6.24 export HOST2=10.1.6.45 # ä½¿ç”¨ä½ çš„ä¸»æœºåæ›´æ–° NAME0ã€NAME1 å’Œ NAME2 export NAME0=\u0026#34;containerd-master1\u0026#34; export NAME1=\u0026#34;containerd-master2\u0026#34; export NAME2=\u0026#34;containerd-master3\u0026#34; # åˆ›å»ºä¸´æ—¶ç›®å½•æ¥å­˜å‚¨å°†è¢«åˆ†å‘åˆ°å…¶å®ƒä¸»æœºä¸Šçš„æ–‡ä»¶ mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ HOSTS=(${HOST0} ${HOST1} ${HOST2}) NAMES=(${NAME0} ${NAME1} ${NAME2}) for i in \u0026#34;${!HOSTS[@]}\u0026#34;; do HOST=${HOSTS[$i]} NAME=${NAMES[$i]} cat \u0026lt;\u0026lt; EOF \u0026gt; /tmp/${HOST}/kubeadmcfg.yaml --- apiVersion: \u0026#34;kubeadm.k8s.io/v1beta3\u0026#34; kind: InitConfiguration nodeRegistration: name: ${NAME} localAPIEndpoint: advertiseAddress: ${HOST} --- apiVersion: \u0026#34;kubeadm.k8s.io/v1beta3\u0026#34; kind: ClusterConfiguration etcd: local: dataDir: /var/lib/etcds serverCertSANs: - \u0026#34;${HOST}\u0026#34; peerCertSANs: - \u0026#34;${HOST}\u0026#34; extraArgs: initial-cluster: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 imageRepository: registry.aliyuncs.com/google_containers EOF done ç”Ÿæˆè¯ä¹¦é¢å‘æœºæ„ å¦‚æœä½ è¿˜æ²¡æœ‰ CAï¼Œåˆ™åœ¨ $HOST0ï¼ˆä½ ä¸º kubeadm ç”Ÿæˆé…ç½®æ–‡ä»¶çš„ä½ç½®ï¼‰ä¸Šè¿è¡Œæ­¤å‘½ä»¤ã€‚\nkubeadm init phase certs etcd-ca è¿™ä¸€æ“ä½œå°†ä¼šç”Ÿæˆ /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key ä¸ºæ¯ä¸ªæˆå‘˜åˆ›å»ºè¯ä¹¦ kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST2}/ # æ¸…ç†ä¸å¯é‡å¤ä½¿ç”¨çš„è¯ä¹¦ find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST1}/ find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete # HOST0ä¸éœ€è¦è¿›è¡Œç§»åŠ¨ kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml å¤åˆ¶è¯ä¹¦å’Œ kubeadm é…ç½® scp -r /tmp/${HOST1}/* root@${HOST1}: scp -r /tmp/${HOST2}/* root@${HOST2}: chown -R root:root pki/ mv pki /etc/kubernetes/ è¯·æ£€æŸ¥è¯ä¹¦æ–‡ä»¶æ˜¯å¦éƒ½å­˜åœ¨ æ£€æŸ¥$HOST0\n[root@containerd-master1 ~]# tree /etc/kubernetes/pki/ /etc/kubernetes/pki/ â”œâ”€â”€ apiserver-etcd-client.crt â”œâ”€â”€ apiserver-etcd-client.key â””â”€â”€ etcd â”œâ”€â”€ ca.crt â”œâ”€â”€ ca.key â”œâ”€â”€ healthcheck-client.crt â”œâ”€â”€ healthcheck-client.key â”œâ”€â”€ peer.crt â”œâ”€â”€ peer.key â”œâ”€â”€ server.crt â””â”€â”€ server.key 1 directory, 10 files æ£€æŸ¥$HOST1\n[root@containerd-master2 ~]# tree /etc/kubernetes/pki/ /etc/kubernetes/pki/ â”œâ”€â”€ apiserver-etcd-client.crt â”œâ”€â”€ apiserver-etcd-client.key â””â”€â”€ etcd â”œâ”€â”€ ca.crt â”œâ”€â”€ healthcheck-client.crt â”œâ”€â”€ healthcheck-client.key â”œâ”€â”€ peer.crt â”œâ”€â”€ peer.key â”œâ”€â”€ server.crt â””â”€â”€ server.key 1 directory, 9 files æ£€æŸ¥$HOST2\n[root@containerd-master3 ~]# tree /etc/kubernetes/pki/ /etc/kubernetes/pki/ â”œâ”€â”€ apiserver-etcd-client.crt â”œâ”€â”€ apiserver-etcd-client.key â””â”€â”€ etcd â”œâ”€â”€ ca.crt â”œâ”€â”€ healthcheck-client.crt â”œâ”€â”€ healthcheck-client.key â”œâ”€â”€ peer.crt â”œâ”€â”€ peer.key â”œâ”€â”€ server.crt â””â”€â”€ server.key 1 directory, 9 files åˆ›å»ºEtcdçš„Podæ¸…å• è¯·åœ¨$HOST0è¿›è¡Œæ‰§è¡Œ\nkubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml è¯·åœ¨$HOST1è¿›è¡Œæ‰§è¡Œ\nkubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml è¯·åœ¨$HOST2è¿›è¡Œæ‰§è¡Œ\nkubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml æ£€æŸ¥Etcdçš„Podæ˜¯å¦è¿è¡Œ ä¸‰å°Etcdä¸»æœºå…¨éƒ¨ä½¿ç”¨crictl ps -a è¿›è¡ŒæŸ¥çœ‹EtcdPodæ˜¯å¦å¤„äºrunningçŠ¶æ€ [root@containerd-master1 ~]# crictl ps -a CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID POD 0a183925d2542 0048118155842 52 seconds ago Running ","date":"2023-02-26T00:00:00Z","image":"https://img.linux.net.cn/data/attachment/album/201501/29/141718izklanww82qm888k.png","permalink":"http://localhost:1313/kubernetes/InstallEtcdHA/","title":"ä½¿ç”¨Kubeadmåˆ›å»ºä¸€ä¸ªé«˜å¯ç”¨çš„ETCDé›†ç¾¤"},{"content":"ConfigMap ConfigMap æ˜¯ä¸€ç§ API å¯¹è±¡ï¼Œç”¨æ¥å°†éæœºå¯†æ€§çš„æ•°æ®ä¿å­˜åˆ°é”®å€¼å¯¹ä¸­ã€‚ä½¿ç”¨æ—¶ï¼Œ Pods å¯ä»¥å°†å…¶ç”¨ä½œç¯å¢ƒå˜é‡ã€å‘½ä»¤è¡Œå‚æ•°æˆ–è€…å­˜å‚¨å·ä¸­çš„é…ç½®æ–‡ä»¶ã€‚\nConfigMap å°†ä½ çš„ç¯å¢ƒé…ç½®ä¿¡æ¯å’Œ å®¹å™¨é•œåƒ è§£è€¦ï¼Œä¾¿äºåº”ç”¨é…ç½®çš„ä¿®æ”¹ã€‚ ConfigMap åœ¨è®¾è®¡ä¸Šä¸æ˜¯ç”¨æ¥ä¿å­˜å¤§é‡æ•°æ®çš„ã€‚åœ¨ ConfigMap ä¸­ä¿å­˜çš„æ•°æ®ä¸å¯è¶…è¿‡1MiB(è¿™å…¶å®æ˜¯ETCDçš„è¦æ±‚å“ˆå“ˆå“ˆ)ã€‚å¦‚æœä½ éœ€è¦ä¿å­˜è¶…å‡ºæ­¤å°ºå¯¸é™åˆ¶çš„æ•°æ®ï¼Œä½ å¯èƒ½å¸Œæœ›è€ƒè™‘æŒ‚è½½å­˜å‚¨å· æˆ–è€…ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“æˆ–è€…æ–‡ä»¶æœåŠ¡ã€‚\nè¿™æ˜¯ä¸€ä¸ª ConfigMap çš„ç¤ºä¾‹ï¼Œå®ƒçš„ä¸€äº›é”®åªæœ‰ä¸€ä¸ªå€¼ï¼Œå…¶ä»–é”®çš„å€¼çœ‹èµ·æ¥åƒæ˜¯ é…ç½®çš„ç‰‡æ®µæ ¼å¼ã€‚\né€šè¿‡Keyå’ŒValueè¿™ç§é”®å€¼å¯¹æ¥è¿›è¡Œå†™å…¥æ•°æ® apiVersion: v1 kind: ConfigMap metadata: name: game-demo data: # ç±»å±æ€§é”®ï¼›æ¯ä¸€ä¸ªé”®éƒ½æ˜ å°„åˆ°ä¸€ä¸ªç®€å•çš„å€¼ player_initial_lives: \u0026#34;3\u0026#34; ui_properties_file_name: \u0026#34;user-interface.properties\u0026#34; # ç±»æ–‡ä»¶é”®,ä¸€èˆ¬ç”¨æ¥ä¿å­˜ä¸€ä¸ªæ–‡ä»¶åˆ°æŒ‡å®šç›®å½• game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true ä½ å¯ä»¥ä½¿ç”¨å››ç§æ–¹å¼æ¥ä½¿ç”¨ ConfigMap é…ç½® Pod ä¸­çš„å®¹å™¨ï¼š\nåœ¨å®¹å™¨å‘½ä»¤å’Œå‚æ•°å†… å®¹å™¨çš„ç¯å¢ƒå˜é‡ åœ¨åªè¯»å·é‡Œé¢æ·»åŠ ä¸€ä¸ªæ–‡ä»¶ï¼Œè®©åº”ç”¨æ¥è¯»å– ç¼–å†™ä»£ç åœ¨ Pod ä¸­è¿è¡Œï¼Œä½¿ç”¨ Kubernetes API æ¥è¯»å– ConfigMap é€šè¿‡ç¯å¢ƒå˜é‡çš„æ–¹å¼ä½¿ç”¨ConfigMap é¦–å…ˆæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªDeploymentç„¶åé€šè¿‡Envç¯å¢ƒå˜é‡çš„æ–¹å¼è¿›è¡Œä½¿ç”¨ConfigMap\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-web-beijing namespace: default labels: k8s-app: nginx-web zone: beijing spec: replicas: 1 selector: matchLabels: k8s-app: nginx-web template: metadata: name: nginx-web-beijing labels: k8s-app: nginx-web spec: containers: - name: vue-shop-beijing image: nginx:latest resources: requests: memory: 100Mi cpu: 10m # é€šè¿‡ç¯å¢ƒå˜é‡çš„æ–¹å¼è¿›è¡ŒæŒ‚è½½ env: - name: PLAYER_INITIAL_LIVES valueFrom: configMapKeyRef: name: game-demo # è¡¨ç¤ºå½“å‰é”®æ¥è‡ªgame-demoè¿™ä¸ªConfigMap key: player_initial_lives # è¡¨ç¤ºå–player_initial_livesè¿™ä¸ªé”®çš„å†…å®¹ ç„¶åæˆ‘ä»¬å¯ä»¥è¿›å…¥åˆ°Podå†…éƒ¨è¿›è¡ŒechoæŒ‚è½½çš„å˜é‡åæŸ¥çœ‹æ˜¯å¦æœ‰è¾“å‡º\nroot@nginx-web-beijing-d6d994854-d6tjk:/# echo $PLAYER_INITIAL_LIVES 3 å°†ConfigMapå½“åšæ–‡ä»¶ä½¿ç”¨ åˆ›å»ºä¸€ä¸ª ConfigMap å¯¹è±¡æˆ–è€…ä½¿ç”¨ç°æœ‰çš„ ConfigMap å¯¹è±¡ã€‚å¤šä¸ª Pod å¯ä»¥å¼•ç”¨åŒä¸€ä¸ª ConfigMapã€‚ ä¿®æ”¹ Pod å®šä¹‰ï¼Œåœ¨ spec.volumes[] ä¸‹æ·»åŠ ä¸€ä¸ªå·ã€‚ ä¸ºè¯¥å·è®¾ç½®ä»»æ„åç§°ï¼Œä¹‹åå°† spec.volumes[].configMap.name å­—æ®µè®¾ç½®ä¸ºå¯¹ä½ çš„ ConfigMap å¯¹è±¡çš„å¼•ç”¨ã€‚ ä¸ºæ¯ä¸ªéœ€è¦è¯¥ ConfigMap çš„å®¹å™¨æ·»åŠ ä¸€ä¸ª .spec.containers[].volumeMounts[]ã€‚ è®¾ç½® .spec.containers[].volumeMounts[].readOnly=true å¹¶å°† .spec.containers[].volumeMounts[].mountPath è®¾ç½®ä¸ºä¸€ä¸ªæœªä½¿ç”¨çš„ç›®å½•åï¼Œ ConfigMap çš„å†…å®¹å°†å‡ºç°åœ¨è¯¥ç›®å½•ä¸­ã€‚ æ›´æ”¹ä½ çš„é•œåƒæˆ–è€…å‘½ä»¤è¡Œï¼Œä»¥ä¾¿ç¨‹åºèƒ½å¤Ÿä»è¯¥ç›®å½•ä¸­æŸ¥æ‰¾æ–‡ä»¶ã€‚ConfigMap ä¸­çš„æ¯ä¸ª data é”®ä¼šå˜æˆ mountPath ä¸‹é¢çš„ä¸€ä¸ªæ–‡ä»¶åã€‚ åˆ›å»ºä¸€ä¸ªæŒ‚è½½ConfigMapçš„Deployment\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-web-beijing namespace: default labels: k8s-app: nginx-web zone: beijing spec: replicas: 1 selector: matchLabels: k8s-app: nginx-web template: metadata: name: nginx-web-beijing labels: k8s-app: nginx-web spec: containers: - name: vue-shop-beijing image: nginx:latest resources: requests: memory: 100Mi cpu: 10m volumeMounts: - name: vue-config mountPath: \u0026#34;/etc/vue-config\u0026#34; readOnly: true volumes: - name: vue-config configMap: name: game-demo è¿›å…¥å®¹å™¨ä¸­æŸ¥çœ‹/etc/vue-configç›®å½•ä¸‹æ˜¯å¦æœ‰é…ç½®æ–‡ä»¶\nroot@nginx-web-beijing-5649b8f646-pzmm4:/etc/vue-config# ls service-interface.properties root@nginx-web-beijing-5649b8f646-pzmm4:/etc/vue-config# cat service-interface.properties port: 4000 å¦‚æœ Pod ä¸­æœ‰å¤šä¸ªå®¹å™¨ï¼Œåˆ™æ¯ä¸ªå®¹å™¨éƒ½éœ€è¦è‡ªå·±çš„ volumeMounts å—ï¼Œä½†é’ˆå¯¹æ¯ä¸ª ConfigMapï¼Œä½ åªéœ€è¦è®¾ç½®ä¸€ä¸ª spec.volumes å—ã€‚\nè¢«æŒ‚è½½çš„ConfigMapå†…å®¹ä¼šè¢«è‡ªåŠ¨æ›´æ–° å½“å·ä¸­ä½¿ç”¨çš„ ConfigMap è¢«æ›´æ–°æ—¶ï¼Œæ‰€æŠ•å°„çš„é”®æœ€ç»ˆä¹Ÿä¼šè¢«æ›´æ–°ã€‚ kubelet ç»„ä»¶ä¼šåœ¨æ¯æ¬¡å‘¨æœŸæ€§åŒæ­¥æ—¶æ£€æŸ¥æ‰€æŒ‚è½½çš„ ConfigMap æ˜¯å¦ä¸ºæœ€æ–°ã€‚ ä¸è¿‡ï¼Œkubelet ä½¿ç”¨çš„æ˜¯å…¶æœ¬åœ°çš„é«˜é€Ÿç¼“å­˜æ¥è·å¾— ConfigMap çš„å½“å‰å€¼ã€‚ é«˜é€Ÿç¼“å­˜çš„ç±»å‹å¯ä»¥é€šè¿‡ KubeletConfiguration ç»“æ„. çš„ ConfigMapAndSecretChangeDetectionStrategy å­—æ®µæ¥é…ç½®ã€‚\nConfigMap æ—¢å¯ä»¥é€šè¿‡ watch æ“ä½œå®ç°å†…å®¹ä¼ æ’­ï¼ˆé»˜è®¤å½¢å¼ï¼‰ï¼Œä¹Ÿå¯å®ç°åŸºäº TTL çš„ç¼“å­˜ï¼Œè¿˜å¯ä»¥ç›´æ¥ç»è¿‡æ‰€æœ‰è¯·æ±‚é‡å®šå‘åˆ° API æœåŠ¡å™¨ã€‚ å› æ­¤ï¼Œä» ConfigMap è¢«æ›´æ–°çš„é‚£ä¸€åˆ»ç®—èµ·ï¼Œåˆ°æ–°çš„ä¸»é”®è¢«æŠ•å°„åˆ° Pod ä¸­å»ï¼Œ è¿™ä¸€æ—¶é—´è·¨åº¦å¯èƒ½ä¸ kubelet çš„åŒæ­¥å‘¨æœŸåŠ ä¸Šé«˜é€Ÿç¼“å­˜çš„ä¼ æ’­å»¶è¿Ÿç›¸ç­‰ã€‚ è¿™é‡Œçš„ä¼ æ’­å»¶è¿Ÿå–å†³äºæ‰€é€‰çš„é«˜é€Ÿç¼“å­˜ç±»å‹ ï¼ˆåˆ†åˆ«å¯¹åº” watch æ“ä½œçš„ä¼ æ’­å»¶è¿Ÿã€é«˜é€Ÿç¼“å­˜çš„ TTL æ—¶é•¿æˆ–è€… 0ï¼‰ã€‚\nä»¥ç¯å¢ƒå˜é‡æ–¹å¼ä½¿ç”¨çš„ ConfigMap æ•°æ®ä¸ä¼šè¢«è‡ªåŠ¨æ›´æ–°ã€‚ æ›´æ–°è¿™äº›æ•°æ®éœ€è¦é‡æ–°å¯åŠ¨ Podã€‚\nSecret Secret æ˜¯ä¸€ç§åŒ…å«å°‘é‡æ•æ„Ÿä¿¡æ¯ä¾‹å¦‚å¯†ç ã€ä»¤ç‰Œæˆ–å¯†é’¥çš„å¯¹è±¡ã€‚ è¿™æ ·çš„ä¿¡æ¯å¯èƒ½ä¼šè¢«æ”¾åœ¨ Pod è§„çº¦ä¸­æˆ–è€…é•œåƒä¸­ã€‚ ä½¿ç”¨ Secret æ„å‘³ç€ä½ ä¸éœ€è¦åœ¨åº”ç”¨ç¨‹åºä»£ç ä¸­åŒ…å«æœºå¯†æ•°æ®ã€‚\nç”±äºåˆ›å»º Secret å¯ä»¥ç‹¬ç«‹äºä½¿ç”¨å®ƒä»¬çš„ Podï¼Œ å› æ­¤åœ¨åˆ›å»ºã€æŸ¥çœ‹å’Œç¼–è¾‘ Pod çš„å·¥ä½œæµç¨‹ä¸­æš´éœ² Secretï¼ˆåŠå…¶æ•°æ®ï¼‰çš„é£é™©è¾ƒå°ã€‚ Kubernetes å’Œåœ¨é›†ç¾¤ä¸­è¿è¡Œçš„åº”ç”¨ç¨‹åºä¹Ÿå¯ä»¥å¯¹ Secret é‡‡å–é¢å¤–çš„é¢„é˜²æªæ–½ï¼Œ ä¾‹å¦‚é¿å…å°†æœºå¯†æ•°æ®å†™å…¥éæ˜“å¤±æ€§å­˜å‚¨ã€‚\nSecret ç±»ä¼¼äº ConfigMap ä½†ä¸“é—¨ç”¨äºä¿å­˜æœºå¯†æ•°æ®ã€‚\næ³¨æ„: é»˜è®¤æƒ…å†µä¸‹ï¼ŒKubernetes Secret æœªåŠ å¯†åœ°å­˜å‚¨åœ¨ API æœåŠ¡å™¨çš„åº•å±‚æ•°æ®å­˜å‚¨ï¼ˆetcdï¼‰ä¸­ã€‚ ä»»ä½•æ‹¥æœ‰ API è®¿é—®æƒé™çš„äººéƒ½å¯ä»¥æ£€ç´¢æˆ–ä¿®æ”¹ Secretï¼Œä»»ä½•æœ‰æƒè®¿é—® etcd çš„äººä¹Ÿå¯ä»¥ã€‚ æ­¤å¤–ï¼Œä»»ä½•æœ‰æƒé™åœ¨å‘½åç©ºé—´ä¸­åˆ›å»º Pod çš„äººéƒ½å¯ä»¥ä½¿ç”¨è¯¥è®¿é—®æƒé™è¯»å–è¯¥å‘½åç©ºé—´ä¸­çš„ä»»ä½• Secretï¼› è¿™åŒ…æ‹¬é—´æ¥è®¿é—®ï¼Œä¾‹å¦‚åˆ›å»º Deployment çš„èƒ½åŠ›ã€‚\nä¸ºäº†å®‰å…¨åœ°ä½¿ç”¨ Secretï¼Œè¯·è‡³å°‘æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\nä¸º Secret å¯ç”¨é™æ€åŠ å¯†ã€‚ ä»¥æœ€å°ç‰¹æƒè®¿é—® Secret å¹¶å¯ç”¨æˆ–é…ç½® RBAC è§„åˆ™ã€‚ é™åˆ¶ Secret å¯¹ç‰¹å®šå®¹å™¨çš„è®¿é—®ã€‚ è€ƒè™‘ä½¿ç”¨å¤–éƒ¨ Secret å­˜å‚¨é©±åŠ¨ã€‚ Secretçš„ä½¿ç”¨ Pod å¯ä»¥ç”¨ä¸‰ç§æ–¹å¼ä¹‹ä¸€æ¥ä½¿ç”¨ Secretï¼š\nä½œä¸ºæŒ‚è½½åˆ°ä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨ä¸Šçš„å· ä¸­çš„æ–‡ä»¶ã€‚ ä½œä¸ºå®¹å™¨çš„ç¯å¢ƒå˜é‡ã€‚ ç”± kubelet åœ¨ä¸º Pod æ‹‰å–é•œåƒæ—¶ä½¿ç”¨ã€‚ Kubernetesæ§åˆ¶é¢ä¹Ÿä½¿ç”¨ Secretï¼› ä¾‹å¦‚ï¼Œå¼•å¯¼ä»¤ç‰Œ Secret æ˜¯ä¸€ç§å¸®åŠ©è‡ªåŠ¨åŒ–èŠ‚ç‚¹æ³¨å†Œçš„æœºåˆ¶ã€‚\nSecret ä¸»è¦ä½¿ç”¨çš„æœ‰ä»¥ä¸‹ä¸‰ç§ç±»å‹ï¼š\nOpaqueï¼šbase64 ç¼–ç æ ¼å¼çš„ Secretï¼Œç”¨æ¥å­˜å‚¨å¯†ç ã€å¯†é’¥ç­‰ï¼›ä½†æ•°æ®ä¹Ÿå¯ä»¥é€šè¿‡base64 â€“decodeè§£ç å¾—åˆ°åŸå§‹æ•°æ®ï¼Œæ‰€æœ‰åŠ å¯†æ€§å¾ˆå¼±ã€‚ kubernetes.io/dockerconfigjsonï¼šç”¨æ¥å­˜å‚¨ç§æœ‰docker registryçš„è®¤è¯ä¿¡æ¯ã€‚ kubernetes.io/service-account-tokenï¼šç”¨äº ServiceAccount, ServiceAccount åˆ›å»ºæ—¶ Kubernetes ä¼šé»˜è®¤åˆ›å»ºä¸€ä¸ªå¯¹åº”çš„ Secret å¯¹è±¡ï¼ŒPod å¦‚æœä½¿ç”¨äº† ServiceAccountï¼Œå¯¹åº”çš„ Secret ä¼šè‡ªåŠ¨æŒ‚è½½åˆ° Pod ç›®å½• /run/secrets/kubernetes.io/serviceaccount ä¸­ã€‚ bootstrap.kubernetes.io/tokenï¼šç”¨äºèŠ‚ç‚¹æ¥å…¥é›†ç¾¤çš„æ ¡éªŒçš„ Secret Opaque Secretçš„ä½¿ç”¨ Opaque ç±»å‹çš„æ•°æ®æ˜¯ä¸€ä¸ª map ç±»å‹ï¼Œè¦æ±‚ value å¿…é¡»æ˜¯ base64 ç¼–ç æ ¼å¼ï¼Œæ¯”å¦‚æˆ‘ä»¬æ¥åˆ›å»ºä¸€ä¸ªç”¨æˆ·åä¸º adminï¼Œå¯†ç ä¸º admin321 çš„ Secret å¯¹è±¡ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦å…ˆæŠŠç”¨æˆ·åå’Œå¯†ç åš base64 ç¼–ç ï¼š\n[root@Online-Beijing-master1 ~]# echo -n \u0026#34;admin321\u0026#34; | base64 YWRtaW4zMjE= ç„¶åæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨ä¸Šé¢ç¼–ç è¿‡åçš„æ•°æ®æ¥ç¼–å†™ä¸€ä¸ª YAML æ–‡ä»¶ï¼š(opaque-demo.yaml)\napiVersion: v1 kind: Secret metadata: name: base-user-info type: Opaque data: username: YWRtaW4= password: YWRtaW4zMjE= åˆ›å»ºå¥½ Secretå¯¹è±¡åï¼Œæœ‰ä¸¤ç§æ–¹å¼æ¥ä½¿ç”¨å®ƒï¼š\nä»¥ç¯å¢ƒå˜é‡çš„å½¢å¼ ä»¥Volumeçš„å½¢å¼æŒ‚è½½ é€šè¿‡ç¯å¢ƒå˜é‡æŒ‚è½½Secret apiVersion: apps/v1 kind: Deployment metadata: name: nginx-web-beijing namespace: default labels: k8s-app: nginx-web zone: beijing spec: replicas: 1 selector: matchLabels: k8s-app: nginx-web template: metadata: name: nginx-web-beijing labels: k8s-app: nginx-web spec: containers: - name: vue-shop-beijing image: nginx:latest resources: requests: memory: 100Mi cpu: 10m env: - name: USERNAME valueFrom: secretKeyRef: name: base-user-info key: username - name: PASSWORD valueFrom: secretKeyRef: name: base-user-info key: password é€šè¿‡VolumeæŒ‚è½½ Secret æŠŠä¸¤ä¸ª key æŒ‚è½½æˆäº†ä¸¤ä¸ªå¯¹åº”çš„æ–‡ä»¶ã€‚å½“ç„¶å¦‚æœæƒ³è¦æŒ‚è½½åˆ°æŒ‡å®šçš„æ–‡ä»¶ä¸Šé¢ï¼Œæ˜¯ä¸æ˜¯ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸Šä¸€èŠ‚è¯¾çš„æ–¹æ³•ï¼šåœ¨ secretName ä¸‹é¢æ·»åŠ  items æŒ‡å®š key å’Œ path\napiVersion: v1 kind: Pod metadata: name: secret2-pod spec: containers: - name: secret2 image: busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ls /etc/secrets\u0026#34;] volumeMounts: - name: secrets mountPath: /etc/secrets volumes: - name: secrets secret: secretName: base-user-info ä¸€èˆ¬æ¥è¯´Podé»˜è®¤çš„è®¿é—®API Serverçš„Tokenéƒ½ä¼šæŒ‚è½½åˆ°/var/run/secrets/kubernetes.io/serviceaccountå½“ä¸­,åˆ©ç”¨è‡ªå¸¦çš„tokenå’Œca.crtï¼Œé»˜è®¤æƒ…å†µä¸‹æ‰€æœ‰çš„Podéƒ½ä¼šè¢«æ³¨å…¥å½“å‰namespaceä¸‹çš„tokenå’Œca.crtè¿™æ ·ä»¥æ¥å°±å¯ä»¥å»è®¿é—®APIServeräº†\nroot@nginx-web-beijing-87c9f478f-knrfp:/var/run/secrets/kubernetes.io/serviceaccount# ls ca.crt namespace token kubernetes.io/dockerconfigjson é™¤äº†ä¸Šé¢çš„ Opaque è¿™ç§ç±»å‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ¥åˆ›å»ºç”¨æˆ· docker registry è®¤è¯çš„ Secretï¼Œç›´æ¥ä½¿ç”¨kubectl create å‘½ä»¤åˆ›å»ºå³å¯ï¼Œå¦‚ä¸‹ï¼š\nkubectl create secret docker-registry beijing-harbor --docker-server=DOCKER_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL kubectl create secret docker-registry beijing-harbor --docker-server=127.0.0.1 --docker-username=admin --docker-password=123123 --docker-email=beilanzhisen@163.com é™¤äº†ä¸Šé¢è¿™ç§æ–¹æ³•ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æŒ‡å®šæ–‡ä»¶çš„æ–¹å¼æ¥åˆ›å»ºé•œåƒä»“åº“è®¤è¯ä¿¡æ¯ï¼Œéœ€è¦æ³¨æ„å¯¹åº”çš„ KEY å’Œ TYPEï¼š\nkubectl create secret generic beijing-harbor --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson å¦‚æœæˆ‘ä»¬éœ€è¦æ‹‰å–ç§æœ‰ä»“åº“ä¸­çš„ Docker é•œåƒçš„è¯å°±éœ€è¦ä½¿ç”¨åˆ°ä¸Šé¢çš„ myregistry è¿™ä¸ª Secretï¼š\npiVersion: apps/v1 kind: Deployment metadata: name: foo spec: containers: - name: foo image: 192.168.1.100:5000/test:v1 imagePullSecrets: - name: beijing-harbor ImagePullSecrets ä¸ Secrets ä¸åŒï¼Œå› ä¸º Secrets å¯ä»¥æŒ‚è½½åˆ° Pod ä¸­ï¼Œä½†æ˜¯ ImagePullSecrets åªèƒ½ç”± Kubelet è®¿é—®ã€‚\nServiceAccount ServiceAccount ä¸»è¦æ˜¯ç”¨äºè§£å†³ Pod åœ¨é›†ç¾¤ä¸­çš„èº«ä»½è®¤è¯é—®é¢˜çš„ã€‚è®¤è¯ä½¿ç”¨çš„æˆæƒä¿¡æ¯å…¶å®å°±æ˜¯åˆ©ç”¨å‰é¢æˆ‘ä»¬è®²åˆ°çš„ä¸€ä¸ªç±»å‹ä¸º kubernetes.io/service-account-token è¿›è¡Œç®¡ç†çš„ã€‚\nServiceAccount æ˜¯å‘½åç©ºé—´çº§åˆ«çš„ï¼Œæ¯ä¸€ä¸ªå‘½åç©ºé—´åˆ›å»ºçš„æ—¶å€™å°±ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªåä¸º default çš„ ServiceAccount å¯¹è±¡:\nkubectl create ns kube-test kubectl get secret -n kube-test NAME TYPE DATA AGE default-token-vn4tr kubernetes.io/service-account-token 3 2m27s å®ç°åŸç† apiVersion: v1 data: ca.crt: LS0tLS... namespace: a3ViZS10ZXN0 token: ZXlKaG... kind: Secret metadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 75b3314b-e949-4f7b-9450-9bcd89c8c972 creationTimestamp: \u0026#34;2019-11-23T04:19:47Z\u0026#34; name: default-token-vn4tr namespace: kube-test resourceVersion: \u0026#34;4297521\u0026#34; selfLink: /api/v1/namespaces/kube-test/secrets/default-token-vn4tr uid: e3e60f95-f255-471b-a6c0-600a3c0ee53a type: kubernetes.io/service-account-token åœ¨ data åŒºåŸŸæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰3ä¸ªä¿¡æ¯ï¼š\nca.crtï¼šç”¨äºæ ¡éªŒæœåŠ¡ç«¯çš„è¯ä¹¦ä¿¡æ¯ namespaceï¼šè¡¨ç¤ºå½“å‰ç®¡ç†çš„å‘½åç©ºé—´ tokenï¼šç”¨äº Pod èº«ä»½è®¤è¯çš„ Token å‰é¢æˆ‘ä»¬ä¹Ÿæåˆ°äº†é»˜è®¤æƒ…å†µä¸‹å½“å‰ namespace ä¸‹é¢çš„ Pod ä¼šé»˜è®¤ä½¿ç”¨ default è¿™ä¸ª ServiceAccountï¼Œå¯¹åº”çš„ Secret ä¼šè‡ªåŠ¨æŒ‚è½½åˆ° Pod çš„ /var/run/secrets/kubernetes.io/serviceaccount/ ç›®å½•ä¸­ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨ Pod é‡Œé¢è·å–åˆ°ç”¨äºèº«ä»½è®¤è¯çš„ä¿¡æ¯äº†ã€‚\nå®é™…ä¸Šè¿™ä¸ªè‡ªåŠ¨æŒ‚è½½è¿‡ç¨‹æ˜¯åœ¨ Pod åˆ›å»ºçš„æ—¶å€™é€šè¿‡ Admisson Controllerï¼ˆå‡†å…¥æ§åˆ¶å™¨ï¼‰ æ¥å®ç°çš„ï¼Œå…³äºå‡†å…¥æ§åˆ¶å™¨çš„è¯¦ç»†ä¿¡æ¯æˆ‘ä»¬ä¼šåœ¨åé¢çš„å®‰å…¨ç« èŠ‚ä¸­å’Œå¤§å®¶ç»§ç»­å­¦ä¹ ã€‚\nAdmission Controllerï¼ˆå‡†å…¥æ§åˆ¶ï¼‰æ˜¯ Kubernetes API Server ç”¨äºæ‹¦æˆªè¯·æ±‚çš„ä¸€ç§æ‰‹æ®µã€‚Admission å¯ä»¥åšåˆ°å¯¹è¯·æ±‚çš„èµ„æºå¯¹è±¡è¿›è¡Œæ ¡éªŒï¼Œä¿®æ”¹ï¼ŒPod åˆ›å»ºæ—¶ Admission Controller ä¼šæ ¹æ®æŒ‡å®šçš„çš„ ServiceAccountï¼ˆé»˜è®¤çš„ defaultï¼‰æŠŠå¯¹åº”çš„ Secret æŒ‚è½½åˆ°å®¹å™¨ä¸­çš„å›ºå®šç›®å½•ä¸‹ /var/run/secrets/kubernetes.io/serviceaccount/ã€‚\n","date":"2023-02-14T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/configmaporservice/","title":"ConfigMapå’ŒSecretçš„ä½¿ç”¨"},{"content":"HorizontalPodAutoscaler HPAå®˜æ–¹æ–‡æ¡£ åœ¨Kubernetes ä¸­HorizontalPodAutoscalerè‡ªåŠ¨æ›´æ–°å·¥ä½œè´Ÿè½½èµ„æº ï¼ˆä¾‹å¦‚ Deployment æˆ–è€… StatefulSetï¼‰ï¼Œ ç›®çš„æ˜¯è‡ªåŠ¨æ‰©ç¼©å·¥ä½œè´Ÿè½½ä»¥æ»¡è¶³éœ€æ±‚ã€‚\næ°´å¹³æ‰©ç¼©æ„å‘³ç€å¯¹å¢åŠ çš„è´Ÿè½½çš„å“åº”æ˜¯éƒ¨ç½²æ›´å¤šçš„ Podã€‚ è¿™ä¸å‚ç›´(Vertical)æ‰©ç¼©ä¸åŒï¼Œå¯¹äº Kubernetesï¼Œ å‚ç›´æ‰©ç¼©æ„å‘³ç€å°†æ›´å¤šèµ„æºï¼ˆä¾‹å¦‚ï¼šå†…å­˜æˆ– CPUï¼‰åˆ†é…ç»™å·²ç»ä¸ºå·¥ä½œè´Ÿè½½è¿è¡Œçš„ Podã€‚\nå¦‚æœè´Ÿè½½å‡å°‘ï¼Œå¹¶ä¸”Podçš„æ•°é‡é«˜äºé…ç½®çš„æœ€å°å€¼ï¼ŒHorizontalPodAutoscaler ä¼šæŒ‡ç¤ºå·¥ä½œè´Ÿè½½èµ„æºï¼ˆDeploymentã€StatefulSet æˆ–å…¶ä»–ç±»ä¼¼èµ„æºï¼‰ç¼©å‡ã€‚\næ°´å¹³Podè‡ªåŠ¨æ‰©ç¼©ä¸é€‚ç”¨äºæ— æ³•æ‰©ç¼©çš„å¯¹è±¡: ä¾‹å¦‚DemonSetè¿™ç§\næˆ‘ä»¬å¯ä»¥ç®€å•çš„é€šè¿‡ kubectl autoscale å‘½ä»¤æ¥åˆ›å»ºä¸€ä¸ª HPA èµ„æºå¯¹è±¡ï¼ŒHPA Controlleré»˜è®¤30sè½®è¯¢ä¸€æ¬¡ï¼ˆå¯é€šè¿‡ kube-controller-manager çš„--horizontal-pod-autoscaler-sync-period å‚æ•°è¿›è¡Œè®¾ç½®ï¼‰ï¼ŒæŸ¥è¯¢æŒ‡å®šçš„èµ„æºä¸­çš„ Pod èµ„æºä½¿ç”¨ç‡ï¼Œå¹¶ä¸”ä¸åˆ›å»ºæ—¶è®¾å®šçš„å€¼å’ŒæŒ‡æ ‡åšå¯¹æ¯”ï¼Œä»è€Œå®ç°è‡ªåŠ¨ä¼¸ç¼©çš„åŠŸèƒ½ã€‚\nHorizontalPodAutoscaler æ˜¯å¦‚ä½•å·¥ä½œçš„ Kubernetes å°†æ°´å¹³ Pod è‡ªåŠ¨æ‰©ç¼©å®ç°ä¸ºä¸€ä¸ªé—´æ­‡è¿è¡Œçš„æ§åˆ¶å›è·¯ï¼ˆå®ƒä¸æ˜¯ä¸€ä¸ªè¿ç»­çš„è¿‡ç¨‹ï¼‰ã€‚é—´éš”ç”± kube-controller-manager çš„ --horizontal-pod-autoscaler-sync-period å‚æ•°è®¾ç½®ï¼ˆé»˜è®¤é—´éš”ä¸º 15 ç§’ï¼‰ã€‚\nåœ¨æ¯ä¸ªæ—¶é—´æ®µå†…ï¼Œæ§åˆ¶å™¨ç®¡ç†å™¨éƒ½ä¼šæ ¹æ®æ¯ä¸ª HorizontalPodAutoscaler å®šä¹‰ä¸­æŒ‡å®šçš„æŒ‡æ ‡æŸ¥è¯¢èµ„æºåˆ©ç”¨ç‡ã€‚ æ§åˆ¶å™¨ç®¡ç†å™¨æ‰¾åˆ°ç”± scaleTargetRef å®šä¹‰çš„ç›®æ ‡èµ„æºï¼Œç„¶åæ ¹æ®ç›®æ ‡èµ„æºçš„ .spec.selector æ ‡ç­¾é€‰æ‹© Podï¼Œ å¹¶ä»èµ„æºæŒ‡æ ‡ APIï¼ˆé’ˆå¯¹æ¯ä¸ª Pod çš„èµ„æºæŒ‡æ ‡ï¼‰æˆ–è‡ªå®šä¹‰æŒ‡æ ‡è·å–æŒ‡æ ‡ APIï¼ˆé€‚ç”¨äºæ‰€æœ‰å…¶ä»–æŒ‡æ ‡ï¼‰\nå¯¹äºæŒ‰ Pod ç»Ÿè®¡çš„èµ„æºæŒ‡æ ‡ï¼ˆå¦‚ CPUï¼‰ï¼Œæ§åˆ¶å™¨ä»èµ„æºæŒ‡æ ‡ API ä¸­è·å–æ¯ä¸€ä¸ª HorizontalPodAutoscaler æŒ‡å®šçš„ Pod çš„åº¦é‡å€¼ï¼Œå¦‚æœè®¾ç½®äº†ç›®æ ‡ä½¿ç”¨ç‡ï¼Œæ§åˆ¶å™¨è·å–æ¯ä¸ª Pod ä¸­çš„å®¹å™¨èµ„æºä½¿ç”¨æƒ…å†µï¼Œ å¹¶è®¡ç®—èµ„æºä½¿ç”¨ç‡ã€‚å¦‚æœè®¾ç½®äº† target å€¼ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆä¸å†è®¡ç®—ç™¾åˆ†æ¯”ï¼‰ã€‚ æ¥ä¸‹æ¥ï¼Œæ§åˆ¶å™¨æ ¹æ®å¹³å‡çš„èµ„æºä½¿ç”¨ç‡æˆ–åŸå§‹å€¼è®¡ç®—å‡ºæ‰©ç¼©çš„æ¯”ä¾‹ï¼Œè¿›è€Œè®¡ç®—å‡ºç›®æ ‡å‰¯æœ¬æ•°ã€‚ å¦‚æœ Pod ä½¿ç”¨è‡ªå®šä¹‰æŒ‡ç¤ºï¼Œæ§åˆ¶å™¨æœºåˆ¶ä¸èµ„æºæŒ‡æ ‡ç±»ä¼¼ï¼ŒåŒºåˆ«åœ¨äºè‡ªå®šä¹‰æŒ‡æ ‡åªä½¿ç”¨åŸå§‹å€¼ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ç‡ã€‚ å¦‚æœ Pod ä½¿ç”¨å¯¹è±¡æŒ‡æ ‡å’Œå¤–éƒ¨æŒ‡æ ‡ï¼ˆæ¯ä¸ªæŒ‡æ ‡æè¿°ä¸€ä¸ªå¯¹è±¡ä¿¡æ¯ï¼‰ã€‚ è¿™ä¸ªæŒ‡æ ‡å°†ç›´æ¥æ ¹æ®ç›®æ ‡è®¾å®šå€¼ç›¸æ¯”è¾ƒï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªä¸Šé¢æåˆ°çš„æ‰©ç¼©æ¯”ä¾‹ã€‚ åœ¨ autoscaling/v2 ç‰ˆæœ¬ API ä¸­ï¼Œè¿™ä¸ªæŒ‡æ ‡ä¹Ÿå¯ä»¥æ ¹æ® Pod æ•°é‡å¹³åˆ†åå†è®¡ç®—ã€‚ HorizontalPodAutoscalerçš„å¸¸è§ç”¨é€”æ˜¯å°†å…¶é…ç½®ä¸ºä»èšåˆ API ï¼ˆmetrics.k8s.ioã€custom.metrics.k8s.io æˆ– external.metrics.k8s.ioï¼‰è·å–æŒ‡æ ‡ã€‚ metrics.k8s.io API é€šå¸¸ç”±åä¸ºMetrics Serverçš„æ’ä»¶æä¾›ï¼Œéœ€è¦å•ç‹¬å¯åŠ¨ã€‚æœ‰å…³èµ„æºæŒ‡æ ‡çš„æ›´å¤šä¿¡æ¯ï¼Œ è¯·å‚é˜… Metrics Serverã€‚\nMetrics-Server åœ¨ HPA çš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ Heapster æä¾› CPU å’Œå†…å­˜æŒ‡æ ‡ï¼Œåœ¨ HPA v2 è¿‡åå°±éœ€è¦å®‰è£… Metrcis Server äº†ï¼ŒMetrics Server å¯ä»¥é€šè¿‡æ ‡å‡†çš„ Kubernetes API æŠŠç›‘æ§æ•°æ®æš´éœ²å‡ºæ¥ï¼Œæœ‰äº† Metrics Server ä¹‹åï¼Œæˆ‘ä»¬å°±å®Œå…¨å¯ä»¥é€šè¿‡æ ‡å‡†çš„ Kubernetes API æ¥è®¿é—®æˆ‘ä»¬æƒ³è¦è·å–çš„ç›‘æ§æ•°æ®äº†ï¼š\nhttps://api.k8s.io:8443/metrics.k8s.io/v1beta1/namespaces/\u0026lt;namespace-name\u0026gt;/pods/\u0026lt;pod-name\u0026gt; æ¯”å¦‚å½“æˆ‘ä»¬è®¿é—®ä¸Šé¢çš„ API çš„æ—¶å€™ï¼Œæˆ‘ä»¬å°±å¯ä»¥è·å–åˆ°è¯¥ Pod çš„èµ„æºæ•°æ®ï¼Œè¿™äº›æ•°æ®å…¶å®æ˜¯æ¥è‡ªäº kubelet çš„ Summary API é‡‡é›†è€Œæ¥çš„ã€‚ä¸è¿‡éœ€è¦è¯´æ˜çš„æ˜¯æˆ‘ä»¬è¿™é‡Œå¯ä»¥é€šè¿‡æ ‡å‡†çš„ API æ¥è·å–èµ„æºç›‘æ§æ•°æ®ï¼Œå¹¶ä¸æ˜¯å› ä¸º Metrics Server å°±æ˜¯ APIServer çš„ä¸€éƒ¨åˆ†ï¼Œè€Œæ˜¯é€šè¿‡ Kubernetes æä¾›çš„ Aggregator æ±‡èšæ’ä»¶æ¥å®ç°çš„ï¼Œæ˜¯ç‹¬ç«‹äº APIServer ä¹‹å¤–è¿è¡Œçš„ã€‚\nèšåˆ API Aggregator å…è®¸å¼€å‘äººå‘˜ç¼–å†™ä¸€ä¸ªè‡ªå·±çš„æœåŠ¡ï¼ŒæŠŠè¿™ä¸ªæœåŠ¡æ³¨å†Œåˆ° Kubernetes çš„ APIServer é‡Œé¢å»ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åƒåŸç”Ÿçš„ APIServer æä¾›çš„ API ä½¿ç”¨è‡ªå·±çš„ API äº†ï¼Œæˆ‘ä»¬æŠŠè‡ªå·±çš„æœåŠ¡è¿è¡Œåœ¨ Kubernetes é›†ç¾¤é‡Œé¢ï¼Œç„¶å Kubernetes çš„ Aggregator é€šè¿‡ Service åç§°å°±å¯ä»¥è½¬å‘åˆ°æˆ‘ä»¬è‡ªå·±å†™çš„ Service é‡Œé¢å»äº†ã€‚è¿™æ ·è¿™ä¸ªèšåˆå±‚å°±å¸¦æ¥äº†å¾ˆå¤šå¥½å¤„ï¼š\nå¢åŠ äº† API çš„æ‰©å±•æ€§ï¼Œå¼€å‘äººå‘˜å¯ä»¥ç¼–å†™è‡ªå·±çš„ API æœåŠ¡æ¥æš´éœ²ä»–ä»¬æƒ³è¦çš„ APIã€‚ ä¸°å¯Œäº† APIï¼Œæ ¸å¿ƒ kubernetes å›¢é˜Ÿé˜»æ­¢äº†å¾ˆå¤šæ–°çš„ API ææ¡ˆï¼Œé€šè¿‡å…è®¸å¼€å‘äººå‘˜å°†ä»–ä»¬çš„ API ä½œä¸ºå•ç‹¬çš„æœåŠ¡å…¬å¼€ï¼Œè¿™æ ·å°±æ— é¡»ç¤¾åŒºç¹æ‚çš„å®¡æŸ¥äº†ã€‚ å¼€å‘åˆ†é˜¶æ®µå®éªŒæ€§ APIï¼Œæ–°çš„ API å¯ä»¥åœ¨å•ç‹¬çš„èšåˆæœåŠ¡ä¸­å¼€å‘ï¼Œå½“å®ƒç¨³å®šä¹‹åï¼Œåœ¨åˆå¹¶ä¼š APIServer å°±å¾ˆå®¹æ˜“äº†ã€‚ ç¡®ä¿æ–° API éµå¾ª Kubernetes çº¦å®šï¼Œå¦‚æœæ²¡æœ‰è¿™é‡Œæå‡ºçš„æœºåˆ¶ï¼Œç¤¾åŒºæˆå‘˜å¯èƒ½ä¼šè¢«è¿«æ¨å‡ºè‡ªå·±çš„ä¸œè¥¿ï¼Œè¿™æ ·å¾ˆå¯èƒ½é€ æˆç¤¾åŒºæˆå‘˜å’Œç¤¾åŒºçº¦å®šä¸ä¸€è‡´ã€‚ éƒ¨ç½²HPA æˆ‘ä»¬è¦ä½¿ç”¨ HPAï¼Œå°±éœ€è¦åœ¨é›†ç¾¤ä¸­å®‰è£… Metrics Server æœåŠ¡ï¼Œè¦å®‰è£… Metrics Server å°±éœ€è¦å¼€å¯ Aggregatorï¼Œå› ä¸º Metrics Server å°±æ˜¯é€šè¿‡è¯¥ä»£ç†è¿›è¡Œæ‰©å±•çš„ï¼Œä¸è¿‡æˆ‘ä»¬é›†ç¾¤æ˜¯é€šè¿‡ Kubeadm æ­å»ºçš„ï¼Œé»˜è®¤å·²ç»å¼€å¯äº†ï¼Œå¦‚æœæ˜¯äºŒè¿›åˆ¶æ–¹å¼å®‰è£…çš„é›†ç¾¤ï¼Œéœ€è¦å•ç‹¬é…ç½® kube-apsierver æ·»åŠ å¦‚ä¸‹æ‰€ç¤ºçš„å‚æ•°ï¼š\n--requestheader-client-ca-file=\u0026lt;path to aggregator CA cert\u0026gt; --requestheader-allowed-names=aggregator --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --proxy-client-cert-file=\u0026lt;path to aggregator proxy cert\u0026gt; --proxy-client-key-file=\u0026lt;path to aggregator proxy key\u0026gt; Aggregator èšåˆå±‚å¯åŠ¨å®Œæˆåï¼Œå°±å¯ä»¥æ¥å®‰è£… Metrics Server äº†ï¼Œæˆ‘ä»¬å¯ä»¥è·å–è¯¥ä»“åº“çš„å®˜æ–¹å®‰è£…èµ„æºæ¸…å•ï¼š\nå®˜æ–¹ä»“åº“åœ°å€ï¼šhttps://github.com/kubernetes-sigs/metrics-server # è¯·ä¿®æ”¹é•œåƒä¸º: registry.aliyuncs.com/google_containers/metrics-server:v0.6.2 wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.2/components.yaml å¦‚æœå‡ºç°x509: cannot validate certificate for 10.151.30.22 because it doesnâ€™t contain any IP SANsè¿™ç§é”™è¯¯,å› ä¸ºéƒ¨ç½²é›†ç¾¤çš„æ—¶å€™ï¼ŒCA è¯ä¹¦å¹¶æ²¡æœ‰æŠŠå„ä¸ªèŠ‚ç‚¹çš„ IP ç­¾ä¸Šå»ï¼Œæ‰€ä»¥è¿™é‡Œ Metrics Server é€šè¿‡ IP å»è¯·æ±‚æ—¶ï¼Œæç¤ºç­¾çš„è¯ä¹¦æ²¡æœ‰å¯¹åº”çš„IPæ‰€å¯¼è‡´çš„,æˆ‘ä»¬å¯ä»¥æ·»åŠ ä¸€ä¸ª--kubelet-insecure-tlså‚æ•°è·³è¿‡è¯ä¹¦æ ¡éªŒï¼š\n- args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls # ä¿®æ”¹å®Œæˆåè®°å¾—éƒ¨ç½² kubectl apply -f components.yaml éªŒè¯HPAæ˜¯å¦å®‰è£…æˆåŠŸ,ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡ kubectl top å‘½ä»¤æ¥è·å–åˆ°èµ„æºæ•°æ®äº†ï¼Œè¯æ˜ Metrics Server å·²ç»å®‰è£…æˆåŠŸäº†ã€‚\n[root@Online-Beijing-master1 ~]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% online-beijing-master1 82m 1% 1970Mi 12% online-beijing-master2 59m 0% 1379Mi 8% online-beijing-master3 61m 0% 1389Mi 8% online-beijing-node1 35m 0% 1957Mi 12% online-beijing-node2 33m 0% 1875Mi 11% online-beijing-node3 35m 0% 1045Mi 6% é¦–å…ˆæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªdeploymentï¼Œå‡†å¤‡å¯¹ä»–è¿›è¡ŒHPA\napiVersion: apps/v1 kind: Deployment metadata: name: hpa-demo-nginx namespace: default labels: k8s-app: hpa-demo-nginx spec: replicas: 1 selector: matchLabels: k8s-app: hpa-demo-nginx template: metadata: name: hpa-demo-nginx labels: k8s-app: hpa-demo-nginx spec: containers: - name: hpa-demo-nginx image: nginx:latest resources: requests: cpu: 10m memory: 100Mi securityContext: privileged: false åˆ›å»ºåŸºäºCPUçš„è‡ªåŠ¨æ‰©å®¹ æˆ‘ä»¬è¿™æ¬¡åªé’ˆå¯¹CPUè¿›è¡Œæ“ä½œ,åç»­æˆ‘ä»¬ä¼šæ ¹æ®æ›´å¤šçš„è‡ªå®šä¹‰èµ„æºæ¥è¿›è¡Œæ‰©ç¼©å®¹ã€‚\nç°åœ¨æˆ‘ä»¬æ¥åˆ›å»ºä¸€ä¸ªHPAï¼Œå¯ä»¥ä½¿ç”¨kubectl autoscaleå‘½ä»¤æ¥åˆ›å»ºï¼š\nkubectl autoscale deployment hpa-demo-nginx --cpu-percent=10 --min=1 --max=6 æ­¤å‘½ä»¤åˆ›å»ºäº†ä¸€ä¸ªå…³è”èµ„æºhpa-demo-nginx çš„HPAï¼Œæœ€å°çš„ pod å‰¯æœ¬æ•°ä¸º3ï¼Œæœ€å¤§ä¸º6ã€‚HPAä¼šæ ¹æ®è®¾å®šçš„ cpuä½¿ç”¨ç‡ï¼ˆ10%ï¼‰åŠ¨æ€çš„å¢åŠ æˆ–è€…å‡å°‘podæ•°é‡ã€‚\n[root@Online-Beijing-master1 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo-nginx Deployment/hpa-demo-nginx \u0026lt;unknown\u0026gt;/10% 1 6 0 8s æ¥ä¸‹æ¥å¯¹Podè¿›è¡Œå‹åŠ›æµ‹è¯•,ä¸æ–­çš„å»è¯·æ±‚å½“å‰hpa-demo-nginxPodçš„IP\nkubectl run -i --tty load-generator --image=busybox /bin/sh while true; do wget -q -O- http://10.10.180.71; done æ­£å¸¸å¯ä»¥çœ‹åˆ°HPAå·²ç»æ­£å¸¸å·¥ä½œäº†ï¼ŒPodçš„å‰¯æœ¬æ•°é‡å·²ç»åˆ†é…åˆ°äº†æˆ‘ä»¬å½“æ—¶æŒ‡å®šçš„6ä¸ª\n[root@Online-Beijing-master1 ~]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-demo-nginx Deployment/hpa-demo-nginx 78%/10% 1 6 6 4m20s ä»kubernetesv1.12ç‰ˆæœ¬å¼€å§‹,æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®kube-controller-managerçš„--horizontal-pod-autoscaler-downscale-stabilizationå‚æ•°æ¥è®¾ç½®ä¸€ä¸ªæŒç»­æ—¶é—´,æŒ‡çš„æ˜¯ç”¨äºå½“å‰æ‰©å®¹æ“ä½œå®Œæˆå,å¤šä¹…ä»¥åæ‰è¿›è¡Œä¸€æ¬¡ç¼©æ”¾æ“ä½œã€‚é»˜è®¤ä¸º5åˆ†é’Ÿ,ä¹Ÿå°±æ˜¯äº”åˆ†é’Ÿåæ‰ä¼šè¿›è¡Œç¼©æ”¾ã€‚\nåˆ›å»ºä¸€ä¸ªåŸºäºå†…å­˜çš„è‡ªåŠ¨æ‰©å®¹ è·ŸCPUæ˜¯ä¸€æ ·çš„,éƒ½æ˜¯åŸºäºmetrics-serverè·å–æŒ‡æ ‡ç„¶åè¿›è¡Œæ‰©å®¹ã€‚\napiVersion: apps/v1 kind: Deployment metadata: name: hpa-mem-demo namespace: default labels: k8s-app: hpa-mem-demo spec: replicas: 1 selector: matchLabels: k8s-app: hpa-mem-demo template: metadata: name: hpa-mem-demo labels: k8s-app: hpa-mem-demo spec: containers: - name: hpa-mem-demo image: nginx:latest resources: requests: memory: 20Mi cpu: 10m securityContext: privileged: true volumeMounts: - name: mount-configmap mountPath: /etc/script volumes: - name: mount-configmap configMap: name: increase-mem-config è¿™é‡Œå’Œå‰é¢æ™®é€šçš„åº”ç”¨æœ‰ä¸€äº›åŒºåˆ«ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªåä¸º increase-mem-config çš„ ConfigMap èµ„æºå¯¹è±¡æŒ‚è½½åˆ°äº†å®¹å™¨ä¸­ï¼Œè¯¥é…ç½®æ–‡ä»¶æ˜¯ç”¨äºåé¢å¢åŠ å®¹å™¨å†…å­˜å ç”¨çš„è„šæœ¬ï¼Œé…ç½®æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼šï¼ˆincrease-mem-cm.yamlï¼‰\napiVersion: v1 kind: ConfigMap metadata: name: increase-mem-config data: increase-mem.sh: | #!/bin/bash mkdir /tmp/memory mount -t tmpfs -o size=40M tmpfs /tmp/memory dd if=/dev/zero of=/tmp/memory/block sleep 60 rm /tmp/memory/block umount /tmp/memory rmdir /tmp/memory ç”±äºè¿™é‡Œå¢åŠ å†…å­˜çš„è„šæœ¬éœ€è¦ä½¿ç”¨åˆ° mount å‘½ä»¤ï¼Œè¿™éœ€è¦å£°æ˜ä¸ºç‰¹æƒæ¨¡å¼ï¼Œæ‰€ä»¥æˆ‘ä»¬æ·»åŠ äº† securityContext.privileged=true è¿™ä¸ªé…ç½®ã€‚ç°åœ¨æˆ‘ä»¬ç›´æ¥åˆ›å»ºä¸Šé¢çš„èµ„æºå¯¹è±¡å³å¯\nkubectl apply -f hpa-demo-mem.yaml kubectl apply -f increase-mem-config.yaml ","date":"2023-02-14T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/HorizontalPodAutoscaler/","title":"HorizontalPodAutoscaler"},{"content":"è®¿é—®æ§åˆ¶æ¦‚è§ˆ Kubernetes APIçš„æ¯ä¸ªè¯·æ±‚éƒ½ä¼šç»è¿‡å¤šé˜¶æ®µçš„è®¿é—®æ§åˆ¶ä¹‹åæ‰ä¼šè¢«æ¥å—,è¿™ä¸€é˜¶æ®µåŒ…æ‹¬è®¤è¯ã€æˆæƒã€ä»¥åŠå‡†å…¥æ§åˆ¶(Admission Control)ç­‰\nè®¤è¯æ’ä»¶ x509è¯ä¹¦ï¼šä½¿ç”¨x509è¯ä¹¦åªéœ€è¦API Serverå¯åŠ¨çš„æ—¶å€™é…ç½® --client-ca-file=SOMEFILEã€‚åœ¨è¯ä¹¦è®¤è¯çš„æ—¶å€™,å…¶CNåŸŸååšç”¨æˆ·å,è€Œç»„ç»‡æœºæ„ç”¨ä½œgroupåã€‚ é™æ€Tokenæ–‡ä»¶ï¼šä½¿ç”¨é™æ€Tokenæ–‡ä»¶è®¤è¯åªéœ€è¦åœ¨API Serverå¯åŠ¨çš„æ—¶å€™é…ç½® --token-auth-file=SOMEFILEã€‚è¯¥æ–‡ä»¶ä¸ºcsvæ ¼å¼,æ¯è¡Œè‡³å°‘åŒ…æ‹¬ä¸‰åˆ—tokenã€usernameã€user id å¼•å¯¼Token ä¸ºäº†æ”¯æŒå¹³æ»‘çš„å¯åŠ¨å’Œå¼•å¯¼æ–°çš„é›†ç¾¤,kubernetesåŒ…å«äº†ä¸€ç§åŠ¨æ€ç®¡ç†çš„æŒæœ‰ä»¤ç‰Œç±»å‹,ç§°ä½œå¯åŠ¨å¼•å¯¼ä»¤ç‰Œ(Bootstrap Token) è¿™äº›ä»¤ç‰Œä»¥Secretçš„å½¢å¼ä¿å­˜åœ¨kube-systemçš„åç§°ç©ºé—´ä¸­,å¯ä»¥åŠ¨æ€çš„ç®¡ç†å’Œåˆ›å»ºã€‚ æ§åˆ¶å™¨ç®¡ç†å™¨åŒ…å«çš„TokenCleaneræ§åˆ¶å™¨èƒ½å¤Ÿåœ¨å¯åŠ¨å¼•å¯¼ä»¤ç‰Œè¿‡æœŸæ—¶å°†å…¶åˆ é™¤ã€‚ åœ¨ä½¿ç”¨kubeadméƒ¨ç½²kubernetesçš„æ—¶å€™,å¯ä»¥é€šè¿‡kubeadm token listè¿›è¡ŒæŸ¥è¯¢ã€‚ ServiceAccountï¼šæ˜¯kubernetesè‡ªåŠ¨ç”Ÿæˆçš„,å¹¶ä¸”ä¼šè‡ªåŠ¨æŒ‚è½½åˆ°å®¹å™¨çš„/run/secrets/kubernetes.io/serviceaccountç›®å½•å½“ä¸­ Webhookä»¤ç‰Œèº«ä»½è®¤è¯ --authentication-token-webhook-config-fileï¼šæŒ‡å‘ä¸€ä¸ªé…ç½®æ–‡ä»¶,å…¶ä¸­æè¿°å¦‚ä½•è®¿é—®è¿œç¨‹çš„WebhookæœåŠ¡ --authentication-token-webhook-cache-ttlï¼šç”¨æ¥è®¾å®šèº«ä»½è®¤è¯å†³å®šçš„ç¼“å­˜æ—¶é—´ã€‚é»˜è®¤ä¸º2åˆ†é’Ÿã€‚ é™æ€Tokenç”¨æ³• æ–°å»ºä¸€ä¸ªå­˜æ”¾é™æ€Tokençš„ç›®å½• mkdir -p /etc/kubernetes/auth å°†Tokenå†…å®¹å†™å…¥åˆ°æ–‡ä»¶å½“ä¸­ æ³¨æ„ï¼šè¯¥æ–‡ä»¶æ ¼å¼ä¸ºCSVæ ¼å¼ï¼Œå…¶å®ä½ ä¹Ÿå¯ä»¥éšä¾¿å†™:happy:\næè¿°ï¼š Tokenå€¼ ç”¨æˆ·åç§° ç”¨æˆ·ID å¯é€‰ç»„å kube-token,kubeadminer,1000,\u0026#34;group1,group2,group3\u0026#34; å‡è®¾è¿™æ˜¯æˆ‘ä»¬è¯·æ±‚åç§°ç©ºé—´çš„è¯·æ±‚: curl -k -v -XGET -H \u0026quot;Authrization: Bearer kube-token\u0026quot; https://api.k8s.version.cn:6443/api/v1/namespaces/default\næ­£å¸¸è¯·æ±‚ä¼šè¿”å›ï¼Œå› ä¸ºæˆ‘æ²¡æœ‰åˆ›å»ºè¿™ä¸ªkube-token\n{ \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;namespaces \\\u0026#34;default\\\u0026#34; is forbidden: User \\\u0026#34;system:anonymous\\\u0026#34; cannot get resource \\\u0026#34;namespaces\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; in the namespace \\\u0026#34;default\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;namespaces\u0026#34; }, \u0026#34;code\u0026#34;: 403 è®¾ç½®API Server æ³¨æ„ï¼š æ“ä½œçš„æ—¶å€™è¯·å¤‡ä»½ä½ çš„API Serveræ–‡ä»¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯.\n# vim /etc/kubernetes/manifests/kube-apiserver.yaml # æŒ‚è½½æœ¬åœ°çš„/etc/basic-authè·¯å¾„ volumeMounts: - mountPath: /etc/basic-auth name: auth-files readOnly: true volumes: - hostPath: path: /etc/basic-auth type: DirectoryOrCreate name: auth-files # ä¿®æ”¹API Serverå¯åŠ¨å‚æ•° - --token-auth-file=/etc/basic-auth/kube-token.csv è¯·æ‚¨ç­‰å¾…APIServeré‡å¯å®Œæˆ\nå½“æˆ‘ä»¬å†æ¬¡é€šè¿‡kube-tokenè¿›è¡Œè¯·æ±‚API Server å¦‚æœèƒ½æˆåŠŸè¯†åˆ«åˆ°æˆ‘ä»¬çš„ç”¨æˆ·å³å¯ï¼Œä¹Ÿå°±çŸ¥é“æˆ‘ä»¬å½“å‰çš„ç”¨æˆ·ä¸ºkubeadminer\n{ \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;namespaces \\\u0026#34;default\\\u0026#34; is forbidden: User \\\u0026#34;kubeadminer\\\u0026#34; cannot get resource \\\u0026#34;namespaces\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; in the namespace \\\u0026#34;default\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;namespaces\u0026#34; }, \u0026#34;code\u0026#34;: 403 UserAccountå’ŒServiceAccountçš„åŒºåˆ« UserAccountæ˜¯ä¸å¤–éƒ¨è®¤è¯äºŒæ¬¡å¼€å‘å¯¹æ¥å®ç°çš„,ç­¾å‘çš„Tokenæ˜¯å¤–éƒ¨ç³»ç»Ÿ,æ¯æ¬¡APIServeréœ€è¦å¸¦æ­¤Tokenè¯·æ±‚å¤–éƒ¨è®¤è¯æœåŠ¡å™¨ã€‚ æ„å»ºç¬¦åˆKubernetesè§„èŒƒçš„è®¤è¯æœåŠ¡ éœ€è¦ä¾ç…§kubernetesè§„èŒƒï¼Œæ¥æ„å»ºè®¤è¯æœåŠ¡è¿›è¡Œè®¤è¯\nURLï¼šhttp://api.k8s.verbos/authenticate Method: POST(éœ€è¦æºå¸¦è¯·æ±‚æ•°æ®ç­‰ç­‰â€¦) Inputï¼š Outputï¼š WebHookè®¤è¯ç”¨æ³• æ–°å»ºä¸€ä¸ªwebhook-config.json { \u0026#34;kind\u0026#34;: \u0026#34;Config\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;preferences\u0026#34;: {}, \u0026#34;clusters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;github-authn\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;https://192.168.1.100:8443/authenticate\u0026#34; } } ], \u0026#34;users\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;authn-apiserver\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;token\u0026#34;: \u0026#34;secret\u0026#34; } } ], \u0026#34;contexts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;webhook\u0026#34;, \u0026#34;context\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;github-authn\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;authn-apiserver\u0026#34; } } ], \u0026#34;current-context\u0026#34;: \u0026#34;webhook\u0026#34; } æˆæƒ æˆæƒä¸»è¦æ˜¯ç”¨äºå¯¹äºé›†ç¾¤èµ„æºçš„è®¿é—®æ§åˆ¶ï¼Œé€šè¿‡æ£€æŸ¥è¯·æ±‚åŒ…å«çš„ç›¸å…³å±æ€§å€¼ï¼Œä¸ç›¸å¯¹åº”çš„è®¿é—®ç­–ç•¥è¿›è¡Œæ¯”è¾ƒï¼ŒAPIè¯·æ±‚å¿…é¡»æ»¡è¶³æŸäº›ç­–ç•¥æ‰èƒ½è¢«å¤„ç†ã€‚è·Ÿè®¤è¯ç±»ä¼¼ï¼Œkubernetesä¹Ÿæ”¯æŒå¤šç§æˆæƒæœºåˆ¶ï¼Œå¹¶æ”¯æŒåŒæ—¶å¼€å¯å¤šä¸ªæ’ä»¶æˆæƒï¼ˆåªè¦æœ‰ä¸€ä¸ªé€šè¿‡å³å¯ï¼‰ã€‚å¦‚æœæˆæƒæˆåŠŸï¼Œåˆ™ç”¨æˆ·çš„è¯·æ±‚ä¼šå‘é€åˆ°å‡†å…¥æ§åˆ¶æ¨¡å—è¿›è¡Œä¸‹ä¸€æ­¥å¤„ç†ã€‚\nkubernetesæˆæƒä»…å¤„ç†ä»¥ä¸‹è¯·æ±‚å±æ€§\nuserã€groupã€extra APIã€è¯·æ±‚æ–¹æ³•ã€è¯·æ±‚è·¯å¾„ è¯·æ±‚èµ„æºå’Œå­èµ„æº Namespace API Group kubernetesæ”¯æŒä»¥ä¸‹æˆæƒæ’ä»¶\nABAC RBAC WebHook Node RBACå’ŒABAC ABAC(Attribute Based Access Control)æœ¬æ¥æ˜¯ä¸é”™çš„æ¦‚å¿µï¼Œä½†æ˜¯åœ¨kubernetesä¸­å®ç°çš„æ¯”è¾ƒéš¾äºç®¡ç†å’Œç†è§£ï¼Œè€Œä¸”éœ€è¦å¯¹masteræ‰€åœ¨èŠ‚ç‚¹è¿›è¡ŒSSHå’Œæ–‡ä»¶ç³»ç»Ÿæƒé™ï¼Œè¦ä½¿å¾—å¯¹æˆæƒçš„å˜æ›´ç”Ÿæ•ˆè¿˜éœ€è¦é‡å¯API Server\nè€ŒRBACçš„ç­–ç•¥å¯ä»¥åˆ©ç”¨kubeletæˆ–è€…kubernetes APIè¿›è¡Œé…ç½®ã€‚RBACå¯ä»¥æˆæƒç»™ç”¨æˆ·ï¼Œè®©ç”¨æˆ·æœ‰æƒè¿›è¡Œæˆæƒç®¡ç†ï¼Œè¿™æ ·å°±å¯ä»¥æ— éœ€æ¥è§¦èŠ‚ç‚¹ï¼Œç›´æ¥è¿›è¡Œæˆæƒç®¡ç†ã€‚RBACåœ¨kubernetesä¸­è¢«æ˜ å°„ä¸ºAPIèµ„æºå’Œæ“ä½œã€‚\nRoleå’ŒClusterRole Role(è§’è‰²)æ˜¯ä¸€ç³»åˆ—ç‰¹å®šè§’è‰²æƒé™çš„é›†åˆï¼Œä¾‹å¦‚ä¸€ä¸ªè§’è‰²å¯ä»¥åŒ…å«è¯»å–Podçš„æƒé™å’Œåˆ—å‡ºPodçš„æƒé™ã€‚\nRoleåªèƒ½ç”¨æ¥ç»™æŸä¸ªç‰¹å®šçš„namespaceä¸­çš„èµ„æºä½œé‰´æƒï¼Œå¯¹å¤šnamespaceå’Œé›†ç¾¤çº§çš„èµ„æºæ´»ç€æ˜¯éèµ„æºç±»çš„API(å¦‚/healthz)ä½¿ç”¨ClusterRole\nä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„Roleé…ç½®\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: reader-pods rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•åœ°ClusterRoleé…ç½®\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: reader-pods rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] Binding å¦‚æœä½ æƒ³ä½¿å…¨å±€ç”Ÿæ•ˆå¯ä»¥ä½¿ç”¨kind: ClusterRoleBingdingå¹¶ä¸”ä¸å†™namespace\nkind: RoleBingding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: reader-pods # å½“å‰ç”¨æˆ·åªæœ‰developmentçš„è¯»å–æƒé™ namespace: development subjects: - kind: User name: guest01 apiGroup: rbac.authorization.k8s.io/v1 roleRef: kind: ClusterRole name: reader-pods apiGroup: rbac.authorization.k8s.io/v1 è´¦æˆ·/ç»„çš„ç®¡ç† è§’è‰²ç»‘å®š(Role Bingding)æ˜¯å°†è§’è‰²ä¸­å®šä¹‰çš„æƒé™èµ‹äºˆä¸€ä¸ªæˆ–è€…ä¸€ç»„ç”¨æˆ·ã€‚ å®ƒåŒ…å«ä¸»ä½“(ç”¨æˆ·ã€ç»„æˆ–æœåŠ¡è´¦æˆ·)çš„åˆ—è¡¨å’Œå¯¹è¿™äº›ä¸»ä½“æ‰€è·å¾—çš„è§’è‰²çš„å¼•ç”¨ ç»„çš„æ¦‚å¿µ\nå½“å¤–éƒ¨ç³»ç»Ÿè®¤è¯å¯¹æ¥çš„æ—¶å€™,ç”¨æˆ·ä¿¡æ¯å¯åŒ…å«ç»„ä¿¡æ¯ï¼Œæˆæƒå¯ä»¥é’ˆå¯¹ç”¨æˆ·ç¾¤ç»„è¿›è¡Œ å½“å¯¹Service Accountæˆæƒçš„æ—¶å€™,Groupä»£è¡¨æŸä¸ªNamespaceä¸‹çš„æ‰€æœ‰Service Account å¦‚æœé’ˆå¯¹ç¾¤ç»„æˆæƒ kind: ClusterRoleBingding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: reader-pods-global subjects: - kind: Group name: guest01 apiGroup: rbac.authorization.k8s.io/v1 roleRef: kind: ClusterRole name: reader-pods apiGroup: rbac.authorization.k8s.io/v1 name: system:serviceaccountsè¡¨ç¤ºå½“å‰æ˜¯ä¸€ä¸ªserviceaccountçš„æ ¼å¼è¿›è¡Œå‘½åçš„,å¹¶ä¸”æˆæƒç»™developmentè¿™ä¸ªNamespaceé‡Œé¢æ‰€æœ‰çš„ServiceAccount\nkind: ClusterRoleBingding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: reader-pods-global subjects: subjects: - kind: Group name: system:serviceaccounts:development apiGroup: rbac.authorization.k8s.io/v1 roleRef: kind: ClusterRole name: reader-pods apiGroup: rbac.authorization.k8s.io/v1 è§„åˆ’ç³»ç»Ÿè§’è‰² User:\nç®¡ç†å‘˜ï¼šç®¡ç†æ‰€æœ‰èµ„æºçš„æƒé™ æ™®é€šç”¨æˆ· æ˜¯å¦æœ‰è¯¥ç”¨æˆ·åˆ›å»ºçš„namespaceä¸‹çš„æ‰€æœ‰objectçš„æ“ä½œæƒé™ï¼Ÿ å¯¹å…¶ä»–ç”¨æˆ·çš„namespaceèµ„æºæ˜¯å¦å¯è¯»ã€æ˜¯å¦å¯å†™ SystemAccount Systemæ˜¯å¼€å‘è€…åˆ›å»ºåº”ç”¨åï¼Œåº”ç”¨äºapiserveré€šè®¯éœ€è¦çš„èº«ä»½ ç”¨æˆ·å¯ä»¥åˆ›å»ºè‡ªå®šçš„ServiceAccountï¼Œkubernetesä¹Ÿä¸ºæ¯ä¸ªnamespaceåˆ›å»ºdefault ServiceAccount Default ServiceAccounté€šå¸¸éœ€è¦ç»™å®šæƒé™ä»¥åæ‰èƒ½å¯¹API Serverè¿›è¡Œå†™æ“ä½œ ä¸æƒé™ç›¸å…³çš„å…¶ä»–æœ€ä½³å®è·µ ClusterRoleæ˜¯énamespaceç»‘å®šçš„,é’ˆå¯¹æ•´ä¸ªé›†ç¾¤ç”Ÿæ•ˆã€‚ é€šå¸¸éœ€è¦åˆ›å»ºä¸€ä¸ªç®¡ç†å‘˜è§’è‰²ï¼Œå¹¶ä¸”ç»‘å®šç»™å¼€å‘è¿è¥å›¢é˜Ÿçš„æˆå‘˜ ThirdPartyResourceå’ŒCustomResourceDefinitionæ˜¯å…¨å±€èµ„æºï¼Œæ™®é€šç”¨æˆ·åˆ›å»ºä»¥åéœ€è¦ç®¡ç†å‘˜æˆæƒæ‰èƒ½çœŸæ­£æ“ä½œè¯¥å¯¹è±¡ã€‚\nsshåˆ°masterèŠ‚ç‚¹é€šè¿‡insecure portè®¿é—®apiserverå¯ä»¥ç»•è¿‡é‰´æƒ,å½“éœ€è¦åšç®¡ç†æ“ä½œåˆæ²¡æœ‰æƒé™çš„æ—¶å€™å¯ä»¥ä½¿ç”¨(ä¸æ¨è)\nå‡†å…¥ å‡†å…¥æ§åˆ¶ å‡†å…¥æ§åˆ¶(Admission Control)åœ¨æˆæƒåå¯¹è¯·æ±‚åšè¿›ä¸€æ­¥çš„éªŒè¯æˆ–æ·»åŠ é»˜è®¤å‚æ•°ã€‚ä¸åŒäºæˆæƒå’Œè®¤è¯åªå…³å¿ƒè¯·æ±‚çš„ç”¨æˆ·å’Œæ“ä½œï¼Œå‡†å…¥æ§åˆ¶è¿˜å¤„ç†è¯·æ±‚çš„å†…å®¹ï¼Œå¹¶ä¸”ä»…å¯¹åˆ›å»ºã€æ›´æ–°ã€åˆ é™¤æˆ–è¿æ¥ç­‰æœ‰æ•ˆï¼Œè€Œå¯¹è¯»æ“ä½œæ— æ•ˆã€‚ å‡†å…¥æ§åˆ¶æ”¯æŒåŒæ—¶å¼€å¯å¤šä¸ªæ’ä»¶ï¼Œå®ƒä»¬ä¾æ¬¡è°ƒç”¨ï¼Œåªæœ‰å…¨éƒ¨æ’ä»¶éƒ½é€šè¿‡è¯·æ±‚æ‰å¯ä»¥è¿›å…¥ç³»ç»Ÿã€‚ é…é¢ç®¡ç†\nåŸå› ï¼šèµ„æºæœ‰é™ï¼Œå¦‚ä½•é™å®šæŸä¸ªç”¨æˆ·æœ‰å¤šå°‘èµ„æº æ–¹æ¡ˆ é¢„å®šä¹‰æ¯ä¸ªNamespaceçš„ResourceQuota,å¹¶ä¸”æŠŠspecä¿å­˜ä¸ºConfigMap ç”¨æˆ·å¯ä»¥åˆ›å»ºå¤šå°‘ä¸ªPod? BestEffortPod QuSPod åˆ›å»ºResourceQuota Controller: ä¸€èˆ¬ç”¨äºç›‘æ§namespaceåˆ›å»ºäº‹ä»¶ï¼Œå½“namespaceåˆ›å»ºçš„æ—¶å€™ï¼Œåœ¨è¯¥namespaceåˆ›å»ºå¯¹åº”çš„ResourceQuotaå¯¹è±¡ # é™åˆ¶default namespaceåªèƒ½åˆ›å»º3ä¸ªconfigmap apiVersion: v1 kind: ResourceQuota metadata: name: default-counts namespace: default spec: hard: configmaps: \u0026#34;3\u0026#34; å‡†å…¥æ§åˆ¶æ’ä»¶ AlwaysAdmit: æ¥å—æ‰€æœ‰è¯·æ±‚ AlwaysPullImages: æ€»æ˜¯æ‹‰æ–°çš„é•œåƒã€‚ä¸€èˆ¬ç”¨å¤šç§Ÿæˆ·åœºæ™¯ DenyEscalatingExec: ç¦æ­¢ç‰¹æƒå®¹å™¨çš„execå’Œattachæ“ä½œ ImagePolicyWebhook: é€šè¿‡webhookå†³å®šimageç­–ç•¥,éœ€è¦åŒæ—¶é…ç½®--admission-control-config-file ServiceAccount: è‡ªåŠ¨åˆ›å»ºé»˜è®¤Service Accountï¼Œå¹¶ç¡®ä¿Podså¼•ç”¨çš„ServiceAccountå·²ç»å­˜åœ¨ SecurityContextDeny: æ‹’ç»åŒ…å«éæ³•SecurityContexté…ç½®çš„å®¹å™¨ ResourceQuotaï¼šé™åˆ¶Podçš„è¯·æ±‚ä¸ä¼šè¶…è¿‡é…é¢ï¼Œéœ€è¦åœ¨namespaceä¸­åˆ›å»ºResourceQuotaå¯¹è±¡ MutatingWebhookConfiguration: å˜å½¢æ’ä»¶ï¼Œæ”¯æŒå¯¹å‡†å…¥å¯¹è±¡çš„ä¿®æ”¹ ValidatingWebhoookConfiguration: æ ¡éªŒæ’ä»¶ï¼Œåªèƒ½å¯¹å‡†å…¥å¯¹è±¡åˆæ³•æ€§è¿›è¡Œæ ¡éªŒ å¤ªå¤šäº† æˆ‘å°±ä¸ä¸€ä¸ªä¸€ä¸ªå†™äº†â€¦é™¤äº†é»˜è®¤çš„å‡†å…¥æ§åˆ¶æ’ä»¶ä»¥å¤–ï¼Œkubernetesé¢„ç•™äº†å‡†å…¥æ§åˆ¶æ’ä»¶çš„æ‰©å±•ç‚¹ï¼Œç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰å‡†å…¥æ§åˆ¶æ’ä»¶\nå‡†å…¥æ§åˆ¶æ’ä»¶çš„æ¼”ç¤º MutatingWebhookConfiguration: è¯´ç™½äº†å°±æ˜¯å¯¹å‡†å…¥æ§åˆ¶çš„å¯¹è±¡å†…å®¹è¿›è¡Œä¿®æ”¹ ç¡®ä¿å¯ç”¨ MutatingAdmissionWebhook å’Œ ValidatingAdmissionWebhook æ§åˆ¶å™¨ã€‚ è¿™é‡Œ æ˜¯ä¸€ç»„æ¨èçš„ admission æ§åˆ¶å™¨ï¼Œé€šå¸¸å¯ä»¥å¯ç”¨ã€‚ ç¡®ä¿å¯ç”¨äº†admissionregistration.k8s.io/v1çš„API git clone https://github.com/cncamp/admission-controller-webhook-demo.git è¿™ä¸ªæš‚ä¸”å…ˆä¸æ¼”ç¤ºäº†â€¦æˆ‘ä¹Ÿåœ¨ç ”ç©¶å“ˆå“ˆ\né™æµ æ¼æ–—ç®—æ³• æ¼æ´ç®—æ³•ä¹Ÿå¾ˆå®¹æ˜“ç†è§£ï¼Œè¯·æ±‚è¿›æ¥ä»¥åé¦–å…ˆè¿›å…¥æ¼æ–—é‡Œé¢ï¼Œç„¶åæ¼æ–—ä»¥æ’å®šçš„é€Ÿç‡å°†è¯·æ±‚æµå‡ºè¿›è¡Œå¤„ç†ï¼Œä»è€Œèµ·åˆ°å¹³æ»‘æµé‡çš„ä½œç”¨ã€‚ å½“è¯·æ±‚é‡è¿‡å¤§ï¼Œæ¼æ–—è¾¾åˆ°æœ€å¤§å®¹é‡çš„æ—¶å€™ä¼šæº¢å‡ºï¼Œæ­¤æ—¶è¯·æ±‚è¢«ä¸¢å¼ƒ åœ¨ç³»ç»Ÿçœ‹æ¥ï¼Œè¯·æ±‚æ°¸è¿œæ˜¯ä»¥å¹³æ»‘çš„é€Ÿç‡è¿‡æ¥ï¼Œä»è€Œèµ·åˆ°äº†ä¿æŠ¤ç³»ç»Ÿçš„ä½œç”¨\nä»¤ç‰Œæ¡¶ç®—æ³• ä»¤ç‰Œæ¡¶ç®—æ³•æ˜¯å¯¹æ¼æ–—ç®—æ³•çš„ä¸€ç§æ”¹è¿›ï¼Œé™¤äº†èƒ½å¤Ÿèµ·åˆ°é™æµçš„ä½œç”¨å¤–ï¼Œè¿˜å…è®¸ä¸€å®šç¨‹åº¦çš„æµé‡çªå‘ åœ¨ä»¤ç‰Œæ¡¶ç®—æ³•ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªä»¤ç‰Œæ¡¶ï¼Œç®—æ³•ä¸­å­˜åœ¨ä¸€ç§æœºåˆ¶ä»¥æ’å®šçš„é€Ÿç‡å‘ä»¤ç‰Œæ¡¶ä¸­æ”¾å…¥ä»¤ç‰Œã€‚ ä»¤ç‰Œæ¡¶ä¹Ÿæœ‰ä¸€å®šçš„å®¹é‡ï¼Œå¦‚æœæ»¡äº†çš„è¯ä»¤ç‰Œä¹Ÿæ— æ³•æ”¾è¿›å» å½“æœ‰è¯·æ±‚è¿›å…¥çš„æ—¶å€™ï¼Œä¼šé¦–å…ˆåˆ°ä»¤ç‰Œæ¡¶ä¸­å»é‚£ä»¤ç‰Œï¼Œåˆ™è¯¥è¯·æ±‚ä¼šè¢«å¤„ç†ï¼Œå¹¶æ¶ˆè€—æ‰æ‹¿åˆ°çš„ä»¤ç‰Œï¼Œå¦‚æœä»¤ç‰Œæ¡¶ä¸ºç©ºï¼Œåˆ™è¯·æ±‚ä¼šè¢«ä¸¢å¼ƒã€‚ API Serverçš„é™æµ max-requests-inflight: åœ¨ç»™å®šçš„æ—¶é—´å†…çš„æœ€å¤§çš„non-mutatingè¯·æ±‚æ•° max-mutating-requests-inflight: åœ¨ç»™å®šæ—¶é—´å†…çš„æœ€å¤§mutatingè¯·æ±‚æ•°ï¼Œè°ƒæ•´apiserverçš„æµæ§qos ååº”å‡ºæ¥çš„é—®é¢˜\nç²’åº¦ç²—: æ— æ³•ä¸ºä¸åŒç”¨æˆ·ï¼Œä¸åŒåœºæ™¯è®¾ç½®ä¸åŒçš„é™æµ å•é˜Ÿåˆ—ï¼šå…±äº«é™æµçš„çª—å£/æ¡¶ï¼Œä¸€ä¸ªåç”¨æˆ·å¯èƒ½å°†æ•´ä¸ªç³»ç»Ÿå µå¡ ä¸å…¬å¹³ï¼šæ­£å¸¸ç”¨æˆ·çš„è¯·æ±‚ä¼šè¢«æ’åˆ°é˜Ÿå°¾ï¼Œæ— æ³•åŠæ—¶å¤„ç†è¯·æ±‚è€Œé¥¿æ­» æ— ä¼˜å…ˆçº§: é‡è¦çš„ç³»ç»ŸæŒ‡ä»¤ä¸€å¹¶è¢«é™æµï¼Œç³»ç»Ÿæ•…éšœéš¾ä»¥æ¢å¤ é»˜è®¤å€¼ èŠ‚ç‚¹æ•°1000-3000 èŠ‚ç‚¹æ•°\u0026gt;3000 max-requests-inflight 400 1500 3000 max-mutating-requests-inflight 200 500 1000 API Priority and Fairness APFä»¥æ›´ç»†é¢—ç²’åº¦çš„æ–¹å¼å¯¹è¯·æ±‚è¿›è¡Œåˆ†ç±»å’Œéš”ç¦»ã€‚ å®ƒè¿˜å¼•å…¥äº†ç©ºé—´æœ‰é™çš„æ’é˜Ÿæœºåˆ¶ï¼Œå› æ­¤åœ¨éå¸¸çŸ­æš‚çš„çªå‘æƒ…å†µä¸‹ï¼ŒAPIæœåŠ¡å™¨ä¸ä¼šæ‹’ç»ä»»ä½•è¯·æ±‚ é€šè¿‡ä½¿ç”¨å…¬å¹³æ’é˜ŸæŠ€æœ¯ä»é˜Ÿåˆ—ä¸­åˆ†å‘è¯·æ±‚ï¼Œè¿™æ ·ä¸€ä¸ªè¡Œä¸ºä¸ä½³çš„æ§åˆ¶å™¨å°±ä¸ä¼šé¥¿æ­»å…¶ä»–æ§åˆ¶å™¨ APFçš„æ ¸å¿ƒ å¤šç­‰çº§ å¤šé˜Ÿåˆ— APF çš„å®ç°ä¾èµ–ä¸¤ä¸ªéå¸¸é‡è¦çš„èµ„æº FlowSchema, PriorityLevelConfiguration APF å¯¹è¯·æ±‚è¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ†ç±»ï¼Œæ¯ä¸€ä¸ªè¯·æ±‚åˆ†ç±»å¯¹åº”ä¸€ä¸ª FlowSchema (FS) FS å†…çš„è¯·æ±‚åˆä¼šæ ¹æ® distinguisher è¿›ä¸€æ­¥åˆ’åˆ†ä¸ºä¸åŒçš„ Flow. FS ä¼šè®¾ç½®ä¸€ä¸ªä¼˜å…ˆçº§ (Priority Level, PL)ï¼Œä¸åŒä¼˜å…ˆçº§çš„å¹¶å‘èµ„æºæ˜¯éš”ç¦»çš„ã€‚æ‰€ä»¥ä¸åŒä¼˜å…ˆçº§çš„èµ„æºä¸ä¼šç›¸äº’æ’æŒ¤ã€‚ç‰¹å®šä¼˜å…ˆçº§çš„è¯·æ±‚å¯ä»¥è¢«é«˜ä¼˜å¤„ç†ã€‚ ä¸€ä¸ª PL å¯ä»¥å¯¹åº”å¤šä¸ª FSï¼ŒPL ä¸­ç»´æŠ¤äº†ä¸€ä¸ª QueueSetï¼Œç”¨äºç¼“å­˜ä¸èƒ½åŠæ—¶å¤„ç†çš„è¯·æ±‚ï¼Œè¯·æ±‚ä¸ä¼šå› ä¸ºè¶…å‡º PL çš„å¹¶å‘é™åˆ¶è€Œè¢«ä¸¢å¼ƒã€‚ FS ä¸­çš„æ¯ä¸ª Flow é€šè¿‡ shuffle sharding ç®—æ³•ä» QueueSet é€‰å–ç‰¹å®šçš„ queues ç¼“å­˜è¯·æ±‚ã€‚ æ¯æ¬¡ä» QueueSet ä¸­å–è¯·æ±‚æ‰§è¡Œæ—¶ï¼Œä¼šå…ˆåº”ç”¨ fair queuing ç®—æ³•ä» QueueSet ä¸­é€‰ä¸­ä¸€ä¸ª queueï¼Œç„¶åä»è¿™ä¸ª queue ä¸­å–å‡º oldest è¯·æ±‚æ‰§è¡Œã€‚æ‰€ä»¥å³ä½¿æ˜¯åŒä¸€ä¸ª PL å†…çš„è¯·æ±‚ï¼Œä¹Ÿä¸ä¼šå‡ºç°ä¸€ä¸ª Flow å†…çš„è¯·æ±‚ä¸€ç›´å ç”¨èµ„æºçš„ä¸å…¬å¹³ç°è±¡ã€‚ é€šè¿‡ kubectl get flowschemaæŸ¥çœ‹å½“å‰çš„flow\nFlow Schema FlowSchemaä¼šåŒ¹é…ä¸€äº›å…¥ç«™è¯·æ±‚,å¹¶å°†ä»–ä»¬åˆ†é…ç»™ä¼˜å…ˆçº§ æ¯ä¸ªå…¥ç«™è¯·æ±‚éƒ½ä¼šæœ‰å¯¹åº”çš„FlowSchemaæµ‹è¯•æ˜¯å¦åŒ¹é…ï¼Œé¦–å…ˆä»matchingPrecedenceæ•°å€¼æœ€ä½çš„åŒ¹é…å¼€å§‹(æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯é€»è¾‘ä¸ŠåŒ¹é…æœ€é«˜)ï¼Œç„¶åä¾æ¬¡è¿›è¡ŒåŒ¹é…ï¼Œç›´åˆ°é¦–ä¸ªåŒ¹é…å‡ºç°.\napiVersion: flowcontrol.apiserver.k8s.io/v1beta1 kind: FlowSchema metadata: name: kube-scheduler # FlowSchemaåç§° spec: distinguisherMethod: type: ByNamespace # Distinguisher åŒºåˆ†å™¨ matchingPrecedence: 800 # è§„åˆ™ä¼˜å…ˆçº§,æ•°å­—è¶Šå°çº§åˆ«è¶Šé«˜ priorityLevelConfiguration: # å¯¹åº”çš„ä¼˜å…ˆçº§é˜Ÿåˆ— name: workload-high # ä¼˜å…ˆçº§é˜Ÿåˆ—åç§° rules: - resourceRules: - resources: - \u0026#39;*\u0026#39; # å¯¹åº”çš„èµ„æºå’Œè¯·æ±‚ç±»å‹ verbs: - \u0026#39;*\u0026#39; subjects: - kind: User user: name: system:kube-scheduler PriorityLevelConfiguration(ä¼˜å…ˆçº§é˜Ÿåˆ—) ä¸€ä¸ªPriorityLevelConfigurationè¡¨ç¤ºå•ä¸ªéš”ç¦»ç±»å‹ã€‚ æ¯ä¸ªPriorityLevelConfigurationå¯¹æœªå®Œæˆçš„è¯·æ±‚æ•°æœ‰å„è‡ªçš„é™åˆ¶,å¯¹æ’é˜Ÿä¸­çš„è¯·æ±‚æ•°ä¹Ÿæœ‰é™åˆ¶ã€‚\nPriorityLevelConfigurationæ˜¯å¯ä»¥è¢«å¤šä¸ªFlowSchemaè¿›è¡Œå¤ç”¨çš„\napiVersion: flowcontrol.apiserver.k8s.io/v1beta1 kind: PriorityLevelConfiguration metadata: name: global-default spec: limited: assuredConcurrencyShares: 20 # å…è®¸çš„å¹¶å‘è¯·æ±‚ limitResponse: queuing: handSize: 6 # shuffle shardingé…ç½®,æ¯ä¸ªflowschmea+distinguisherçš„è¯·æ±‚ä¼šè¢«enqueueåˆ°å¤šå°‘ä¸ªé˜Ÿåˆ— queueLengthLimit: 50 # æ¯ä¸ªé˜Ÿåˆ—ä¸­çš„å¯¹è±¡æ•°é‡ queues: 128 # å½“å‰PriorityLevelçš„é˜Ÿåˆ—æ€»æ•° type: Queue type: Limited å¯ä»¥é€šè¿‡kubectl get PriorityLevelConfigurationæŸ¥çœ‹å½“å‰kubernetesä¸­çš„ä¼˜å…ˆçº§é˜Ÿåˆ—\nsystem: ç”¨äº system:nodes ç»„ï¼ˆå³ kubeletï¼‰çš„è¯·æ±‚ï¼› kubelet å¿…é¡»èƒ½è¿ä¸Š API æœåŠ¡å™¨ï¼Œä»¥ä¾¿å·¥ä½œè´Ÿè½½èƒ½å¤Ÿè°ƒåº¦åˆ°å…¶ä¸Šã€‚ leader-election: ç”¨äºå†…ç½®æ§åˆ¶å™¨çš„é¢†å¯¼é€‰ä¸¾çš„è¯·æ±‚ ï¼ˆç‰¹åˆ«æ˜¯æ¥è‡ª kube-system åç§°ç©ºé—´ä¸­ system:kube- controller-manager å’Œ system:kube-scheduler ç”¨æˆ·å’ŒæœåŠ¡è´¦å·ï¼Œé’ˆå¯¹ endpointsã€ configmaps æˆ– leases çš„è¯·æ±‚ï¼‰ã€‚ å°†è¿™äº›è¯·æ±‚ä¸å…¶ä»–æµé‡ç›¸éš”ç¦»éå¸¸é‡è¦ï¼Œå› ä¸ºé¢†å¯¼è€…é€‰ä¸¾å¤±è´¥ä¼šå¯¼è‡´æ§åˆ¶å™¨å‘ç”Ÿæ•…éšœå¹¶é‡æ–°å¯åŠ¨ï¼Œè¿™åè¿‡æ¥ä¼šå¯¼è‡´æ–°å¯åŠ¨çš„æ§åˆ¶å™¨åœ¨åŒæ­¥ä¿¡æ¯æ—¶ï¼Œæµé‡å¼€é”€æ›´å¤§ã€‚ workload-high: ä¼˜å…ˆçº§ç”¨äºå†…ç½®æ§åˆ¶å™¨çš„è¯·æ±‚ workload-low: ä¼˜å…ˆçº§é€‚ç”¨äºæ¥è‡ªä»»ä½•æœåŠ¡å¸æˆ·çš„è¯·æ±‚ï¼Œé€šå¸¸åŒ…æ‹¬æ¥è‡ª Pods ä¸­è¿è¡Œçš„æ§åˆ¶å™¨çš„æ‰€æœ‰è¯·æ±‚ã€‚ global-default: ä¼˜å…ˆçº§å¯å¤„ç†æ‰€æœ‰å…¶ä»–æµé‡ï¼Œä¾‹å¦‚ï¼šéç‰¹æƒç”¨æˆ·è¿è¡Œçš„äº¤äº’å¼ kubectl å‘½ä»¤ã€‚ exempt: ä¼˜å…ˆçº§çš„è¯·æ±‚å®Œå…¨ä¸å—æµæ§é™åˆ¶ï¼šå®ƒä»¬æ€»æ˜¯ç«‹åˆ»è¢«åˆ†å‘ã€‚ ç‰¹æ®Šçš„ exempt FlowSchemaæŠŠ system:masters ç»„çš„æ‰€æœ‰è¯·æ±‚éƒ½å½’å…¥è¯¥ä¼˜å…ˆçº§ç»„ã€‚ catch-all: ä¼˜å…ˆçº§ä¸ç‰¹æ®Šçš„ catch-all FlowSchema ç»“åˆä½¿ç”¨ï¼Œä»¥ç¡®ä¿æ¯ä¸ªè¯·æ±‚éƒ½åˆ†ç±»ã€‚ ä¸€èˆ¬ä¸åº”è¯¥ä¾èµ–äº catch-all çš„é…ç½®ï¼Œè€Œåº”é€‚å½“åœ°åˆ›å»ºè‡ªå·±çš„ catch-all FlowSchema å’ŒPriorityLevelConfigurationsï¼ˆæˆ–ä½¿ç”¨é»˜è®¤å®‰è£…çš„ global-default é…ç½®ï¼‰ã€‚ ä¸ºäº†å¸®åŠ©æ•è·éƒ¨åˆ†è¯·æ±‚æœªåˆ†ç±»çš„é…ç½®é”™è¯¯ï¼Œå¼ºåˆ¶è¦æ±‚ catch-all ä¼˜å…ˆçº§ä»…å…è®¸5ä¸ªå¹¶å‘ä»½é¢ï¼Œå¹¶ä¸”ä¸å¯¹è¯·æ±‚è¿›è¡Œæ’é˜Ÿï¼Œä½¿å¾—ä»…ä¸ catch-all FlowSchema åŒ¹é…çš„æµé‡è¢«æ‹’ç»çš„å¯èƒ½æ€§æ›´é«˜ï¼Œå¹¶æ˜¾ç¤º HTTP 429 é”™è¯¯ã€‚ [root@Online-Beijing-master1 ~]# kubectl get PriorityLevelConfiguration NAME TYPE ASSUREDCONCURRENCYSHARES QUEUES HANDSIZE QUEUELENGTHLIMIT AGE catch-all Limited 5 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 13h exempt Exempt \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 13h global-default Limited 20 128 6 50 13h leader-election Limited 10 16 4 50 13h node-high Limited 40 64 6 50 13h system Limited 30 64 6 50 13h workload-high Limited 40 128 6 50 13h workload-low Limited 100 128 6 50 13h è¯¦ç»†è¯´ä¸€ä¸‹åˆ†æµç­–ç•¥\næ ¹æ®service-accountsçš„flowè¿›è¡Œé™åˆ¶,distinguisherMethodæ ¹æ®ä¸åŒçš„ç”¨æˆ·è¿›è¡Œé™æµã€‚ è¿™ä¸€ç±»çš„flowåº”è¯¥é€šè¿‡priorityLevelConfigurationä¸­å®šä¹‰çš„workload-lowè¿›è¡Œé™æµ é€šè¿‡workload-lowä¸­å®šä¹‰çš„assuredConcurrencySharesè®¾ç½®å½“å‰è¯·æ±‚çš„æœ€å¤§å¹¶å‘æ•°é‡ é«˜å¯ç”¨APIServer æ­å»ºå¤šç§Ÿæˆ·çš„kubernetesé›†ç¾¤ æˆä¿¡\nè®¤è¯: ç¦æ­¢åŒ¿åè®¿é—®ï¼Œåªå…è®¸å¯ä¿¡ç”¨æˆ·åšæ“ä½œã€‚ æˆæƒï¼šåŸºäºæˆä¿¡çš„æ“ä½œï¼Œé˜²æ­¢å¤šç”¨æˆ·ä¹‹é—´äº’ç›¸å½±å“ï¼Œæ¯”å¦‚æ™®é€šç”¨æˆ·åˆ é™¤Kubernetesæ ¸å¿ƒæœåŠ¡ï¼Œæˆ–è€…Aç”¨æˆ·åˆ é™¤æˆ–ä¿®æ”¹Bç”¨æˆ· çš„åº”ç”¨ã€‚ éš”ç¦»\nå¯è§è¡Œéš”ç¦»ï¼šç”¨æˆ·åªå…³å¿ƒè‡ªå·±çš„åº”ç”¨ï¼Œæ— éœ€çœ‹åˆ°å…¶ä»–ç”¨æˆ·çš„æœåŠ¡å’Œéƒ¨ç½²ã€‚ èµ„æºéš”ç¦»ï¼šæœ‰äº›å…³é”®é¡¹ç›®å¯¹èµ„æºéœ€æ±‚è¾ƒé«˜ï¼Œéœ€è¦æœ‰ä¸“ä¸šè®¾å¤‡ï¼Œä¸ä¸å…¶ä»–äººå…±äº«ã€‚ åº”ç”¨è®¿é—®éš”ç¦»ï¼šç”¨æˆ·åˆ›å»ºçš„æœåŠ¡ï¼ŒæŒ‰ç…§æ—¢å®šè§„åˆ™å…è®¸å…¶ä»–ç”¨æˆ·è®¿é—®ã€‚ èµ„æºç®¡ç†\nQuotaç®¡ç†: è°èƒ½ç”¨å¤šå°‘èµ„æº ","date":"2023-02-07T00:00:00Z","image":"https://bj.bcebos.com/baidu-rmb-video-cover-1/2b6495c8749e3f4e4369e28cb50eeb87.png","permalink":"http://localhost:1313/kubernetes/ApiServerRead/","title":"Kubernetesä¸­Api-Serverç®€å•è§£è¯»"},{"content":"å®˜æ–¹WebUIéƒ¨ç½² Dashboard å®‰è£…éƒ¨ç½² ä»å®˜æ–¹ä»“åº“éƒ¨ç½²\n[root@containerd-kube-master ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml å¦‚æœæ— æ³•ä¸‹è½½è¯·æ–°å»ºdashboard.yamlå¤åˆ¶ä»¥ä¸‹å†…å®¹è¿›è¡Œåº”ç”¨\n# Copyright 2017 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. apiVersion: v1 kind: Namespace metadata: name: kubernetes-dashboard --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kubernetes-dashboard type: Opaque --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kubernetes-dashboard type: Opaque data: csrf: \u0026#34;\u0026#34; --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kubernetes-dashboard type: Opaque --- kind: ConfigMap apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kubernetes-dashboard --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard rules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] resourceNames: [\u0026#34;kubernetes-dashboard-key-holder\u0026#34;, \u0026#34;kubernetes-dashboard-certs\u0026#34;, \u0026#34;kubernetes-dashboard-csrf\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;delete\u0026#34;] # Allow Dashboard to get and update \u0026#39;kubernetes-dashboard-settings\u0026#39; config map. - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;] resourceNames: [\u0026#34;kubernetes-dashboard-settings\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;update\u0026#34;] # Allow Dashboard to get metrics. - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;] resourceNames: [\u0026#34;heapster\u0026#34;, \u0026#34;dashboard-metrics-scraper\u0026#34;] verbs: [\u0026#34;proxy\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services/proxy\u0026#34;] resourceNames: [\u0026#34;heapster\u0026#34;, \u0026#34;http:heapster:\u0026#34;, \u0026#34;https:heapster:\u0026#34;, \u0026#34;dashboard-metrics-scraper\u0026#34;, \u0026#34;http:dashboard-metrics-scraper\u0026#34;] verbs: [\u0026#34;get\u0026#34;] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\u0026#34;metrics.k8s.io\u0026#34;] resources: [\u0026#34;pods\u0026#34;, \u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: securityContext: seccompProfile: type: RuntimeDefault containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.6.1 imagePullPolicy: Always ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kubernetes-dashboard # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- kind: Service apiVersion: v1 metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboard spec: ports: - port: 8000 targetPort: 8000 selector: k8s-app: dashboard-metrics-scraper --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboard spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: dashboard-metrics-scraper template: metadata: labels: k8s-app: dashboard-metrics-scraper spec: securityContext: seccompProfile: type: RuntimeDefault containers: - name: dashboard-metrics-scraper image: kubernetesui/metrics-scraper:v1.0.8 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - mountPath: /tmp name: tmp-volume securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 serviceAccountName: kubernetes-dashboard nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: tmp-volume emptyDir: {} åˆ›å»ºåº”ç”¨\nkubectl apply -f dashboard.yaml æŸ¥çœ‹Dashboardçš„Podæ˜¯å¦è¿è¡Œ\n[root@containerd-kube-master ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kubernetes-dashboard dashboard-metrics-scraper-8c47d4b5d-t92v9 1/1 Running 0 26m kubernetes-dashboard kubernetes-dashboard-6c75475678-zlqn7 1/1 Running 0 26m ä¿®æ”¹Dashboardçš„Services\n[root@containerd-kube-master ~]# kubectl edit services kubernetes-dashboard -n kubernetes-dashboard # Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;k8s-app\u0026#34;:\u0026#34;kubernetes-dashboard\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;kubernetes-dashboard\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kubernetes-dashboard\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;ports\u0026#34;:[{\u0026#34;port\u0026#34;:443,\u0026#34;targetPort\u0026#34;:8443}],\u0026#34;selector\u0026#34;:{\u0026#34;k8s-app\u0026#34;:\u0026#34;kubernetes-dashboard\u0026#34;}}} creationTimestamp: \u0026#34;2022-09-06T02:58:06Z\u0026#34; labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard resourceVersion: \u0026#34;5359\u0026#34; uid: 31019fac-9b16-46e5-9172-2cc3f63f2c86 spec: clusterIP: 10.101.114.92 clusterIPs: - 10.101.114.92 internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - port: 443 protocol: TCP targetPort: 8443 nodePort: 30000 # æ·»åŠ ä¸€è¡ŒnodePort selector: k8s-app: kubernetes-dashboard sessionAffinity: None type: ClusterIP # ä¿®æ”¹ä¸ºNodePort status: loadBalancer: {} æ”¹å®Œä»¥åä¸€å®šè¦çœ‹ä¸€çœ¼ç”Ÿæ•ˆæ— ç”Ÿæ•ˆå•Š\næŸ¥çœ‹åˆ°TYPEå¦‚æœæ˜¯NodePort æŸ¥çœ‹PORTå¯¹åº”443æ˜ å°„åˆ°äº†30000ç«¯å£ [root@containerd-kube-master ~]# kubectl get services --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 98m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 98m kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.103.193.200 \u0026lt;none\u0026gt; 8000/TCP 43m kubernetes-dashboard kubernetes-dashboard NodePort 10.101.114.92 \u0026lt;none\u0026gt; 443:30000/TCP 43m è®¿é—®é¡µé¢ è®¿é—®https://IP:PORT ä¾‹å¦‚: https://10.1.6.45:30000/#/login æ–°ç‰ˆæœ¬ä¸­é»˜è®¤ä¸å­˜å‚¨secretçš„åŠ å¯†æ•°æ®äº†,æ‰€ä»¥å¦‚æœä½ æƒ³ä½¿ç”¨Tokenè¿›è¡Œç™»å½•çš„è¯,ä½ éœ€è¦æ–°å»ºä¸€ä¸ªç”¨æˆ·å¹¶ä¸”ç»‘å®šè§’è‰²\næ–°å»ºä¸€ä¸ªuser.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard [root@containerd-kube-master ~]# kubectl apply -f user.yaml åŸºäºadmin-useråˆ›å»ºä¸€ä¸ªtoken,ç„¶åæŠŠè¾“å‡ºçš„tokenç²˜è´´åˆ°Dashboard\n[root@Online-Beijing-master1 ~]# kubectl create token admin-user -n kubernetes-dashboard eyJhbGciOiJSUzI1NiIsImtpZCI6InN2amtNUDdoaTRoU3VUVUw3aC1UTkVCRXlBRkhnQmlPTWMzWWZmbUhTd0kifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjc3NTk5NjEwLCJpYXQiOjE2Nzc1OTYwMTAsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiZGU3ZDYxNWItMDk5MC00ZjI3LTlkOGQtNzkwMzQyNjZjNjRlIn19LCJuYmYiOjE2Nzc1OTYwMTAsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.uvntmSuffTF8HGIZNacfmDJr8DdMTY4SR_H6Gyxck6Te_p2J4u5mNNmId2F5uWaoeySCYam-CUkkAO2F5Uh2WjxfD0wb2YI8-FRKic8n40n30TgKhdDeK-wlliDK5kVlf9yvy5uDEc8TWZhkXz-e9WLCMqta4J6_vc6t5PYJf8u1I_SYVOiC5fgP46z3W9bZMYeGV7xmKQGl9OeofQokC1TJCojtrt0AZJ0MW_2FQjZG7ONdiywaOYnST-Ii5rK12hJFoAjZem6aTzXikOJJ03zw7Kw4WqVdMAqnbs81Clkefi-ZPxReED2wi65RbLf8KVXFVWMn9EccX5IQC5Y49Q æ¨èå‡ ä¸ªå…¶ä»–çš„WebUi KubeSphere KubeGems Kuboard\n","date":"2023-02-03T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/KubeDashboard/","title":"kubernetes-dashboard"},{"content":"2022ç»ˆå°†é€å» 2022çš„æœ€åä¸€å¤©,å’Œæœ‹å‹ä¸€èµ·åè½¦å»äº†å¤©æ´¥ã€‚ æ—©ä¸Šäº”ç‚¹èµ·åºŠå¼€å§‹æ”¶æ‹¾ä¸œè¥¿äº†,å¿™å‰å¿™åçš„å‘ç°åå…¬äº¤å·²ç»èµ¶ä¸ä¸Šäº†,ç®—äº†è¿˜æ˜¯æ‰“è½¦å§ã€‚ä¸Šé«˜é“çš„æ—¶å€™çœŸçš„æ˜¯å¤ªå›°äº†â€¦ä»¥è‡³äºä¸€è·¯ä¸Šæ²¡æ‹åˆ°ä»€ä¹ˆé£æ™¯ã€‚\nå…¶å®æœ‰ä¸€ä¸ªèƒ½å’Œä½ è¯´èµ°å°±èµ°çš„æœ‹å‹çœŸçš„å¾ˆå¥½\nè¯´ä¸€ä¸‹æ­¤è¡Œæ™¯ç‚¹ Day1: å’Œæœ‹å‹ç›´æ¥ååˆ°æ»¨æµ·-\u0026gt;è€ç å¤´(å®åˆ™æ²¡å»)-\u0026gt;å›½å®¶æµ·æ´‹åšç‰©é¦†(å­¦äº†å¾ˆå¤š)-\u0026gt;ä¸œå ¤å…¬å›­(çœ‹æ—¥å‡ºğŸŒ„) Day2ï¼šå’Œæœ‹å‹ä»æ»¨æµ·ååˆ°å¤©æ´¥-\u0026gt;äº”å¤§é“-\u0026gt;æ„å¤§åˆ©é£æƒ…åŒº-\u0026gt;å¸‚åŒº çœ‹æµ· çœ‹æµ·æ˜¯æˆ‘è§‰å¾—äººç”Ÿä¸­æœ€å¿«ä¹çš„äº‹æƒ…(å¯¹æµ·æœ‰ä¸€ç§çˆ±ä¸é‡Šæ‰‹çš„æ„Ÿè§‰),æ¯æ¬¡ä¸€çœ‹åˆ°æµ·å°±èƒ½æ„Ÿè§‰åˆ°è®©è‡ªå·±çš„å…¨èº«æ”¾ç©º,çœŸçš„å¤ªå¥½çœ‹äº†ã€‚æŠŠçƒ‚äº‹ä¸¢åœ¨2022! æˆ‘åœ¨å¤©æ´¥å¾ˆæƒ³ä½  è¿™ä¸ªåº”è¯¥æ˜¯åº—å®¶è‡ªå·±ä¹°çš„,æ¥æ‹ç…§çš„äººä¸ç®—å¾ˆå¤š,ä½†æ˜¯ä¹Ÿæ˜¯ä¸€ä¸ªæ ‡å¿—æ€§çš„æŸ±å­äº†(å“ˆå“ˆ). é‡Œé¢æ˜¯ä¸€ä¸ªå·å°é¦†,ç”±äºæˆ‘æœ‹å‹ä¸èƒ½åœ¨å¤–é¢åƒé¥­(æ— ç¼˜å“å°äº†) åŸå¸‚å¤œæ™¯ ä¸å¾—ä¸è¯´,æˆ‘è§‰å¾—å¤©æ´¥çœŸçš„æ¯”åŒ—äº¬æœ‰çƒŸç«æ°”çš„å¤šã€‚ æˆ‘åœ¨è§£æ”¾æ¡¥æµ·æ²³ä¸Šçœ‹åˆ°äº†å¤©æ´¥çš„çƒŸèŠ±,è¶³è¶³åœ°æ”¾äº†2ä¸ªå¤šå°æ—¶â€¦æ˜¯ä¸€ä¸ªé€‚åˆç”Ÿæ´»çš„åŸå¸‚,è·¯ä¸Šçš„è¡Œäººä¹Ÿéå¸¸éå¸¸çš„å¤š(ä¸çŸ¥é“æ˜¯ä¸æ˜¯å› ä¸ºèŠ‚å‡æ—¥çš„åŸå› )ã€‚è·¯ä¸Šé¡ºå¸¦æ‹äº†å‡ å¼ å¤©æ´¥çš„åŸå¸‚å¤œæ™¯ã€‚\nè‡ªå·±ç»™å®ƒå–ä¸ªåå­—(æ ¼æ—å¨æ²»å¤§é’Ÿ) \u0026ndash;å¯ä»¥å¬å¬äº”æœˆå¤©çš„ã€Šæ­¥æ­¥ã€‹ æ ¼æ—å¨æ²»å¤§é’Ÿå‰ å½’é›¶è¶…è½½çš„ä¼¤æ‚² èƒŒç€æˆ‘å’Œæˆ‘çš„è¯ºè¨€ ä¸€èµ·è®¡åˆ’çš„è·¯çº¿å¯¹ç…§å­¤å•çš„æ—…åº— è¡—å¤´éšä¾¿æ‹çš„-å›æ¥æäº†ä¸ªå¾·å›½è¡—å¤´çš„è°ƒè‰² ","date":"2023-01-01T00:00:00Z","image":"https://img12.360buyimg.com/ddimg/jfs/t1/52521/8/22190/1015219/63b4f0deFd2378228/7dd2825c978f8f8c.jpg","permalink":"http://localhost:1313/tourists/tianjin/","title":"æ—…è¡Œæ—¥è®°-è·¨å¹´â€¢å¤©æ´¥"},{"content":"kubernetes1.22.10éƒ¨ç½² å‡†å¤‡å·¥ä½œ å…¼å®¹çš„ Linux ä¸»æœºã€‚Kubernetes é¡¹ç›®ä¸ºåŸºäº Debian å’Œ Red Hat çš„ Linux å‘è¡Œç‰ˆä»¥åŠé‚£äº›æ²¡æœ‰åŒ…ç®¡ç†å™¨çš„å‘è¡Œç‰ˆæä¾›äº†é€šç”¨è¯´æ˜ã€‚ æ¯å°æœºå™¨ 2 GB æˆ–æ›´å¤š RAMï¼ˆä»»ä½•æ›´å°‘éƒ½ä¼šä¸ºæ‚¨çš„åº”ç”¨ç¨‹åºç•™ä¸‹å¾ˆå°çš„ç©ºé—´ï¼‰ã€‚ 2 ä¸ª CPU æˆ–æ›´å¤šã€‚ é›†ç¾¤ä¸­æ‰€æœ‰æœºå™¨ä¹‹é—´çš„å®Œæ•´ç½‘ç»œè¿æ¥ï¼ˆå…¬å…±æˆ–ä¸“ç”¨ç½‘ç»œéƒ½å¯ä»¥ï¼‰ã€‚ æ¯ä¸ªèŠ‚ç‚¹çš„å”¯ä¸€ä¸»æœºåã€MAC åœ°å€å’Œ product_uuidã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§æ­¤å¤„ã€‚ æ‚¨çš„æœºå™¨ä¸Šçš„æŸäº›ç«¯å£æ˜¯å¼€æ”¾çš„ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§æ­¤å¤„ã€‚ äº¤æ¢Swapåˆ†åŒºã€‚å¿…é¡»ç¦ç”¨Swapæ‰èƒ½ä½¿ kubelet æ­£å¸¸å·¥ä½œã€‚ æˆ‘çš„æœåŠ¡å™¨é…ç½®åˆ—è¡¨ æ²¡æœ‰å¿…è¦æŒ‰ç…§æˆ‘çš„è¿™ä¸ªé…ç½®å»æ“ä½œä¸ªäººå»ºè®®å®éªŒç¯å¢ƒï¼šæ­£å¸¸æ¼”ç¤ºç¯å¢ƒ2æ ¸2Gå°±å¤Ÿäº†\néœ€è¦å¼€æ”¾çš„ç«¯å£ è™½ç„¶ etcd ç«¯å£åŒ…å«åœ¨æ§åˆ¶å¹³é¢éƒ¨åˆ†ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥åœ¨å¤–éƒ¨æˆ–è‡ªå®šä¹‰ç«¯å£ä¸Šæ‰˜ç®¡è‡ªå·±çš„ etcd é›†ç¾¤ã€‚ å¯ä»¥è¦†ç›–æ‰€æœ‰é»˜è®¤ç«¯å£å·ã€‚å½“ä½¿ç”¨è‡ªå®šä¹‰ç«¯å£æ—¶ï¼Œè¿™äº›ç«¯å£éœ€è¦æ‰“å¼€è€Œä¸æ˜¯æ­¤å¤„æåˆ°çš„é»˜è®¤å€¼ã€‚ ä¸€ä¸ªå¸¸è§çš„ä¾‹å­æ˜¯ API æœåŠ¡å™¨ç«¯å£ï¼Œæœ‰æ—¶ä¼šåˆ‡æ¢åˆ° 443ã€‚æˆ–è€…ï¼Œé»˜è®¤ç«¯å£ä¿æŒåŸæ ·ï¼ŒAPI æœåŠ¡å™¨æ”¾åœ¨è´Ÿè½½å‡è¡¡å™¨åé¢ï¼Œè¯¥è´Ÿè½½å‡è¡¡å™¨ç›‘å¬ 443 å¹¶å°†è¯·æ±‚è·¯ç”±åˆ°é»˜è®¤ç«¯å£ä¸Šçš„ API æœåŠ¡å™¨ã€‚\nå‡†å¤‡ä¸»æœºåœ°å€ ä¿®æ”¹æ¯ä¸€å°ä¸»æœºçš„/etc/hostsé…ç½® # vim /etc/hosts 10.1.6.45 containerd-kube-master 10.1.6.46 containerd-kube-work1 10.1.6.47 containerd-kube-work2 å…³é—­swapåˆ†åŒºä»¥åŠé˜²ç«å¢™ è¿›å…¥fstabåæ‰¾åˆ°ä½ æŒ‚è½½çš„swapåˆ†åŒºæ³¨é‡Šå³å¯.\n[root@bogon ~]# swapoff -a [root@localhost ~]# echo \u0026#34;vm.swappiness = 0\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf [root@bogon ~]# vim /etc/fstab # /dev/mapper/rl-swap none swap defaults 0 0 [root@localhost ~]# systemctl stop firewalld \u0026amp;\u0026amp; systemctl disable firewalld # å…³é—­å¹¶ä¸”ç¦ç”¨é˜²ç«å¢™ æ‰€æœ‰å†…å®¹å‡†å¤‡å®Œæˆåé‡å¯ä¸‰å°æœåŠ¡å™¨!\nContainerdçš„åŸºç¡€å®‰è£…å’Œæ“ä½œ æœ¬æ–‡æ¡£åç»­åŸºäºContainerd+RockyLinux+Kubeadminéƒ¨ç½²Kubernetes1.24ç‰ˆæœ¬çš„ç¯å¢ƒã€‚\ncontainerd Docker CRI-O éœ€è¦æ³¨æ„çš„æ˜¯,æ ¹æ®Kuberneteså®˜æ–¹ç»™å‡ºçš„å…¬å‘Šã€‚Kubernetes 1.20xç‰ˆæœ¬å°†ä¼šåºŸå¼ƒå¯¹Dockerçš„æ”¯æŒ\nå‚è€ƒè¿æ¥ é€šè¿‡é˜¿é‡Œäº‘é•œåƒæºå®‰è£… å®˜æ–¹é•œåƒç«™ ä¸‰å°ä¸»æœºå…¨éƒ¨æ‰§è¡Œæ­¤æ“ä½œ [root@containerd-kube-master ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 [root@containerd-kube-master ~]# yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo [root@containerd-kube-master ~]# yum -y install containerd.io æŸ¥çœ‹ä¸€ä¸‹containerdçš„ç‰ˆæœ¬\n[root@containerd-kube-master ~]# containerd -v containerd containerd.io 1.6.8 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 ç”Ÿæˆcontainerdçš„é…ç½®æ–‡ä»¶ ä¸‰å°ä¸»æœºå…¨éƒ¨æ‰§è¡Œæ­¤æ“ä½œ é»˜è®¤æƒ…å†µä¸‹åœ¨/etc/containerd/config.tomlå·²ç»æœ‰è¿™ä¸ªæ–‡ä»¶äº†,ä½†æ˜¯é‡Œé¢æ˜¯ä¸€äº›ç®€çŸ­çš„é…ç½®. [root@containerd-kube-master containerd]# mkdir - /etc/containerd/ [root@containerd-kube-master containerd]# containerd config default | tee /etc/containerd/config.toml # ç”Ÿæˆcontainedçš„é»˜è®¤é…ç½® ä¿®æ”¹sandbox_img pauseï¼š æ­¤é•œåƒæ˜¯kubernetesçš„åŸºç¡€å®¹å™¨ ä¸‰å°ä¸»æœºå…¨éƒ¨æ‰§è¡Œæ­¤æ“ä½œ ç”±äºéƒ¨åˆ†ç”¨æˆ·æ— æ³•è¿›å…¥k8s.gcr.ioèµ„æºåœ°å€,éœ€è¦å¯¹æ­¤åœ°å€è¿›è¡Œæ›¿æ¢. [root@containerd-kube-master containerd]# vim /etc/containerd/config.toml sandbox_image = \u0026#34;k8s.gcr.io/pause:3.6\u0026#34; # æ‰¾åˆ°æ­¤é€‰é¡¹å¹¶ä¸”ä¿®æ”¹ä¸º: registry.aliyuncs.com/google_containers/pause:3.6 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.default_runtime] runtime_type = \u0026#34;io.containerd.runtime.v1.linux\u0026#34;# ä¿®æ”¹ä¸ºio.containerd.runtime.v1.linux ä¿®æ”¹Systemd Cgroupé©±åŠ¨ [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true ç„¶åé‡å¯containerd\nsystemctl restart containerd è®¾ç½®Crictl æŸ¥æ‰¾çš„å®¹å™¨è¿è¡Œæ—¶ crictl config runtime-endpoint unix:///var/run/containerd/containerd.sock (é¢å¤–) è®¾ç½®Containerdçš„ç§æœ‰ä»“åº“ å¦‚æœä½ æƒ³ç”¨è‡ªå·±çš„ç§æœ‰ä»“åº“çš„è¯ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼è¿›è¡Œè®¾å®š\n[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.image_decryption] key_model = \u0026#34;node\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.auths] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs.\u0026#34;10.1.6.15\u0026#34;.auth] username = \u0026#34;\u0026#34; # ç§æœ‰ä»“åº“è´¦å· password = \u0026#34;\u0026#34; # ç§æœ‰ä»“åº“å¯†ç  [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.headers] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;10.1.6.15\u0026#34;] endpoint = [\u0026#34;\u0026#34;] # ç§æœ‰ä»“åº“åœ°å€ å¯åŠ¨containerd ä¸‰å°ä¸»æœºå…¨éƒ¨æ‰§è¡Œæ­¤æ“ä½œ ä¿è¯Active: active(running)çš„çŠ¶æ€å³å¯ [root@containerd-kube-master containerd]# systemctl restart containerd [root@containerd-kube-master containerd]# systemctl enable containerd [root@containerd-kube-master containerd]# systemctl status containerd â— containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; disabled; vendor preset: disabled) Active: active (running) since Mon 2022-09-05 02:53:02 EDT; 6s ago Docs: \u0026lt;https://containerd.io\u0026gt; Process: 8465 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 8467 (containerd) Tasks: 12 Memory: 25.2M CGroup: /system.slice/containerd.service â””â”€8467 /usr/bin/containerd Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159619104-04:00\u0026#34; level=info msg=\u0026#34;Start subscribing containerd event\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159662811-04:00\u0026#34; level=info msg=\u0026#34;Start recovering state\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159718042-04:00\u0026#34; level=info msg=\u0026#34;Start event monitor\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159737174-04:00\u0026#34; level=info msg=\u0026#34;Start snapshots syncer\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159750064-04:00\u0026#34; level=info msg=\u0026#34;Start cni network conf syncer for default\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159756351-04:00\u0026#34; level=info msg=\u0026#34;Start streaming server\u0026#34; Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159868546-04:00\u0026#34; level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.159906215-04:00\u0026#34; level=info msg=serving... address=/run/containerd/containerd.sock Sep 05 02:53:02 containerd-kube-master containerd[8467]: time=\u0026#34;2022-09-05T02:53:02.160196660-04:00\u0026#34; level=info msg=\u0026#34;containerd successfully booted in 0.021144s\u0026#34; Sep 05 02:53:02 containerd-kube-master systemd[1]: Started containerd container runtime. é…ç½®IPV4è½¬å‘ ä¸‰å°å…¨éƒ¨æ‰§è¡Œ cat \u0026lt;\u0026lt;EOF | tee /etc/modules-load.d/kubernetes1.24.conf overlay br_netfilter EOF cat \u0026lt;\u0026lt;EOF | tee /etc/sysctl.d/kubernetes1.24-forsys.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF [root@containerd-kube-master containerd]# modprobe br_netfilter [root@containerd-kube-master ~]# sysctl --system kuberneteså®‰è£… kubernetesçš„å®‰è£…ä¾æ—§æ˜¯åŸºäºaliyun\né€šè¿‡é˜¿é‡Œäº‘é•œåƒæºå®‰è£… ä¸‰å°å…¨éƒ¨å®‰è£… ç”±äºå®˜ç½‘æœªå¼€æ”¾åŒæ­¥æ–¹å¼, å¯èƒ½ä¼šæœ‰ç´¢å¼•gpgæ£€æŸ¥å¤±è´¥çš„æƒ…å†µ, è¿™æ—¶è¯·ç”¨ yum install -y --nogpgcheck kubelet kubeadm kubectl å®‰è£… cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg EOF [root@containerd-kube-work1 ~]# yum install -y kubelet-1.22.10 kubeadm-1.22.10 kubectl-1.22.10 å¯ä»¥é€šè¿‡yum --showduplicates list kubeletæŸ¥çœ‹å½“å‰ä»“åº“ä¸­å¯ç”¨çš„ç‰ˆæœ¬\nå®‰è£…å‘½ä»¤æç¤º å®‰è£…åå¯ä»¥ä½¿ç”¨tabè¿›è¡Œå¿«æ·æç¤º\n[root@containerd-kube-master ~]# yum -y install bash-completion [root@containerd-kube-master ~]# source \u0026lt;(kubeadm completion bash) \u0026amp;\u0026amp; source \u0026lt;(kubectl completion bash) å¦‚æœä½ æƒ³è¦æ°¸ä¹…çš„ä½¿å…¶ç”Ÿæ•ˆ,è¯·æŠŠä»–ä»¬åŠ å…¥åˆ°.bashrcå½“ä¸­\ncd ~ [root@containerd-kube-master ~]# vim .bashrc # .bashrc # User specific aliases and functions alias rm=\u0026#39;rm -i\u0026#39; alias cp=\u0026#39;cp -i\u0026#39; alias mv=\u0026#39;mv -i\u0026#39; # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi source \u0026lt;(kubeadm completion bash) source \u0026lt;(kubectl completion bash) source \u0026lt;(crictl completion bash) å¯åŠ¨kubelet [root@containerd-kube-master containerd]# systemctl enable kubelet åˆå§‹åŒ–é›†ç¾¤é…ç½®ä¿¡æ¯ [root@containerd-kube-master containerd]# kubeadm config print init-defaults \u0026gt; init.yaml apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.1.6.45 # åˆå§‹åŒ–çš„ç¬¬ä¸€å°masterä¸»æœºçš„åœ°å€ bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock imagePullPolicy: IfNotPresent name: kubernetes-master # å®šä¹‰ä¸»æœºçš„å”¯ä¸€åç§° taints: null --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # ä¿®æ”¹ä¸ºé˜¿é‡Œäº‘åœ°å€ kind: ClusterConfiguration kubernetesVersion: 1.22.10 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 # Podä½¿ç”¨çš„å­ç½‘åœ°å€ scheduler: {} [root@containerd-kube-master containerd]# kubeadm init --config=init.yaml # ç­‰å¾…é•œåƒPullå®Œæˆ [init] Using Kubernetes version: v1.24.0 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; åˆ›å»ºadminé…ç½®ç›®å½• [root@containerd-kube-master containerd]# mkdir -p $HOME/.kube [root@containerd-kube-master containerd]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@containerd-kube-master containerd]# sudo chown $(id -u):$(id -g) $HOME/.kube/config åˆ›å»ºé›†ç¾¤ç½‘ç»œ å› ä¸ºflannelä¸æ”¯æŒç½‘ç»œéš”ç¦»,æ‰€ä»¥ä¸æƒ³ç”¨äº†!\nä¸å†åŸºäºflannel,è€Œæ˜¯åŸºäºcalico [root@containerd-kube-master .kube]# curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O # --\u0026gt;è¿™ä¸ªåœ°å€å¥½åƒæ˜¯ç”¨ä¸äº†äº†,ç°åœ¨404äº†... curl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O ç¼–è¾‘calico.yaml\n- name: CALICO_IPV4POOL_CIDR # ä¿®æ”¹CIDRä¸ºKubernetesçš„å­ç½‘åœ°å€,å³åˆå§‹åŒ–é›†ç¾¤çš„serviceSubnet value: \u0026#34;10.96.0.0/12\u0026#34; åˆ›å»ºcalicoç½‘ç»œ\n[root@containerd-kube-master ~]# kubectl apply -f calico.yaml åŠ å…¥é›†ç¾¤ å¦‚æœåˆå§‹åŒ–æˆåŠŸä¼šå‡ºç°Your Kubernetes control-plane has initialized successfully!\nåœ¨nodeèŠ‚ç‚¹æ‰§è¡Œ [root@containerd-kube-work1 containerd]# kubeadm join 10.1.6.45:6443 --token abcdef.0123456789abcdef \\\\ --discovery-token-ca-cert-hash sha256:417d4385cc4f0cc572b106a6a13bf59fd865421f12a401f3660afa6ca19e500d éªŒè¯é›†ç¾¤ æŸ¥çœ‹masterèŠ‚ç‚¹çš„Podæ˜¯å¦å…¨éƒ¨å¯åŠ¨\n[root@containerd-kube-master ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6799f5f4b4-pf8w8 1/1 Running 0 2m16s kube-system calico-node-lzcjk 1/1 Running 0 54s kube-system calico-node-mrqkd 1/1 Running 0 2m16s kube-system calico-node-r45bc 1/1 Running 0 71s kube-system coredns-74586cf9b6-gkmbl 1/1 Running 0 2m37s kube-system coredns-74586cf9b6-tgh6f 1/1 Running 0 2m37s kube-system etcd-kubernetes-master 1/1 Running 2 2m42s kube-system kube-apiserver-kubernetes-master 1/1 Running 2 2m43s kube-system kube-controller-manager-kubernetes-master 1/1 Running 2 2m43s kube-system kube-proxy-mx4bg 1/1 Running 0 54s kube-system kube-proxy-ssw98 1/1 Running 0 71s kube-system kube-proxy-tpgfj 1/1 Running 0 2m38s kube-system kube-scheduler-kubernetes-master 1/1 Running 2 2m43s ä»masterä¸ŠæŸ¥çœ‹èŠ‚ç‚¹æ˜¯å¦å·²ç»å…¨éƒ¨Ready\n[root@containerd-kube-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION containerd-kube-work1 Ready \u0026lt;none\u0026gt; 55s v1.24.2 containerd-kube-work2 Ready \u0026lt;none\u0026gt; 38s v1.24.2 containerd-kube-master Ready control-plane 2m29s v1.24.2 åˆ°æ­¤ä¸ºæ­¢,1.24çš„kuberneteså·²ç»å®‰è£…å®Œæ¯•\næä¸€ä¸ªå°é—®é¢˜ çœ‹ä¸€ä¸‹ä½ ä»¬çš„corednsæ˜¯å¦åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Š,å¦‚æœåœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Š,å»ºè®®é‡æ–°åˆ†é…ä¸€ä¸‹corednsä¿è¯å…¶é«˜å¯ç”¨æ€§\n[root@containerd-kube-master ~]# kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-74586cf9b6-gkmbl 1/1 Running 0 32m 10.105.56.1 kubernetes-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-74586cf9b6-tgh6f 1/1 Running 0 32m 10.105.56.3 kubernetes-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; é‡æ–°åˆ†é…coredns\n[root@containerd-kube-master ~]# kubectl --namespace kube-system rollout restart deployment coredns é—®é¢˜è§£å†³ ä½¿ç”¨crictl imageå‡ºç°WARN[0000] image connect using default endpoints å‡ºç°è¯¥é—®é¢˜çš„åŸå› æ˜¯ç”±äºcrictlä¸çŸ¥é“ä½¿ç”¨é‚£ä¸ªsockå¯¼è‡´çš„ [root@containerd-kube-master ~]# crictl image WARN[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. è§£å†³æ–¹æ³•å¦‚ä¸‹\né»˜è®¤æƒ…å†µä¸‹containerdçš„sockå­˜æ”¾äº/run/containerd/containerd.sock # é‡æ–°è®¾ç½®ä¸€ä¸‹ä½¿ç”¨çš„runtime-endpoint crictl config runtime-endpoint unix:///run/containerd/containerd.sock é»˜è®¤ç”Ÿæˆçš„crictlå­˜æ”¾åœ¨/etc/crictl.yaml\n[root@containerd-kube-master containerd]# vim /etc/crictl.yaml ç¼–è¾‘é…ç½®æ–‡ä»¶\nruntime-endpoint: \u0026#34;unix:///run/containerd/containerd.sock\u0026#34; image-endpoint: \u0026#34;unix:///run/containerd/containerd.sock\u0026#34; # æ–°ç‰ˆæœ¬å¢åŠ äº†image-endpointéœ€è¦æ‰‹åŠ¨æ›´æ”¹ timeout: 10 debug: false pull-image-on-create: false disable-pull-on-run: false [root@containerd-kube-master containerd]# systemctl daemon-reload [root@containerd-kube-master containerd]# crictl image IMAGE TAG IMAGE ID SIZE Masterä¸»é›†ç¾¤åŠ å…¥tokenè¿‡æœŸå¦‚ä½•å¤„ç† é»˜è®¤æƒ…å†µä¸‹,è¯¥tokenåªæœ‰24å°æ—¶,å¦‚æœtokenå€¼è¿‡æœŸçš„è¯éœ€è¦é‡æ–°ç”Ÿæˆ --discovery-token-ca-cert-hash sha256:793106d3b4305808d55c3cdb211f89a768bec86ecef64264b131dc8f2548da16 æŸ¥çœ‹å½“å‰masteré›†ç¾¤çš„tokenåˆ—è¡¨ [root@containerd-kube-master .kube]# kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS abcdef.0123456789abcdef 8h 2022-09-06T10:10:05Z authentication,signing \u0026lt;none\u0026gt; system:bootstrappers:kubeadm:default-node-token é‡æ–°ç”Ÿæˆä¸€ä»½token [root@containerd-kube-master .kube]# kubeadm token create é€šè¿‡è¯ä¹¦hashtokne [root@containerd-kube-master .kube]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; ","date":"2022-12-29T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/InstallKubernetes1.22.10","title":"kubernetes1.22.0å•èŠ‚ç‚¹é›†ç¾¤éƒ¨ç½²"},{"content":"åˆ©ç”¨Kubeadmåˆ›å»ºé«˜å¯ç”¨é›†ç¾¤ ä½¿ç”¨å…·æœ‰å †å çš„æ§åˆ¶å¹³é¢èŠ‚ç‚¹ã€‚è¿™ç§æ–¹æ³•æ‰€éœ€åŸºç¡€è®¾æ–½è¾ƒå°‘ã€‚etcd æˆå‘˜å’Œæ§åˆ¶å¹³é¢èŠ‚ç‚¹ä½äºåŒä¸€ä½ç½®ã€‚ ä½¿ç”¨å¤–éƒ¨ etcd é›†ç¾¤ã€‚è¿™ç§æ–¹æ³•æ‰€éœ€åŸºç¡€è®¾æ–½è¾ƒå¤šã€‚æ§åˆ¶å¹³é¢çš„èŠ‚ç‚¹å’Œ etcd æˆå‘˜æ˜¯åˆ†å¼€çš„ã€‚ åœ¨ä¸‹ä¸€æ­¥ä¹‹å‰ï¼Œä½ åº”è¯¥ä»”ç»†è€ƒè™‘å“ªç§æ–¹æ³•æ›´å¥½åœ°æ»¡è¶³ä½ çš„åº”ç”¨ç¨‹åºå’Œç¯å¢ƒçš„éœ€æ±‚ã€‚ é«˜å¯ç”¨æ‹“æ‰‘é€‰é¡¹ è®²è¿°äº†æ¯ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚ å¦‚ä½•å®‰è£…Kubectlå’ŒKubeadm å¦‚ä½•å®‰è£…å¤–éƒ¨çš„Etcdé›†ç¾¤ å‚ä¸ä¸»æœºåˆ—è¡¨ IP CPU å†…å­˜ ç¡¬ç›˜ è§’è‰² 10.1.6.48 8 16 100 control-plane1 10.1.6.24 8 16 100 control-plane2 10.1.6.45 8 16 100 control-plane3 10.1.6.46 8 16 100 work1 10.1.6.43 8 16 100 work2 10.1.6.47 8 16 100 work3 10.1.6.213 4 4 20 HA+KP1 10.1.6.214 4 4 20 HA+KP2 10.1.6.215 Load_Balancer_IP 10.1.6.51 8 16 100 Etcd1 10.1.6.52 8 16 100 Etcd2 10.1.6.53 8 16 100 Etcd3 ä¸ºKube-apiserveråˆ›å»ºè´Ÿè½½å‡è¡¡å™¨ Keepalived æä¾› VRRP å®ç°ï¼Œå¹¶å…è®¸æ‚¨é…ç½® Linux æœºå™¨ä½¿è´Ÿè½½å‡è¡¡ï¼Œé¢„é˜²å•ç‚¹æ•…éšœã€‚HAProxy æä¾›å¯é ã€é«˜æ€§èƒ½çš„è´Ÿè½½å‡è¡¡ï¼Œèƒ½ä¸ Keepalived å®Œç¾é…åˆã€‚\nç”±äº lb1 å’Œ lb2 ä¸Šå®‰è£…äº† Keepalived å’Œ HAproxyï¼Œå¦‚æœå…¶ä¸­ä¸€ä¸ªèŠ‚ç‚¹æ•…éšœï¼Œè™šæ‹Ÿ IP åœ°å€ï¼ˆå³æµ®åŠ¨ IP åœ°å€ï¼‰å°†è‡ªåŠ¨ä¸å¦ä¸€ä¸ªèŠ‚ç‚¹å…³è”ï¼Œä½¿é›†ç¾¤ä»ç„¶å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œä»è€Œå®ç°é«˜å¯ç”¨ã€‚è‹¥æœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥æ­¤ä¸ºç›®çš„ï¼Œæ·»åŠ æ›´å¤šå®‰è£… Keepalived å’Œ HAproxy çš„èŠ‚ç‚¹ã€‚\nå…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… Keepalived å’Œ HAproxyã€‚\nyum install keepalived haproxy psmisc -y dnf install keepalived haproxy psmisc -y åœ¨ä¸¤å°ç”¨äºè´Ÿè½½å‡è¡¡çš„æœºå™¨ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥é…ç½® Proxyï¼ˆä¸¤å°æœºå™¨çš„ Proxy é…ç½®ç›¸åŒï¼‰ï¼š\nvi /etc/haproxy/haproxy.cfg global log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:8443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server kube-apiserver-1 10.1.6.48:6443 check # Replace the IP address with your own. server kube-apiserver-2 10.1.6.24:6443 check # Replace the IP address with your own. server kube-apiserver-3 10.1.6.45:6443 check # Replace the IP address with your own. å¯åŠ¨Haproxy\nè¯·ç¡®ä¿ä½ çš„LB2ä¹Ÿå·²ç»è¿›è¡Œå¦‚ä¸Šé…ç½®\nsystemctl restart haproxy systemctl enable haproxy é…ç½®Keepalived KP1é…ç½®å¦‚ä¸‹\nvim /etc/keepalived/keepalived.conf global_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 100 interface ens192 # ç½‘å¡åç§° virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip 10.1.6.213 # The IP address of this machine unicast_peer { 10.1.6.214 # The IP address of peer machines } virtual_ipaddress { 10.1.6.215/24 # è™šæ‹ŸIPåœ°å€ } track_script { chk_haproxy } } KP2é…ç½®å¦‚ä¸‹\nvim /etc/keepalived/keepalived.conf global_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 90 interface ens192 # Network card virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip 10.1.6.214 # The IP address of this machine unicast_peer { 10.1.6.214 # The IP address of peer machines } virtual_ipaddress { 10.1.6.215/24 # The VIP address } track_script { chk_haproxy } } è¯·ç¡®ä¿ä½ çš„KP1å’ŒKP2éƒ½å®Œæˆäº†å¦‚ä¸Šé…ç½®\nsystemctl start keepalived systemctl enable keepalived é…ç½®Masterä¸»æœºèŠ‚ç‚¹çš„Hostsæ–‡ä»¶ æ‰€æœ‰çš„Masterä¸»æœºéƒ½éœ€è¦è¿›è¡Œé…ç½®,é˜²æ­¢åç»­è§£æä¸åˆ°api.k8s.verbos.com\n10.1.6.48 containerd-master1 10.1.6.24 containerd-master2 10.1.6.45 containerd-master3 10.1.6.46 containerd-work1 10.1.6.43 containerd-work2 10.1.6.47 containerd-work3 10.1.6.51 etcd1 10.1.6.52 etcd2 10.1.6.53 etcd3 10.1.6.215 api.k8s.verbos.com è®¾ç½®Etcdé›†ç¾¤è¯ä¹¦ å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å·¥ä½œåœ¨WorkèŠ‚ç‚¹çš„Etcdæˆ–è€…å…¶ä»–å•ç‹¬çš„Etcdé›†ç¾¤,è¯·å°†Etcdçš„caè¯ä¹¦è¿›è¡Œæ‹·è´åˆ°MasterèŠ‚ç‚¹å½“ä¸­.\nexport CONTROL_PLANE=\u0026#34;root@10.1.6.48\u0026#34; scp /etc/kubernetes/pki/etcd/ca.crt \u0026#34;${CONTROL_PLANE}\u0026#34;: scp /etc/kubernetes/pki/apiserver-etcd-client.crt \u0026#34;${CONTROL_PLANE}\u0026#34;: scp /etc/kubernetes/pki/apiserver-etcd-client.key \u0026#34;${CONTROL_PLANE}\u0026#34;: è®¾ç½®ç¬¬ä¸€ä¸ªæ§åˆ¶å¹³é¢èŠ‚ç‚¹ ç”¨ä»¥ä¸‹å†…å®¹åˆ›å»ºä¸€ä¸ªåä¸º kubeadm-config.yaml çš„æ–‡ä»¶\napiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: v1.22.10 controlPlaneEndpoint: \u0026#34;api.k8s.verbos.com:8443\u0026#34; apiServer: certSANs: - 10.1.6.48 - 10.1.6.24 - 10.1.6.45 etcd: external: endpoints: - https://10.1.6.51:2379 # é€‚å½“åœ°æ›´æ”¹ ETCD_0_IP - https://10.1.6.52:2379 # é€‚å½“åœ°æ›´æ”¹ ETCD_1_IP - https://10.1.6.53:2379 # é€‚å½“åœ°æ›´æ”¹ ETCD_2_IP caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key imageRepository: registry.aliyuncs.com/google_containers networking: podSubnet: 10.244.0.0/16 serviceSubnet: 10.10.0.0/16 åœ¨èŠ‚ç‚¹ä¸Šè¿è¡Œå¦‚ä¸‹å‘½ä»¤\nkubeadm init --config kubeadm-config.yaml --upload-certs --v=5 æ³¨æ„ï¼šå¦‚æœä½ çš„é›†ç¾¤åˆå§‹åŒ–æˆåŠŸä½ å°†ä¼šçœ‹åˆ°å¦‚ä¸‹ä¿¡æ¯.\nè¯·ä¿å­˜å¥½kubeadm joinçš„å†…å®¹,é»˜è®¤2å°æ—¶åè¿‡æœŸ,è¿‡æœŸåé‡æ–°ç”Ÿæˆå³å¯ Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join api.k8s.verbos.com:8443 --token 9oerz0.fw6s8ft9xa44i077 \\ --discovery-token-ca-cert-hash sha256:be3c70562ae6bf8cfcfbbfa3bb8124fe63af3b1a0671e806a4ccf1bc243d5c6b \\ --control-plane --certificate-key cdf0f280f9e59e18e5f60d98b624008d828ba00ef096a2e38fd9b6b1463be152 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \u0026#34;kubeadm init phase upload-certs --upload-certs\u0026#34; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join api.k8s.verbos.com:8443 --token 9oerz0.fw6s8ft9xa44i077 \\ --discovery-token-ca-cert-hash sha256:be3c70562ae6bf8cfcfbbfa3bb8124fe63af3b1a0671e806a4ccf1bc243d5c6b æ‹·è´é›†ç¾¤é…ç½®æ–‡ä»¶\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config è¿è¡Œç½‘ç»œCNIæ’ä»¶(Calico) æ³¨æ„ï¼šæ­¤æ—¶ä½ åº”å½“æœ‰ä¸€ä¸ªCNIæ’ä»¶æ¥æä¾›ç½‘ç»œæœåŠ¡,å¦‚æœä¸å®‰è£…CNIæ’ä»¶,é‚£ä¹ˆé›†ç¾¤å°†ä¼šå¤„äºä¸å¯ç”¨çŠ¶æ€.\nå¦‚æœä½ å¯ä»¥æ­£å¸¸è®¿é—®github\n[root@containerd-kube-master .kube]# curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O å›½å†…ç”¨æˆ·\ncurl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O ä¿®æ”¹Calico.yamlé…ç½® ä¿®æ”¹CIDRä¸ºKubernetesçš„å­ç½‘åœ°å€,è¯¥åœ°å€ä¸ºserviceSubnetçš„åœ°å€,ä¹Ÿå°±æ˜¯Podè¿è¡Œçš„åœ°å€ã€‚\n- name: CALICO_IPV4POOL_CIDR value: \u0026#34;10.10.0.0/16\u0026#34; æ‰§è¡Œcalico.yaml\nkubectl apply -f calico.yaml å°†å…¶ä»–æ§åˆ¶å¹³é¢èŠ‚ç‚¹åŠ å…¥é›†ç¾¤ å½“ä½ å·²ç»ç¡®ä¿ä½ çš„containerd-master1å®Œæˆåˆå§‹åŒ–çš„æ—¶å€™,ä½ å¯ä»¥å°†å…¶ä»–çš„masterèŠ‚ç‚¹åŠ å…¥åˆ°æ­¤é›†ç¾¤å½“ä¸­.\nkubeadm join api.k8s.verbos.com:8443 --token 9oerz0.fw6s8ft9xa44i077 \\ --discovery-token-ca-cert-hash sha256:be3c70562ae6bf8cfcfbbfa3bb8124fe63af3b1a0671e806a4ccf1bc243d5c6b \\ --control-plane --certificate-key cdf0f280f9e59e18e5f60d98b624008d828ba00ef096a2e38fd9b6b1463be152 åŠ å…¥é›†ç¾¤æˆåŠŸå,è¯·æ‹·è´é›†ç¾¤é…ç½®æ–‡ä»¶å¦åˆ™å°†å½±å“kubeletçš„å·¥ä½œ\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config å°†å·¥ä½œèŠ‚ç‚¹åŠ å…¥é›†ç¾¤ kubeadm join api.k8s.verbos.com:8443 --token mck9bg.renw4t689mmx7oe1 \\ --discovery-token-ca-cert-hash sha256:ed51fca8615acf5f437f63f66a9709e43ef942caf19aea4c595cb905e4a9a00f (å¯é€‰é¡¹)ä¿®æ”¹Kubeletçš„æ•°æ®å­˜å‚¨ç›®å½• å¦‚æœä½ éœ€è¦ä¿®æ”¹kubeletçš„æ•°æ®å­˜å‚¨ç›®å½•,è¯·æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è¿›è¡Œæ“ä½œ\nvim /etc/sysconfig/kubelet è®¾ç½®ä½ çš„kubeletçš„æ•°æ®å­˜å‚¨ç›®å½•(å»ºè®®å•ç‹¬çš„ä¸ºkubeletæŒ‚è½½ä¸€å—æ•°æ®ç›˜)\nKUBELET_EXTRA_ARGS=\u0026#34;--root-dir=/data/k8s/kubelet\u0026#34; é‡å¯kubelet\nsystemctl daemon-reloa systemctl restart kubelet umount $(df -HT | grep \u0026#39;/var/lib/kubelet/pods\u0026#39; | awk \u0026#39;{print $7}\u0026#39;) æŸ¥çœ‹æ–°çš„æ•°æ®ç›®å½•æ˜¯å¦æœ‰kubeletçš„æ•°æ®\n[root@containerd-master2 kubelet]# ls /data/k8s/kubelet/ cpu_manager_state memory_manager_state plugins plugins_registry pod-resources pods å¦‚ä½•å‰”é™¤ä¸€ä¸ªMaster å…ˆè¯´ä¸€ä¸ªé—®é¢˜,å½“æˆ‘ä»¬æ­£å¸¸å¦‚æœä¸€ä¸ªMasterè¦è¿›è¡ŒæŸç§å‡çº§çš„æ—¶å€™,å¦‚ä½•æ­£ç¡®çš„å®‰å…¨çš„æ¥è¿›è¡Œåˆ é™¤è¯¥Masterã€‚ é¦–å…ˆæˆ‘ä»¬è¦å°†è¯¥Masterè®¾ç½®ä¸ºä¸å¯è°ƒåº¦çš„çŠ¶æ€,å¹¶ä¸”é©±é€åœ¨å…¶ä¸Šé¢çš„Pod # è®¾ç½®Masterä¸ºä¸å¯è°ƒåº¦çš„çŠ¶æ€ kubectl cordon online-beijing-master1 # å°†Masterå‰”é™¤è¯¥é›†ç¾¤ kubectl drain online-beijing-master1 --ignore-daemonsets # ç„¶åå°†å…¶èŠ‚ç‚¹ä»é›†ç¾¤ä¸­åˆ é™¤ kubectl delete node online-beijing-master1 è¿™é‡Œæä¸€ä¸ªå°é—®é¢˜: å½“ä½ ä½¿ç”¨kubectl drainçš„æ—¶å€™,å¦‚æœä½ å½“å‰çš„Masterå·²ç»æœ‰æŒ‚è½½çš„emptyDiréœ€è¦ä½¿ç”¨--delete-emptydir-data è¿›è¡Œåˆ é™¤ã€‚ TODO: åˆ é™¤å‰è¯·æ³¨æ„å¤‡ä»½æ‚¨çš„æ•°æ®ã€‚\n","date":"2022-12-29T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/BaseKubeadmHA/","title":"åˆ©ç”¨Kubeadmè¿›è¡Œå¤šMasteré«˜å¯ç”¨éƒ¨ç½²"},{"content":"Kubernetesä¸­Cgroupæ³„æ¼é—®é¢˜ Cgorupæ–‡æ¡£: https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\nç»å¤§å¤šæ•°çš„kubernetesé›†ç¾¤éƒ½æœ‰è¿™ä¸ªéšæ‚£ã€‚åªä¸è¿‡ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ³„æ¼å¾—æ¯”è¾ƒæ…¢ï¼Œè¿˜æ²¡æœ‰è¡¨ç°å‡ºæ¥è€Œå·²ã€‚\nä¸€ä¸ªpodå¯èƒ½æ³„æ¼ä¸¤ä¸ªmemory cgroupæ•°é‡é…é¢ã€‚å³ä½¿podç™¾åˆ†ä¹‹ç™¾å‘ç”Ÿæ³„æ¼ï¼Œ é‚£ä¹Ÿéœ€è¦ä¸€ä¸ªèŠ‚ç‚¹é”€æ¯è¿‡ä¸‰ä¸‡å¤šä¸ªpodä¹‹åï¼Œæ‰ä¼šé€ æˆåç»­podåˆ›å»ºå¤±è´¥ã€‚\nä¸€æ—¦è¡¨ç°å‡ºæ¥ï¼Œè¿™ä¸ªèŠ‚ç‚¹å°±å½»åº•ä¸å¯ç”¨äº†ï¼Œå¿…é¡»é‡å¯æ‰èƒ½æ¢å¤ã€‚\næ•…éšœè¡¨ç° è¯¥å†…å®¹çš„æ•…éšœä¿¡æ¯å·²ç»æäº¤ç»™Github: https://github.com/kubernetes/kubernetes/issues/112940 æˆ‘åœ¨æœåŠ¡å™¨ä¸­æ›´æ–°Podå‡ºç°å¦‚ä¸‹é”™è¯¯ cannot allocate memory\nunable to ensure pod container exists: failed to create container for [kubepods burstable podd5dafc96-2bcd-40db-90fd-c75758746a7a] : mkdir /sys/fs/cgroup/memory/kubepods/burstable/podd5dafc96-2bcd-40db-90fd-c75758746a7a: cannot allocate memory ä½¿ç”¨dmesgæŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—çš„é”™è¯¯å†…å®¹ä¿¡æ¯\nSLUB: Unable to allocate memory on node -1 æœåŠ¡å™¨é…ç½®ä¿¡æ¯ æ“ä½œç³»ç»Ÿ: CentOS Linux release 7.9.2009 (Core) ç³»ç»Ÿå†…æ ¸: 3.10.0-1160.el7.x86_64 Kubernetes: 1.17.9 dockerVersion: 20.10.7 é—®é¢˜åŸå› 1 Kubernetesåœ¨1.9ç‰ˆæœ¬å¼€å¯äº†å¯¹kmemçš„æ”¯æŒ,å› æ­¤ 1.9ä»¥åçš„æ‰€æœ‰ç‰ˆæœ¬éƒ½æœ‰è¯¥é—®é¢˜ï¼Œä½†å¿…é¡»æ­é…3.xå†…æ ¸çš„æœºå™¨æ‰ä¼šå‡ºé—®é¢˜ã€‚ä¸€æ—¦å‡ºç°ä¼šå¯¼è‡´æ–° pod æ— æ³•åˆ›å»ºï¼Œå·²æœ‰ podä¸å—å½±å“ï¼Œä½†pod æ¼‚ç§»åˆ°æœ‰é—®é¢˜çš„èŠ‚ç‚¹å°±ä¼šå¤±è´¥ï¼Œç›´æ¥å½±å“ä¸šåŠ¡ç¨³å®šæ€§ã€‚å› ä¸ºæ˜¯å†…å­˜æ³„éœ²ï¼Œç›´æ¥é‡å¯æœºå™¨å¯ä»¥æš‚æ—¶è§£å†³ï¼Œä½†è¿˜ä¼šå†æ¬¡å‡ºç°ã€‚ cgroupçš„kmem accountç‰¹æ€§åœ¨3.x å†…æ ¸ä¸Šæœ‰å†…å­˜æ³„éœ²é—®é¢˜ï¼Œå¦‚æœå¼€å¯äº†kmem accountç‰¹æ€§ä¼šå¯¼è‡´å¯åˆ†é…å†…å­˜è¶Šæ¥è¶Šå°‘ï¼Œç›´åˆ°æ— æ³•åˆ›å»ºæ–° pod æˆ–èŠ‚ç‚¹å¼‚å¸¸ã€‚\nkmem account æ˜¯cgroup çš„ä¸€ä¸ªæ‰©å±•ï¼Œå…¨ç§°CONFIG_MEMCG_KMEMï¼Œå±äºæœºå™¨é»˜è®¤é…ç½®ï¼Œæœ¬èº«æ²¡å•¥é—®é¢˜ï¼Œåªæ˜¯è¯¥ç‰¹æ€§åœ¨ 3.10 çš„å†…æ ¸ä¸Šå­˜åœ¨æ¼æ´æœ‰å†…å­˜æ³„éœ²é—®é¢˜ï¼Œ4.xçš„å†…æ ¸ä¿®å¤äº†è¿™ä¸ªé—®é¢˜ã€‚ å› ä¸º kmem account æ˜¯ cgroup çš„æ‰©å±•èƒ½åŠ›ï¼Œå› æ­¤runcã€dockerã€k8s å±‚é¢ä¹Ÿè¿›è¡Œäº†è¯¥åŠŸèƒ½çš„æ”¯æŒï¼Œå³é»˜è®¤éƒ½æ‰“å¼€äº†kmem å±æ€§ã€‚ å› ä¸º3.10 çš„å†…æ ¸å·²ç»æ˜ç¡®æç¤º kmem æ˜¯å®éªŒæ€§è´¨ï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨è¯¥ç‰¹æ€§ï¼Œæ‰€ä»¥è¿™å…¶å®ä¸ç®—å†…æ ¸çš„é—®é¢˜ï¼Œæ˜¯ k8s å…¼å®¹é—®é¢˜ã€‚ é—®é¢˜åŸå› 2 memcgæ˜¯ Linux å†…æ ¸ä¸­ç”¨äºç®¡ç† cgroup å†…å­˜çš„æ¨¡å—ï¼Œæ•´ä¸ªç”Ÿå‘½å‘¨æœŸåº”è¯¥æ˜¯è·Ÿéš cgroup çš„ï¼Œä½†æ˜¯åœ¨ä½ç‰ˆæœ¬å†…æ ¸ä¸­(å·²çŸ¥3.10)ï¼Œä¸€æ—¦ç»™æŸä¸ª memory cgroup å¼€å¯ kmem accounting ä¸­çš„ memory.kmem.limit_in_bytes å°±å¯èƒ½ä¼šå¯¼è‡´ä¸èƒ½å½»åº•åˆ é™¤ memcg å’Œå¯¹åº”çš„ cssidï¼Œä¹Ÿå°±æ˜¯è¯´åº”ç”¨å³ä½¿å·²ç»åˆ é™¤äº† cgroup (/sys/fs/cgroup/memory ä¸‹å¯¹åº”çš„ cgroup ç›®å½•å·²ç»åˆ é™¤), ä½†åœ¨å†…æ ¸ä¸­æ²¡æœ‰é‡Šæ”¾ cssidï¼Œå¯¼è‡´å†…æ ¸è®¤ä¸ºçš„ cgroup çš„æ•°é‡å®é™…æ•°é‡ä¸ä¸€è‡´ï¼Œæˆ‘ä»¬ä¹Ÿæ— æ³•å¾—çŸ¥å†…æ ¸è®¤ä¸ºçš„ cgroup æ•°é‡æ˜¯å¤šå°‘ã€‚ è¿™ä¸ªé—®é¢˜å¯èƒ½ä¼šå¯¼è‡´åˆ›å»ºå®¹å™¨å¤±è´¥ï¼Œå› ä¸ºåˆ›å»ºå®¹å™¨ä¸ºå…¶éœ€è¦åˆ›å»º cgroup æ¥åšéš”ç¦»ï¼Œè€Œä½ç‰ˆæœ¬å†…æ ¸æœ‰ä¸ªé™åˆ¶ï¼šå…è®¸åˆ›å»ºçš„ cgroup æœ€å¤§æ•°é‡å†™æ­»ä¸º 65535ï¼Œå¦‚æœèŠ‚ç‚¹ä¸Šç»å¸¸åˆ›å»ºå’Œé”€æ¯å¤§é‡å®¹å™¨å¯¼è‡´åˆ›å»ºå¾ˆå¤š cgroupï¼Œåˆ é™¤å®¹å™¨ä½†æ²¡æœ‰å½»åº•åˆ é™¤ cgroup é€ æˆæ³„éœ²(çœŸå®æ•°é‡æˆ‘ä»¬æ— æ³•å¾—çŸ¥)ï¼Œåˆ°è¾¾ 65535 åå†åˆ›å»ºå®¹å™¨å°±ä¼šæŠ¥åˆ›å»º cgroup å¤±è´¥å¹¶æŠ¥é”™ no space left on deviceï¼Œä½¿ç”¨ kubernetes æœ€ç›´è§‚çš„æ„Ÿå—å°±æ˜¯ pod åˆ›å»ºä¹‹åæ— æ³•å¯åŠ¨æˆåŠŸã€‚\nè§£å†³æ–¹æ¡ˆ ç›®å‰å®˜æ–¹ç»™å‡ºçš„è§£å†³æ–¹æ¡ˆå¦‚ä¸‹:\nkernel upgrade to 4.0+: Update kernel rebuild the kubelet with nokmem args. See nokmem Set cgroup.memory=nokmem in grub: see grub è§£å†³æ–¹æ¡ˆä¸€ æ„Ÿè°¢æä¾›çš„è§£å†³æ–¹æ¡ˆ: https://cloud.tencent.com/developer/article/1739289 https://github.com/torvalds/linux/commit/d6e0b7fa11862433773d986b5f995ffdf47ce672 https://support.mesosphere.com/s/article/Critical-Issue-KMEM-MSPH-2018-0006 è¿™ç§æ–¹å¼çš„ç¼ºç‚¹æ˜¯ï¼š\n1ã€è¦å‡çº§æ‰€æœ‰èŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹é‡å¯çš„è¯å·²æœ‰ pod è‚¯å®šè¦æ¼‚ç§»ï¼Œå¦‚æœèŠ‚ç‚¹è§„æ¨¡å¾ˆå¤§ï¼Œè¿™ä¸ªå‡çº§æ“ä½œä¼šå¾ˆç¹çï¼Œä¸šåŠ¡éƒ¨é—¨ä¹Ÿä¼šæœ‰æ„è§ï¼Œè¦äº‹å…ˆæ²Ÿé€šã€‚ 2ã€è¿™ä¸ªé—®é¢˜å½’æ ¹ç»“åº•æ˜¯è½¯ä»¶å…¼å®¹é—®é¢˜ï¼Œ3.x è‡ªå·±éƒ½è¯´äº†ä¸æˆç†Ÿï¼Œä¸å»ºè®®ä½ ä½¿ç”¨è¯¥ç‰¹æ€§ï¼Œk8sã€dockerå´ è¿˜è¦å¼€å¯è¿™ä¸ªå±æ€§ï¼Œé‚£å°±ä¸æ˜¯å†…æ ¸çš„è´£ä»»ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯äº‘ä¸Šæœºå™¨ï¼Œæƒ³æ›¿æ¢4.x å†…æ ¸éœ€è¦è™šæœºå›¢é˜Ÿåšè¶³å¤Ÿçš„æµ‹è¯•å’Œè¯„å®¡ï¼Œå› æ­¤è¿™æ˜¯ä¸ªé•¿æœŸæ–¹æ¡ˆï¼Œä¸èƒ½ç«‹åˆ»è§£å†³é—®é¢˜ã€‚ 3ã€å·²æœ‰ä¸šåŠ¡åœ¨ 3.x è¿è¡Œæ­£å¸¸ï¼Œä¸ä»£è¡¨å¯ä»¥åœ¨ 4.x ä¹Ÿè¿è¡Œæ­£å¸¸ï¼Œå³å…¨é‡å‡çº§å†…æ ¸ä¹‹å‰éœ€è¦åšè¶³å¤Ÿçš„æµ‹è¯•ï¼Œå°¤å…¶æ˜¯æœ‰äº›ä¸šåŠ¡éœ€æ±‚å¯¹osåšè¿‡å®šåˆ¶ã€‚ è§£å†³æ–¹æ¡ˆ2 ä¿®æ”¹è™šæœºå¯åŠ¨çš„å¼•å¯¼é¡¹ grub ä¸­çš„cgroup.memory=nokmemï¼Œè®©æœºå™¨å¯åŠ¨æ—¶ç›´æ¥ç¦ç”¨ cgroupçš„ kmem å±æ€§\nvim /etc/default/grub GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\u0026#34;$(sed \u0026#39;s, release .*$,,g\u0026#39; /etc/system-release)\u0026#34; GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\u0026#34;console\u0026#34; GRUB_CMDLINE_LINUX=\u0026#34;crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet cgroup.memory=nokmem\u0026#34; GRUB_DISABLE_RECOVERY=\u0026#34;true\u0026#34; æ›´æ”¹å®Œæˆåä½ éœ€è¦ç”Ÿæˆä¸€ä¸‹æ–°çš„cgroupé…ç½®.\n/usr/sbin/grub2-mkconfig -o /boot/grub2/grub.cfg reboot # é‡å¯æœåŠ¡å™¨ è§£å†³æ–¹æ¡ˆ3 å¦‚æœä½ æƒ³åœ¨Kubernetesä¸­ç¦ç”¨è¯¥å±æ€§ã€‚issue ä¸­ä¸€èˆ¬å»ºè®®ä¿®æ”¹ kubeletä»£ç å¹¶é‡æ–°ç¼–è¯‘ã€‚\nå¯¹äºv1.13åŠå…¶ä¹‹å‰ç‰ˆæœ¬çš„kubeletï¼Œéœ€è¦æ‰‹åŠ¨æ›¿æ¢ä»¥ä¸‹ä¸¤ä¸ªå‡½æ•°ã€‚\nvendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go func EnableKernelMemoryAccounting(path string) error { return nil } func setKernelMemory(path string, kernelMemoryLimit int64) error { return nil } é‡æ–°ç¼–è¯‘å¹¶æ›¿æ¢ kubelet\nmake WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS=\u0026#34;-N -l\u0026#34; å¯¹äºv1.14åŠå…¶ä¹‹åç‰ˆæœ¬çš„kubelet,é€šè¿‡æ·»åŠ BUILDTAGSæ¥ç¦æ­¢ kmem accounting.\nmake BUILDTAGS=\u0026#34;nokmem\u0026#34; WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS=\u0026#34;-N -l\u0026#34; é‡åˆ°1.16 ç‰ˆæœ¬çš„BUILDTAGS=â€nokmemâ€œç¼–è¯‘å‡ºæ¥çš„ let è¿˜æ˜¯æœ‰é—®é¢˜ï¼Œè¿˜æ˜¯é€šè¿‡ä¿®æ”¹ä»£ç çš„æ–¹å¼ä½¿å…¶ç”Ÿæ•ˆ\nvendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/kmem.go package fs import ( \u0026#34;errors\u0026#34; ) func EnableKernelMemoryAccounting(path string) error { return nil } func setKernelMemory(path string, kernelMemoryLimit int64) error { return errors.New(\u0026#34;kernel memory accounting disabled in this runc build\u0026#34;) } ç¼–è¯‘å‰ï¼Œå¯ä»¥ç¼–è¾‘ä¸‹æ–‡ä»¶ hack/lib/version.shï¼Œå°† KUBE_GIT_TREE_STATE=\u0026quot;dirty\u0026quot; æ”¹ä¸º KUBE_GIT_TREE_STATE=\u0026quot;clean\u0026quot;ï¼Œç¡®ä¿ç‰ˆæœ¬å·å¹²å‡€ã€‚\nå½±å“èŒƒå›´ k8såœ¨1.9ç‰ˆæœ¬å¼€å¯äº†å¯¹kmemçš„æ”¯æŒï¼Œå› æ­¤1.9ä»¥åçš„æ‰€æœ‰ç‰ˆæœ¬éƒ½æœ‰è¯¥é—®é¢˜,ä½†å¿…é¡»æ­é… 3.xå†…æ ¸çš„æœºå™¨æ‰ä¼šå‡ºé—®é¢˜ã€‚ä¸€æ—¦å‡ºç°ä¼šå¯¼è‡´æ–°podæ— æ³•åˆ›å»º,å·²æœ‰ podä¸å—å½±å“ï¼Œä½†pod æ¼‚ç§»åˆ°æœ‰é—®é¢˜çš„èŠ‚ç‚¹å°±ä¼šå¤±è´¥ï¼Œç›´æ¥å½±å“ä¸šåŠ¡ç¨³å®šæ€§ã€‚å› ä¸ºæ˜¯å†…å­˜æ³„éœ²ï¼Œç›´æ¥é‡å¯æœºå™¨å¯ä»¥æš‚æ—¶è§£å†³ï¼Œä½†è¿˜ä¼šå†æ¬¡å‡ºç°ã€‚\nå¤§æ¦‚å¾—åŸç†ç†è§£ kemeæ˜¯ä»€ä¹ˆ? kmemæ˜¯Cgroupçš„ä¸€ä¸ªæ‰©å±•ï¼Œå…¨ç§°CONFIG_MEMCG_KMEMï¼Œå±äºæœºå™¨é»˜è®¤é…ç½®ã€‚\nå†…æ ¸å†…å­˜ä¸ç”¨æˆ·å†…å­˜ï¼š\nå†…æ ¸å†…å­˜ï¼šä¸“ç”¨äºLinuxå†…æ ¸ç³»ç»ŸæœåŠ¡ä½¿ç”¨ï¼Œæ˜¯ä¸å¯swapçš„ï¼Œå› è€Œè¿™éƒ¨åˆ†å†…å­˜éå¸¸å®è´µçš„ã€‚ä½†ç°å®ä¸­å­˜åœ¨å¾ˆå¤šé’ˆå¯¹å†…æ ¸å†…å­˜èµ„æºçš„æ”»å‡»ï¼Œå¦‚ä¸æ–­åœ°forkæ–°è¿›ç¨‹ä»è€Œè€—å°½ç³»ç»Ÿèµ„æºï¼Œå³æ‰€è°“çš„â€œfork bombâ€ã€‚\nä¸ºäº†é˜²æ­¢è¿™ç§æ”»å‡»ï¼Œç¤¾åŒºä¸­æè®®é€šè¿‡linuxå†…æ ¸é™åˆ¶ cgroupä¸­çš„kmem å®¹é‡ï¼Œä»è€Œé™åˆ¶æ¶æ„è¿›ç¨‹çš„è¡Œä¸ºï¼Œå³kernel memory accountingæœºåˆ¶ã€‚\nä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹KMEMæ˜¯å¦æ‰“å¼€ï¼š\ncat /boot/config-`uname -r`|grep CONFIG_MEMCG CONFIG_MEMCG=y CONFIG_MEMCG_SWAP=y CONFIG_MEMCG_SWAP_ENABLED=y CONFIG_MEMCG_KMEM=y cgroupä¸kmemæœºåˆ¶ ä½¿ç”¨ cgroup é™åˆ¶å†…å­˜æ—¶ï¼Œæˆ‘ä»¬ä¸ä½†éœ€è¦é™åˆ¶å¯¹ç”¨æˆ·å†…å­˜çš„ä½¿ç”¨ï¼Œä¹Ÿéœ€è¦é™åˆ¶å¯¹å†…æ ¸å†…å­˜çš„ä½¿ç”¨ã€‚kernel memory accounting æœºåˆ¶ä¸º cgroup çš„å†…å­˜é™åˆ¶å¢åŠ äº† stack pagesï¼ˆä¾‹å¦‚æ–°è¿›ç¨‹åˆ›å»ºï¼‰ã€slab pages(SLAB/SLUBåˆ†é…å™¨ä½¿ç”¨çš„å†…å­˜)ã€sockets memory pressureã€tcp memory pressureç­‰ï¼Œä»¥ä¿è¯ kernel memory ä¸è¢«æ»¥ç”¨ã€‚\nå½“ä½ å¼€å¯äº†kmem æœºåˆ¶ï¼Œå…·ä½“ä½“ç°åœ¨ memory.kmem.limit_in_bytes è¿™ä¸ªæ–‡ä»¶ä¸Šï¼š\n/sys/fs/cgroup/memory/kubepods/pod632f736f-5ef2-11ea-ad9e-fa163e35f5d4/memory.kmem.limit_in_bytes å®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬å°† memory.kmem.limit_in_bytes è®¾ç½®æˆå¤§äº memory.limit_in_bytesï¼Œä»è€Œåªé™åˆ¶åº”ç”¨çš„æ€»å†…å­˜ä½¿ç”¨ã€‚\ndockerä¸k8sä½¿ç”¨kmem ä»¥ä¸Šæè¿°éƒ½æ˜¯cgroupå±‚é¢å³æœºå™¨å±‚é¢ï¼Œä½†æ˜¯ runc å’Œ docker å‘ç°æœ‰è¿™ä¸ªå±æ€§ä¹‹åï¼Œåœ¨åæ¥çš„ç‰ˆæœ¬ä¸­ä¹Ÿæ”¯æŒäº† kmem ï¼Œk8s å‘ç° dockeræ”¯æŒï¼Œä¹Ÿåœ¨ 1.9 ç‰ˆæœ¬å¼€å§‹æ”¯æŒã€‚\n1.9ç‰ˆæœ¬åŠä¹‹åï¼Œkubelet æ‰å¼€å¯ kmem å±æ€§\nkubelet çš„è¿™éƒ¨åˆ†ä»£ç ä½äºï¼š\nhttps://github.com/kubernetes/kubernetes/blob/release-1.12/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go#L70-L106 å¯¹äºk8sã€docker è€Œè¨€ï¼Œkmem å±æ€§å±äºæ­£å¸¸è¿­ä»£å’Œä¼˜åŒ–ï¼Œè‡³äº3.xçš„å†…æ ¸ä¸Šå­˜åœ¨ bug ä¸èƒ½å…¼å®¹ï¼Œä¸æ˜¯k8s å…³å¿ƒçš„é—®é¢˜ã€‚ä½† issue ä¸­ä¸æ–­æœ‰äººåé¦ˆï¼Œå› æ­¤åœ¨ k8s 1.14 ç‰ˆæœ¬çš„ kubelet ä¸­ï¼Œå¢åŠ äº†ä¸€ä¸ªç¼–è¯‘é€‰é¡¹ make BUILDTAGS=â€œnokmemâ€ï¼Œå°±å¯ä»¥ç¼–è¯‘ kubelet æ—¶å°±ç¦ç”¨ kmemï¼Œé¿å…æ‰è¿™ä¸ªé—®é¢˜ã€‚è€Œ1.8 åˆ°1.14 ä¸­é—´çš„ç‰ˆæœ¬ï¼Œåªèƒ½é€‰æ‹©æ›´æ”¹ kubelet çš„ä»£ç ã€‚\n","date":"2022-10-08T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/MemoryLeakageAnalysis/","title":"Kubernetesä½ç‰ˆæœ¬ä¸­å†…å­˜æ³„æ¼é—®é¢˜"},{"content":"Ansible-with_items é€šè¿‡with_itemsè¿›è¡Œå¾ªç¯ è¯­æ³•\n{{ item }}: ä¸ºè¯»å–with_itemsçš„å›ºå®šå†™æ³• with_items: æ˜¯ä¸€ä¸ªåˆ—è¡¨,ä¸‹é¢å¯ä»¥æœ‰å¤šä¸ªä¸åŒçš„å†…å®¹ - hosts: test remote_user: root gather_facts: false vars_files: ./public_vars.yaml tasks: - name: Services Http start service: name={{ item }} state=started with_items: - httpd - firewalld æ™®é€šå†™æ³• - hosts: test remote_user: root gather_facts: false vars_files: ./public_vars.yaml tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Install httpd yum: name=\u0026#34;httpd\u0026#34; state=present - name: Services Http start service: name={{ item }} state=started with_items: - httpd - firewalld ä½¿ç”¨å˜é‡çš„å¾ªç¯å†™æ³• - hosts: test remote_user: root gather_facts: true tasks: - name: Install httpd yum: name={{ packages }} state=present vars: packages: - httpd - pcre-devel ä½¿ç”¨å˜é‡å­—å…¸å¾ªç¯æ–¹å¼æ‰¹é‡åˆ›å»ºç”¨æˆ· - hosts: test remote_user: root gather_facts: false vars_files: ./public_vars.yaml tasks: - name: Add Users user: name={{ item.name }} groups={{ item.groups }} state=present with_items: - { name: \u0026#34;alex\u0026#34;,groups: \u0026#34;test\u0026#34;} - { name: \u0026#34;alex1\u0026#34;,groups: \u0026#34;test\u0026#34;} ä½¿ç”¨å˜é‡å­—å…¸å¾ªç¯æ‹·è´æ–‡ä»¶ - hosts: test remote_user: root gather_facts: false tasks: - name: Add Users copy: src: \u0026#39;{{ item.src }}\u0026#39; dest: \u0026#39;{{ item.dest }}\u0026#39; mode: \u0026#39;{{ item.mode }}\u0026#39; with_items: - { src: \u0026#34;./1.txt\u0026#34;, dest: \u0026#34;/tmp\u0026#34;, mode: 0644} - { src: \u0026#34;./2.txt\u0026#34;, dest: \u0026#34;/tmp\u0026#34;, mode: 0644} Ansible-Handlers é€šè¿‡notifyè¿›è¡Œç›‘æ§-\u0026gt;é€šè¿‡handlersè§¦å‘ å…³äºHandlerçš„ä¸€äº›å°æ³¨æ„äº‹é¡¹\næ— è®ºä½ æ‹¥æœ‰å¤šå°‘ä¸ªnotifyé€šçŸ¥ç›¸åŒçš„handlers,handlersä»…ä»…ä¼šåœ¨æ‰€æœ‰tasksæ­£å¸¸æ‰§è¡Œå®Œæˆåè¿è¡Œä¸€æ¬¡\nåªæœ‰taskså‘ç”Ÿæ”¹å˜äº†æ‰ä¼šé€šçŸ¥handlers,æ²¡æœ‰æ”¹å˜åˆ™ä¸ä¼šè§¦å‘handlers\nä¸èƒ½ä½¿ç”¨handlersæ›¿ä»£tasks,å› ä¸ºhandlersæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„tasks\nnotifyçš„åç§°è¦ä¸handlers`çš„åç§°ä¸€è‡´\n- hosts: test remote_user: root gather_facts: false tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Install httpd yum: name=\u0026#34;httpd\u0026#34; state=present notify: Debug Message register: install_info handlers: - name: Debug Message debug: msg: \u0026#39;{{install_info}}\u0026#39; Ansible-Tags æ ¹æ®playbookä¸­çš„æŒ‡å®šæ ‡ç­¾çš„å†…å®¹è¿›è¡Œæ‰§è¡Œã€è°ƒè¯•ç­‰æ“ä½œ.\nå¯¹ä¸€ä¸ªtasksæŒ‡å®šä¸€ä¸ªtagsæ ‡ç­¾ å¯¹ä¸€ä¸ªtasksæŒ‡å®šå¤šä¸ªtagsæ ‡ç­¾(çœŸæ²¡å•¥æ„ä¹‰,æ„Ÿè§‰ä¸å®ç”¨ã€‚) å¯¹å¤šä¸ªtasksæŒ‡å®šä¸€ä¸ªæ ‡ç­¾ æ‰§è¡ŒæŒ‡å®šTagsæ ‡ç­¾å†…å®¹ tags: æŒ‡å®šæ ‡ç­¾åç§° - hosts: test remote_user: root gather_facts: false tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: - authorized_key_hosts -t: é€šè¿‡-té€‰é¡¹å‚æ•°è¿›è¡Œé€‰æ‹©æŒ‡å®šæ ‡ç­¾è¿›è¡Œè¿è¡Œ ansible-playbook 1.yaml -t authorized_key_hosts -i hosts è·³è¿‡æŒ‡å®šæ ‡ç­¾æ‰§è¡Œå…¶ä»–å†…å®¹ è·³è¿‡æŒ‡å®šçš„æ ‡ç­¾å†…å®¹,æ‰§è¡Œæ ‡ç­¾å†…å®¹å¤–çš„å…¶ä»–å†…å®¹ ansible-playbook 1.yaml --skip-tags \u0026#34;authorized_key_hosts\u0026#34; -i hosts Ansible-Include ä¸€ä¸ªå¯ä»¥å°†playbookç®€å•çš„è¿›è¡Œå¤ç”¨çš„ä¸€ä¸ªåŠŸèƒ½!\nç®€å•åº”ç”¨ ç¼–å†™ä¸€ä¸ªé‡å¯httpæœåŠ¡çš„é…ç½®\n- name: Start HTTP service: name=httpd state=restarted PlayBookä¸­çš„åº”ç”¨\nincludeï¼š æŸ¥æ‰¾çš„æ–‡ä»¶ç›®å½•ä¸ºä½ å½“å‰æ‰€åœ¨çš„ç›®å½•,å¯ä»¥é€šè¿‡pwdå‘½ä»¤è¿›è¡ŒæŸ¥çœ‹ã€‚ - hosts: test remote_user: root gather_facts: true tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Install HTTP yum: name=\u0026#34;httpd\u0026#34; state=present - name: Restart HTTP include: starthttp.yaml # åŒ…å«ä½ åˆšåˆšå†™çš„é…ç½® # include_tasks: starthttpd.yaml # ä¸¤ç§å†™æ³•éƒ½å¯ä»¥ å¤šä¸ªplaybookåˆæˆ å¦‚æœä½ å†™çš„playbookå­˜åœ¨å¤šä¸ªæ–‡ä»¶,ä½ åªæƒ³æ‰§è¡Œä¸€ä¸ªplaybook,é‚£ä¹ˆå¯ä»¥ä½¿ç”¨import_playbookã€‚\nimport_playbook: å¼•å…¥ä½ éœ€è¦çš„playbookæ–‡ä»¶,å¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„playbookæ–‡ä»¶ - import_playbook: ./tasks1.yaml - import_playbook: ./tasks2.yaml Ansible-ignore_errors åœ¨Ansibleä¸­è¿›è¡Œé”™è¯¯çš„å¿½ç•¥\n- hosts: test remote_user: root gather_facts: true tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Ingoring command: /bin/false ignore_errors: yes è¾“å‡ºç»“æœå¤§æ¦‚æ˜¯è¿™æ ·çš„\nPLAY [test] TASK [Ingoring] ***************************************************************************************************************************************************************************************************************************fatal: [10.1.6.5]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: [\u0026#34;/bin/false\u0026#34;], \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.026834\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-08-29 02:17:00.089749\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;non-zero return code\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;start\u0026#34;: \u0026#34;2022-08-29 02:17:00.062915\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout_lines\u0026#34;: []} ...ignoring PLAY RECAP ********************************************************************************************************************************************************************************************************************************10.1.6.5 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=1 Ansible-changed_when é€šå¸¸ç”¨äºå¤±è´¥åæ‰€æ‰§è¡Œä¸€äº›æ“ä½œ: ä¾‹å¦‚å¤±è´¥åå¼ºåˆ¶è°ƒç”¨handlersã€å¤±è´¥åå¼ºåˆ¶åˆ é™¤ç­‰.\né€šå¸¸è€Œè¨€ï¼Œå¦‚æœä»»åŠ¡å¤±è´¥å¹¶ä¸”playåœ¨è¯¥ä¸»æœºä¸Šä¸­æ­¢ï¼Œåˆ™æ”¶åˆ°playä¸­æ—©å‰ä»»åŠ¡é€šçŸ¥çš„å¤„ç†ç¨‹åºå°†ä¸ä¼šè¿è¡Œã€‚å¦‚æœåœ¨playä¸­è®¾ç½®force_handlers: yeså…³é”®å­—ï¼Œåˆ™å³ä½¿playå› ä¸ºåç»­ä»»åŠ¡å¤±è´¥è€Œä¸­æ­¢ä¹Ÿä¼šè°ƒç”¨è¢«é€šçŸ¥çš„å¤„ç†ç¨‹åºã€‚\nforce_handlers - hosts: test remote_user: root gather_facts: true force_handlers: yes # å¼ºåˆ¶è°ƒç”¨handlers tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Test False command: echo \u0026#34;This is Force \u0026#34; notify: Restart HTTP service - name: Install available yum: name=dbdbdb state=present handlers: - name: Restart HTTP service service: name=httpd state=restarted è™½ç„¶ä»»åŠ¡æ˜¯å¤±è´¥çš„,ä½†æ˜¯ä¾æ—§è°ƒç”¨äº†æœ€åæ‰§è¡Œçš„handlers\n[root@localhost ansible_linux]# ansible-playbook 1.yaml -i hosts TASK [Install available] ******************************************************************************************************************************************************************************************************************fatal: [10.1.6.57]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;failures\u0026#34;: [\u0026#34;No package dbdbdb available.\u0026#34;], \u0026#34;msg\u0026#34;: \u0026#34;Failed to install some of the specified packages\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;results\u0026#34;: []} RUNNING HANDLER [Restart HTTP service] ****************************************************************************************************************************************************************************************************changed: [10.1.6.57] changed_when å½“å‰å‘½ä»¤ç¡®ä¿ä¸ä¼šå¯¹è¢«æ§ç«¯ä¸»æœºè¿›è¡Œå˜æ›´çš„æ—¶å€™,å¯ä»¥ä½¿ç”¨changer_whenæ¥è¿›è¡Œå¿½ç•¥æç¤ºä¸­çš„changed\n- hosts: test remote_user: root gather_facts: true force_handlers: yes # å¼ºåˆ¶è°ƒç”¨handlers tasks: - name: Set authorized_key in dest hosts authorized_key: user: root key: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/root/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; register: result_auth_info tags: authorized_key_hosts - name: Check HTTP shell: ps -aux | grep httpd changed_when: false # è¿™æ ·ps -aux | grep httpd å†ä¹Ÿä¸ä¼šæç¤ºchanged - name: Message debug: msg: \u0026#34;{{ ansible_distribution }}\u0026#34; changed_whenè¿˜å¯ä»¥æ£€æŸ¥tasksä»»åŠ¡è¿”å›çš„ç»“æœ\næŸ¥æ‰¾è¾“å‡ºå½“ä¸­æ˜¯å¦å­˜åœ¨successfulyå¦‚æœæ²¡æœ‰åˆ™ä¸æ‰§è¡Œ ","date":"2022-08-29T00:00:00Z","permalink":"http://localhost:1313/ansible/AnsibleTaskControll/","title":"Ansible-ä»»åŠ¡æ§åˆ¶"},{"content":"Ansibleæ€ä¹ˆå®šä¹‰å˜é‡ é€šè¿‡playbookä¸­çš„playè¿›è¡Œå˜é‡çš„å®šä¹‰ é€šè¿‡inventoryä¸»æœºæ¸…å•è¿›è¡Œå˜é‡å®šä¹‰ é€šè¿‡æ‰§è¡Œplaybookçš„æ—¶å€™å¢åŠ -eé€‰é¡¹è¿›è¡Œå®šä¹‰ é€šè¿‡Playbookä¸­çš„varså®šä¹‰å˜é‡ åœ¨Playbookä¸­é€šè¿‡å†™å…¥varsè¯­æ³•å®šä¹‰å˜é‡ é€šè¿‡{{å˜é‡å}}è¿›è¡Œå¼•ç”¨! - hosts: test remote_user: root vars: - httpd_package: httpd tasks: - name: Install DepencyEnvorment yum: name: {{httpd_package}} state: present update_cache: yes é€šè¿‡å®šä¹‰å˜é‡æ–‡ä»¶è¿›è¡Œä½¿ç”¨ å®šä¹‰ä¸€ä¸ªåå­—ä¸ºpublic_vars.yamlçš„å˜é‡é…ç½®æ–‡ä»¶ depence: [\u0026#39;openssl-devel\u0026#39;,\u0026#39;pcre-devel\u0026#39;,\u0026#39;zlib-devel\u0026#39;] æ³¨æ„: å½“ä½ å¼•ç”¨äº†å˜é‡æ–‡ä»¶ä¸­çš„å˜é‡,è¯·åœ¨è¯»å–å˜é‡çš„æ—¶å€™å¢åŠ åŒå¼•å·\u0026quot;\u0026quot;\n- hosts: test remote_user: root vars_files: - ./public_vars.yaml - ./public_vars2.yaml # å¦‚æœæ˜¯å¤šä¸ªå˜é‡çš„è¯ tasks: - name: \u0026#34;Install De\u0026#34; yum: name: \u0026#34;{{depence}}\u0026#34; # é€šè¿‡åŒå¼•å·å»å¼•å…¥å˜é‡å†…å®¹,ä¸ç„¶ä¼šæŠ¥é”™ state: present update_cache: no é€šè¿‡ç¼–è¾‘inventoryä¸»æœºæ¸…å•è¿›å®šä¹‰ è¿™ç§æ–¹æ³•ä¸€èˆ¬ç”¨çš„å¾ˆå°‘ [test] 10.1.6.205 [test:vars] file_name=group_sys å®˜æ–¹æ¨èçš„æ–¹æ³•: åœ¨é¡¹ç›®ç›®å½•ä¸­åˆ›å»ºä¸¤ä¸ªå˜é‡ç›®å½•host_varså’Œgroup_vars\ngroup_vars mkdir host_vars; mkdir group_vars åˆ›å»ºä¸€ä¸ªåŒåæ–‡ä»¶,ç”¨äºå†™å…¥å˜é‡å†…å®¹\nå¿…é¡»ä¸hostsæ¸…å•ä¸­çš„ç»„åä¿æŒä¸€è‡´,å¦‚æœä¸åŒåä¼šæŠ¥é”™ã€‚ä½†æ˜¯å¦‚æœä½ æƒ³è¦å¤šä¸ªé…ç½®æ–‡ä»¶ä½¿ç”¨åŒä¸€ä¸ªç»„ä¸­çš„å˜é‡,åªéœ€è¦åœ¨group_vars/allæ–°å»ºä¸€ä¸ªallæ–‡ä»¶,æ‰€æœ‰ç»„å¯ç”¨!\n[root@bogon ~]# cat group_vars/test file_name: group_sys host_vars åœ¨host_varsä¸­åˆ›å»ºä¸€ä¸ªæ–‡ä»¶,æ–‡ä»¶åä¸inventoryæ¸…å•ä¸­çš„ä¸»æœºåç§°è¦ä¿æŒå®Œå…¨ä¸€è‡´,å¦‚æœæ˜¯IPåœ°å€,åˆ™åˆ›å»ºç›¸åŒIPåœ°å€çš„æ–‡ä»¶å³å¯ vim host_vars/10.1.6.205 [root@bogon ~]# cat host_vars/10.1.6.205 file_name: group_sys ","date":"2022-08-15T00:00:00Z","permalink":"http://localhost:1313/ansible/AnsibleVarbles/","title":"Ansibleå˜é‡ç›¸å…³å†…å®¹"},{"content":"MySQLæ€§èƒ½ä¼˜åŒ–-ä¼˜åŒ–æ€è·¯ å¤§æ¦‚çš„ä¼˜åŒ–æ€è·¯åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªå†…å®¹\nç¡¬ä»¶å±‚é¢ä¼˜åŒ– ç³»ç»Ÿå±‚é¢ä¼˜åŒ– MySQLç‰ˆæœ¬é€‰æ‹©ä¼˜åŒ– MySQLä¸‰å±‚ç»“æ„åŠå‚æ•°ä¼˜åŒ– MySQLå¼€å‘è§„èŒƒ MySQLçš„ç´¢å¼•ä¼˜åŒ– MySQLçš„äº‹åŠ¡ä»¥åŠé”ä¼˜åŒ– MySQLæ¶æ„ä¼˜åŒ– MySQLå®‰å…¨ä¼˜åŒ– PS: ä¼˜åŒ–æ˜¯æœ‰é£é™©çš„,å¦‚æœä½ è¦ä¼˜åŒ–å°±è¦å˜æ›´ã€‚\nç¡¬ä»¶å±‚é¢ä¼˜åŒ– è¿™ä¸ªåœ°æ–¹å°±ç•¥è¿‡äº†å°±æ˜¯ä¸€äº›åŠ å¤§ç¡¬ä»¶é…ç½®çš„éœ€æ±‚.\nç³»ç»Ÿå±‚é¢ä¼˜åŒ– id: ç©ºé—²çŠ¶æ€,å¦‚æœæ•°å€¼è¶Šå¤§,è¡¨ç¤ºç©ºé—²çŠ¶æ€è¶Šå¤šã€‚å¦‚æœå¯èƒ½è¾¾åˆ°0çš„æƒ…å†µä¸‹,è¡¨ç¤ºå½“å‰CPUçš„æ ¸å¿ƒå¤„äºæ»¡è´Ÿè·çŠ¶æ€ã€‚ us: è¡¨ç¤ºå½“å‰CPUæ ¸å¿ƒæ•°é‡çš„ä½¿ç”¨ç‡ã€‚ sy: è¡¨ç¤ºCPUä¸å†…æ ¸äº¤äº’çš„é¢‘ç‡,å†…æ ¸ä¸CPUå¤„ç†è¯·æ±‚çš„å ç”¨,å¦‚æœæ­¤å‚æ•°é«˜,è¡¨ç¤ºå†…æ ¸å¾ˆå¿™ã€‚ wa: CPUä»å†…å­˜ä¸­åˆ·æ•°æ®åˆ°ç¡¬ç›˜ä¸­çš„å ç”¨,å¯èƒ½ä¼šå‡ºç°I/Oçš„é—®é¢˜ã€‚ [root@mysql-master ~]# top top - 15:05:11 up 35 days, 5:54, 2 users, load average: 0.00, 0.01, 0.05 Tasks: 225 total, 2 running, 223 sleeping, 0 stopped, 0 zombie %Cpu0 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu1 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu2 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu3 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu4 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu5 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu6 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu7 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 24522416 total, 14931524 free, 3675344 used, 5915548 buff/cache KiB Swap: 12386300 total, 12386300 free, 0 used. 20450988 avail Mem é€šè¿‡ top -Hp 10380 æŒ‡å®šå ç”¨é«˜çš„è¿›ç¨‹,å¯ä»¥çœ‹åˆ°å…·ä½“æ˜¯é‚£äº›çº¿ç¨‹å ç”¨è¿‡é«˜\nå‡è®¾ 1893çº¿ç¨‹å ç”¨è¿‡é«˜,å¯ä»¥ä»æ•°æ®åº“ä¸­æŸ¥çœ‹performance_schemaåº“ä¸­å…·ä½“çš„ä¿¡æ¯\nå®šä½æ“ä½œç³»ç»Ÿçº¿ç¨‹-\u0026gt;ä»ç³»ç»Ÿçº¿ç¨‹ä¸­å®šä½æ•°æ®åº“çº¿ç¨‹\n*************************** 38. row *************************** THREAD_ID: 128014 NAME: thread/sql/one_connection TYPE: FOREGROUND PROCESSLIST_ID: 127988 PROCESSLIST_USER: ooooo PROCESSLIST_HOST: 192.168.0.1 PROCESSLIST_DB: test PROCESSLIST_COMMAND: Sleep PROCESSLIST_TIME: 104 PROCESSLIST_STATE: NULL PROCESSLIST_INFO: ** PARENT_THREAD_ID: NULL ROLE: NULL INSTRUMENTED: YES HISTORY: YES CONNECTION_TYPE: SSL/TLS THREAD_OS_ID: 16165 *************************** 39. row *************************** å¦‚æœå¯èƒ½å­˜åœ¨çš„æ˜¯IOé—®é¢˜ æŸ¥è¯¢MySQLä¸­çš„sysåº“ä¸­å­˜åœ¨è®°å½•IOçš„è¡¨ å¦‚æœå­˜åœ¨IOé—®é¢˜ï¼š å¯ä»¥é€‰æ‹©ç”¨å†…å­˜æ¢å–æ—¶é—´çš„æ–¹æ³•â€¦\nmysql\u0026gt; use sys mysql\u0026gt; show tables; | x$io_by_thread_by_latency | | x$io_global_by_file_by_bytes | | x$io_global_by_file_by_latency | | x$io_global_by_wait_by_bytes | | x$io_global_by_wait_by_latency | MySQLç‰ˆæœ¬é€‰æ‹©ä¼˜åŒ– åœ¨è¿™é‡Œâ€¦ç¬”è€…éå¸¸æ¨èMySQL8.0x!!! åŒæ ·çš„æœºå™¨,8.0æ¯”5.7å¿«2.5å€å·¦å³å§\né€‰æ‹©ç¨³å®šç‰ˆ,é€‰æ‹©å¼€æºç¤¾åŒºçš„ç¨³å®šç‰ˆå’ŒGAç‰ˆæœ¬ é€‰æ‹©MySQLæ•°æ®åº“GAç‰ˆæœ¬å‘å¸ƒå6-12ä¸ªæœˆçš„GAåŒæ•°ç‰ˆæœ¬ è¦é€‰æ‹©å¼€å‘å…¼å®¹çš„MySQLç‰ˆæœ¬ MySQLä¸‰å±‚ç»“æ„åŠå‚æ•°ä¼˜åŒ– è¿æ¥å±‚ä¼˜åŒ– ä¸€åˆ‡æ ¹æ®è‡ªå·±æˆ–è€…é¡¹ç›®éœ€è¦è‡ªç”±è®¾ç½®å§!\nmax_connections = 1000 max_connect_errors = 999999 wait_timeout = 600 interactive_wait_timeout = 3600 net_read_timeout = 120 new_write_timeout = 120 max_allowed_packet = 500M Serverå±‚ä¼˜åŒ– ä¸€åˆ‡æ ¹æ®è‡ªå·±æˆ–è€…é¡¹ç›®éœ€è¦è‡ªç”±è®¾ç½®å§!\nsort_buffer_size = 8M sql_safe_updates = 1 slow_query_log = 1 long_query_time = 1 slow_query_log_file = /data/mysql/mysql-slow.log log_queries_not_using_indexes = 10 read_buffer_size = 2M read_rnd_buffer_size = 8M sort_buffer_size = 8M join_buffer_size = 8M key_buffer_size = 16M max_binlog_size = 500M max_execution_time = 28800 log_timestamps = SYSTEM init_connect = \u0026#34;set names utf8mb4\u0026#34; binlog_format = ROW event_scheduler = OFF lock_wait_timeout = sync_binlog = 1 Engineå±‚ä¼˜åŒ– ä¸€åˆ‡æ ¹æ®è‡ªå·±æˆ–è€…é¡¹ç›®éœ€è¦è‡ªç”±è®¾ç½®å§!\ntransaction-isolation = \u0026#34;READ-COMMITIED\u0026#34; innodb_data_home_dir = /xxx innodb_log_group_home_dir = /xxx innodb_log_file_size = 2048M innodb_log_files_in_group = 3 innodb_flush_log_at_trx_commit = 2 innodb_flush_method = O_DIRECT innodb_io_capacity = 1000 innodb_io_capacity_max = 4000 innodb_buffer_pool_size = 64G innodb_buffer_pool_instances = 4 innodb_log_buffer_size = 64M innodb_max_dirty_pages_pct = 85 å…¨å±€é”è¯»Global Read Lock (GRLï¼‰ åŠ é”æ–¹æ³•ï¼šFTWRL,flush tables with read lock.\nè§£é”æ–¹æ³•ï¼šunlock tablesï¼›\nå¯èƒ½å‡ºç°çš„åœºæ™¯\nè®°å½•binlogæ—¥å¿—-\u0026gt;ä¸è®©æ‰€æœ‰äº‹åŠ¡æäº¤\nFTWRL-\u0026gt;ä¸è®©æ–°çš„ä¿®æ”¹è¿›å…¥\nsnapshot innodb-\u0026gt; å…è®¸æ‰€æœ‰çš„DMLè¯­å¥,ä½†æ˜¯ä¸å…è®¸DDL\nå±äºç±»å‹: MDL(matedatalock)å±‚é¢é”\nå½±å“æƒ…å†µ: åŠ é”çš„æœŸé—´,é˜»å¡æ‰€æœ‰äº‹åŠ¡çš„å†™å…¥,é˜»å¡æ‰€æœ‰äº‹åŠ¡çš„commit,æ—¶é—´å—åˆ°lock_wait_timeout=315336000\nå…¨å±€è¯»é”çš„æ’æŸ¥æ–¹æ³• MySQL [(none)]\u0026gt; USE performance_schema MySQL [performance_schema]\u0026gt; # 5.6éœ€è¦æ‰‹åŠ¨å¼€å¯ MySQL [performance_schema]\u0026gt; UPDATE setup_instruments SET ENABLED = \u0026#34;YES\u0026#34;,TIMED = \u0026#34;YES\u0026#34; WHERE NAME=\u0026#39;wait/lock/metadata/sql/mdl\u0026#39;; # æŸ¥çœ‹æ˜¯å¦æœ‰é˜»å¡é—®é¢˜ MySQL [performance_schema]\u0026gt; SELECT * FROM metadata_locks; mysql\u0026gt; SELECT OBJECT_SCHEMA,OBJECT_NAME,LOCK_TYPE,LOCK_DURATION,LOCK_STATUS,OWNER_THREAD_ID,OWNER_EVENT_ID FROM performance_schema.metadata_locks; 5.7ç‰ˆæœ¬å…¨å±€è¯»é”æ’æŸ¥ mysql\u0026gt; SHOW proceslist\\G; mysql\u0026gt; SELECT * FORM sys.schema_table_lock_waits; ç»å…¸æ•…éšœæ¡ˆä¾‹ å‡è®¾æ¨¡æ‹Ÿä¸€ä¸ªå¤§çš„æŸ¥è¯¢æˆ–è€…äº‹ç‰© æ¨¡æ‹Ÿå¤‡ä»½æ—¶çš„TWRL,æ­¤æ—¶ä¼šå‘ç°å‘½ä»¤é˜»å¡ å‘èµ·æ­£å¸¸æŸ¥è¯¢è¯·æ±‚,å‘ç°æŸ¥è¯¢è¢«é˜»å¡ 5.7ç‰ˆæœ¬çš„Xbackup/mysqldumpå¤‡ä»½æ•°æ®åº“å‡ºç°é”è¡¨çŠ¶æ€,æ‰€æœ‰çš„æŸ¥è¯¢ä¸èƒ½æ­£å¸¸è¿›è¡Œ.\nSELECT *,SLEEP(100) FORM `user` WHERE username = \u0026#39;test1\u0026#39; for update; flush tables with read lock; SELECT * FROM icours.user where username = \u0026#39;test\u0026#39; for update Table Lock(è¡¨çº§é”) åŠ é”æ–¹å¼: lock table t1 read; æ‰€æœ‰ä¼šè¯åªè¯»,å±äºMDLé”ã€‚lock table write; å½“å‰ä¼šè¯å¯ä»¥å¯ä»¥RW,å±äºMDLé”. SELECT FOR UPDATE; SELECT FOR SHARE è§£é”æ–¹å¼: unlock tables; æ£€æµ‹æ–¹å¼ [mysqld] performance-schema-instrument = \u0026#39;wait/lock/metadata/sql/mdl=ON\u0026#39; SELECT * FROM performance_schema.metadata_locks; SELECT * FROM performance_schema.threads; MetaDataLock(å…ƒæ•°æ®é”) ä½œç”¨èŒƒå›´: globalã€commitã€tablespaceã€schemaã€table é»˜è®¤æ—¶é—´ï¼š lock_wait_timeout mysql\u0026gt; select @@lock_wait_timeout; +---------------------+ | @@lock_wait_timeout | +---------------------+ | 31536000 | +---------------------+ 1 row in set (0.00 sec) æ£€æµ‹æ–¹å¼ [mysqld] performance-schema-instrument = \u0026#39;wait/lock/metadata/sql/mdl=ON\u0026#39; SELECT * FROM performance_schema.metadata_locks; // æ‰¾åˆ°é˜»å¡çš„Id OWNER_THREAD_ID = 12 mysql\u0026gt; SELECT * FROM threads where thread_id = \u0026#39;12\u0026#39;\\G; kill 12; AutoincLock(è‡ªå¢é”) é€šè¿‡å‚æ•°: innodb_autoinc_lock_mod = 0 | 1 | 2 0 è¡¨é”ï¼šæ¯æ¬¡æ’å…¥éƒ½è¯·æ±‚è¡¨é”,æ•ˆç‡ä½ä¸‹ 1 mutexï¼š é¢„è®¡æ’å…¥å¤šå°‘è¡Œ,é¢„ç”³è¯·è‡ªå¢åºåˆ—.å¦‚æœå‡ºç°loadæˆ–è€…insert selectæ–¹å¼ä¼šé€€åŒ–ä¸º0ã€‚ 2 : å¼ºåˆ¶ä½¿ç”¨mutexçš„æ–¹å¼,å¹¶å‘æ’å…¥ä¼šæ›´é«˜æ•ˆï¼ Innodb Row Lock(è¡Œçº§é”) record lockã€gapã€nextã€lock MySQL [(none)]\u0026gt; SHOW STATUS LIKE \u0026#39;innodb_row_lock\u0026#39;; MySQL [information_schema]\u0026gt; SELECT * FROM information_schema.innodb_trx; MySQL [information_schema]\u0026gt; SELECT * FORM sys.schema_table_lock_waits; MySQL [information_schema]\u0026gt; SELECT * FROM performance_schema.threads; MySQL [information_schema]\u0026gt; SELECT * FROM performance_schema.events_statements_current; MySQL [information_schema]\u0026gt; ä¼˜åŒ–æ–¹å‘ ä¼˜åŒ–ç´¢å¼• å‡å°‘äº‹åŠ¡çš„æ›´æ–°èŒƒå›´ RCçº§åˆ« æ‹†åˆ†è¯­å¥ // å‡è®¾ k1æ˜¯è¾…åŠ©ç´¢å¼• update t1 set num=num+10 where k1\u0026lt;100; // æ”¹ä¸º select id from t1 where k1\u0026lt;100; update t1 set num=num+10 where id in (20,30,50) Dead Lockæ­»é” dead lock å¤šä¸ªå¹¶å‘äº‹åŠ¡ä¹‹é—´å‘ç”Ÿäº¤å‰ä¾èµ–çš„æ—¶å€™,ä¼šå‡ºç°æ­»é”.\nSHOW ENGINE innodb STATUS\\G; innodb_printâ€”â€”all_deadlocks =1 // å¼€å¯è®°å½•æ­»é”æ—¥å¿— ","date":"2022-07-20T00:00:00Z","permalink":"http://localhost:1313/mysql/MysqlOptimization/","title":"MySQLå°å°ä¼˜åŒ–æ€è·¯ç®€å•ç‰ˆæœ¬"},{"content":"Dockerè¿ç§»å­˜å‚¨ç›®å½• é—®é¢˜èµ·å›  ç”±äºå…¬å¸æœ€å¼€å§‹çš„æœåŠ¡å™¨åœ¨/var/lib/dockeræ²¡æœ‰æŒ‚è½½å­˜å‚¨,å®¹é‡åªæœ‰40G,å¯¼è‡´æœåŠ¡å™¨ç£ç›˜ç”¨æ»¡ã€‚ç°å°†åŸæœ‰çš„Dockerç›®å½•æ•°æ®è¿›è¡Œè¿ç§»ã€‚\nè¯·å„ä½Kubernetesç”¨æˆ·ä¸è¦æ“ä½œ,å› ä¸ºå®¹å™¨ç¼–æ’ä¸æ”¯æŒ!\n# å¯åŠ¨å®¹å™¨å‘ç°å¦‚ä¸‹æŠ¥é”™ ERRORï¼šcannot create temporary directory! æ–¹æ³•ä¸€: è½¯è¿æ¥æ–¹å¼ # 1.åœæ­¢dockeræœåŠ¡ systemctl stop docker # 2.å¼€å§‹è¿ç§»ç›®å½• mv /var/lib/docker /data/ # ä½¿ç”¨cpå‘½ä»¤ä¹Ÿå¯ä»¥ cp -arv /var/lib/docker /data/docker # 3.æ·»åŠ è½¯é“¾æ¥ ln -s /data/docker /var/lib/docker # 4.å¯åŠ¨dockeræœåŠ¡ systemctl start docker æ–¹æ³•äºŒ: ä¿®æ”¹dockeré…ç½®æ–‡ä»¶ æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªæ—§ç‰ˆæœ¬dockerä¿®æ”¹å­˜å‚¨ç›®å½•çš„æ–¹å¼.\nvim /etc/docker/daemon.json { \u0026#34;graph\u0026#34;: [ \u0026#34;/data/docker/\u0026#34; ] # æ›´æ”¹dockeré•œåƒçš„å­˜å‚¨ç›®å½• } æ–°ç‰ˆæœ¬ä¿®æ”¹å­˜å‚¨ç›®å½•æ–¹å¼\n# è¯·æ‰¾åˆ°ä½ çš„docker.serviceå­˜æ”¾ä½ç½® vim /usr/lib/systemd/system/docker.service é€šè¿‡åŠ å…¥--data-root=/data/dockerè¿›è¡Œä¿®æ”¹é»˜è®¤çš„æ•°æ®å­˜å‚¨ä½ç½®\n[Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd --data-root=/data/docker ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target ä¿®æ”¹å®Œæˆä¹‹åé‡å¯docker\nsystemctl daemon-reload # é‡è½½serviceé…ç½® systemctl restart docker Dockerå­˜å‚¨ç©ºé—´ä¸è¶³ é—®é¢˜ä¸€: No space left on device é—®é¢˜æè¿°ï¼šdocker run çš„æ—¶å€™ç³»ç»Ÿæç¤ºNo space left on device!\nè¿™ä¸ªé—®é¢˜æ— éå°±ä¸¤ç§æƒ…å†µ\nç£ç›˜æ»¡äº†\nç£ç›˜inodeæ»¡äº†\nå› ä¸º ext3 æ–‡ä»¶ç³»ç»Ÿä½¿ç”¨ inode table å­˜å‚¨ inode ä¿¡æ¯ï¼Œè€Œ xfs æ–‡ä»¶ç³»ç»Ÿä½¿ç”¨ B+ tree æ¥è¿›è¡Œå­˜å‚¨ã€‚è€ƒè™‘åˆ°æ€§èƒ½é—®é¢˜ï¼Œé»˜è®¤æƒ…å†µä¸‹è¿™ä¸ª B+ tree åªä¼šä½¿ç”¨å‰ 1TB ç©ºé—´ï¼Œå½“è¿™ 1TB ç©ºé—´è¢«å†™æ»¡åï¼Œå°±ä¼šå¯¼è‡´æ— æ³•å†™å…¥ inode ä¿¡æ¯ï¼ŒæŠ¥ç£ç›˜ç©ºé—´ä¸è¶³çš„é”™è¯¯ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ mount æ—¶ï¼ŒæŒ‡å®š inode64 å³å¯å°†è¿™ä¸ª B+ tree ä½¿ç”¨çš„ç©ºé—´æ‰©å±•åˆ°æ•´ä¸ªæ–‡ä»¶ç³»ç»Ÿã€‚\n# æŸ¥çœ‹indeä¿¡æ¯ df -i # åˆ é™¤è¿‡å¤šçš„å°æ–‡ä»¶å³å¯ Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda3 593344 56998 536346 10% / tmpfs 238282 1 238281 1% /dev/shm /dev/sda1 51200 39 51161 1% /boot /tmp/1m 128 128 0 100% /app/logs å¦‚æœä¸çŸ¥é“å°æ–‡ä»¶å¦‚ä½•æŸ¥æ‰¾\n# æŸ¥æ‰¾ç³»ç»Ÿä¸­ ç›®å½•å¤§å°å¤§äº1Mï¼ˆç›®å½•ä¸€èˆ¬å¤§å°ä¸º4Kï¼Œæ‰€ä»¥ç›®å½•è¦æ˜¯å¤§äº†é‚£ä¹ˆæ–‡ä»¶å¿…ç„¶å¾ˆå¤šï¼‰ find / -size +4k -type d |xargs ls -ldhi å¦‚æœæ˜¯ç¡¬ç›˜ç©ºé—´æ»¡äº†çš„è¯\n# æŸ¥çœ‹ç£ç›˜ä½¿ç”¨å®¹é‡ df -h # æŸ¥çœ‹åˆ°å…·ä½“å“ªä¸ªç›®å½•æ»¡äº†,ç„¶åé…åˆ du -shå‘½ä»¤è§£å†³å³å¯ Filesystem Size Used Avail Use% Mounted on /dev/sda3 8.8G 8.8G 0 100% / tmpfs 931M 0 931M 0% /dev/shm /dev/sda1 190M 40M 141M 22% /boot ä¼˜é›…åœ°é‡å¯Docker ä¸åœæ­¢é‡å¯,é‡å¯dockeræ˜¯ä¸€ä»¶å¤šä¹ˆç¾å¦™çš„äº‹æƒ…!\nå½“ Docker å®ˆæŠ¤ç¨‹åºç»ˆæ­¢æ—¶ï¼Œå®ƒä¼šå…³é—­æ­£åœ¨è¿è¡Œçš„å®¹å™¨ã€‚ä» Docker-ce 1.12 å¼€å§‹ï¼Œå¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ  live-restore å‚æ•°ï¼Œä»¥ä¾¿åœ¨å®ˆæŠ¤ç¨‹åºå˜å¾—ä¸å¯ç”¨æ—¶å®¹å™¨ä¿æŒè¿è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ Windows å¹³å°æš‚æ—¶è¿˜æ˜¯ä¸æ”¯æŒè¯¥å‚æ•°çš„é…ç½®ã€‚\nvim /etc/docker/daemon.json { \u0026#34;live-restore\u0026#34;: true } åœ¨å®ˆæŠ¤è¿›ç¨‹å…³é—­çš„æ—¶å€™ä¿æŒå®¹å™¨è¿è¡Œ\n# é‡è½½dockeræœåŠ¡ systemctl reload docker.service [root@VM-0-9-centos ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e58a220f03c3 nginx \u0026#34;/docker-entrypoint.â€¦\u0026#34; 5 minutes ago Up 15 seconds 80/tcp web # è¿™ä¸ªæ—¶å€™é‡å¯dockeræœåŠ¡,webæœåŠ¡å¹¶æ²¡æœ‰åœæ­¢å·¥ä½œ [root@VM-0-9-centos ~]# systemctl restart docker [root@VM-0-9-centos ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e58a220f03c3 nginx \u0026#34;/docker-entrypoint.â€¦\u0026#34; 7 minutes ago Up About a minute 80/tcp web live-restoreçš„é™åˆ¶ å½“å‰çš„Live Restoreç‰¹æ€§å¯ä»¥åœ¨è¿›è¡ŒDaemonç»´æŠ¤ï¼Œæˆ–è€…åœ¨Daemonå‘ç”Ÿé—®é¢˜å¯¼è‡´ä¸å¯ç”¨çš„æƒ…å†µï¼Œå‡å°‘å®¹å™¨çš„åœæœºæ—¶é—´ï¼Œä¸è¿‡å…¶ä¹Ÿæœ‰ä¸€å®šçš„é™åˆ¶ã€‚\nDockerç‰ˆæœ¬å‡çº§é™åˆ¶ Live Restoreä»…æ”¯æŒDockerè¡¥ä¸ç‰ˆæœ¬å‡çº§æ—¶å¯ç”¨ï¼Œä¹Ÿå°±æ˜¯ YY.MM.x æœ€åä¸€ä½å‘ç”Ÿå˜åŒ–çš„å‡çº§ï¼Œè€Œä¸æ”¯æŒå¤§ç‰ˆæœ¬çš„å‡çº§ã€‚åœ¨è¿›è¡Œå¤§ç‰ˆæœ¬å‡çº§åï¼Œå¯èƒ½ä¼šå¯¼è‡´Daemonæ— æ³•é‡æ–°è¿æ¥åˆ°è¿è¡Œä¸­å®¹å™¨çš„é—®é¢˜ï¼Œè¿™æ—¶å€™éœ€è¦æ‰‹åŠ¨åœæ­¢è¿è¡Œçš„å®¹å™¨ã€‚ Daemoné€‰é¡¹å˜æ›´ ä¹Ÿå°±æ˜¯è¯´Live Restoreä»…ä»…åœ¨æŸäº›Daemonçº§åˆ«çš„é…ç½®é€‰é¡¹ä¸å‘ç”Ÿæ”¹å˜çš„æƒ…å†µå·¥ä½œï¼Œä¾‹å¦‚Bridgeçš„IPåœ°å€ï¼Œå­˜å‚¨é©±åŠ¨ç±»å‹ç­‰ã€‚å¦‚æœåœ¨é‡å¯Daemonæ—¶å€™ï¼Œè¿™äº›é€‰é¡¹å‘ç”Ÿäº†æ”¹å˜ï¼Œåˆ™å¯èƒ½ä¼šåˆ°Daemonæ— æ³•é‡æ–°è¿æ¥è¿è¡Œä¸­çš„å®¹å™¨ï¼Œè¿™æ—¶ä¹Ÿéœ€è¦æ‰‹åŠ¨åœæ­¢è¿™äº›å®¹å™¨ã€‚ å½±å“å®¹å™¨çš„æ—¥å¿—è¾“å‡º å¦‚æœDaemoné•¿æ—¶é—´åœæ­¢ï¼Œä¼šå½±å“è¿è¡Œå®¹å™¨çš„æ—¥å¿—è¾“å‡ºã€‚å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ï¼Œæ—¥å¿—ç®¡é“çš„ç¼“å†²åŒºå¤§å°ä¸º64kï¼Œå½“ç¼“å†²å†™æ»¡ä¹‹åï¼Œå¿…é¡»å¯åŠ¨Daemonæ¥åˆ·æ–°ç¼“å†²åŒºã€‚ ä¸æ”¯æŒDocker Swarm Live Restoreåªæ˜¯ç‹¬ç«‹Dockerå¼•æ“çš„ç‰¹æ€§ï¼Œè€ŒSwarmçš„æœåŠ¡æ˜¯ç”±Swarmç®¡ç†å™¨ç®¡ç†çš„ã€‚å½“Swarmç®¡ç†å™¨ä¸å¯ç”¨æ—¶ï¼ŒSwarmæœåŠ¡æ˜¯å¯ä»¥åœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šç»§ç»­è¿è¡Œçš„ï¼Œåªæ˜¯ä¸åŒé€šè¿‡Swarmç®¡ç†å™¨è¿›è¡Œç®¡ç†ï¼Œç›´åˆ°Swarmç®¡ç†æ¢å¤å·¥ä½œã€‚ å®¹å™¨å†…éƒ¨ä¸­æ–‡å¼‚å¸¸ é—®é¢˜æè¿°: å®¹å™¨å†…éƒ¨ä¸­æ–‡ä¹±ç ã€æ— æ³•æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡ã€\nä¾‹å¦‚æ˜¾ç¤ºä¸­æ–‡ï¼š--------ï¿½ï¿½ï¿½ # æŸ¥çœ‹å®¹å™¨å†…éƒ¨ç¼–ç  root@e58a220f03c3:/# locale -a C C.UTF-8 POSIX ç„¶è€Œ POSIX å­—ç¬¦é›†æ˜¯ä¸æ”¯æŒä¸­æ–‡çš„ï¼Œè€Œ C.UTF-8 æ˜¯æ”¯æŒä¸­æ–‡çš„åªè¦æŠŠç³»ç»Ÿä¸­çš„ç¯å¢ƒ LANG æ”¹ä¸º \u0026quot;C.UTF-8\u0026quot; æ ¼å¼å³å¯è§£å†³é—®é¢˜ã€‚åŒç†ï¼Œåœ¨ K8S è¿›å…¥ pod ä¸èƒ½è¾“å…¥ä¸­æ–‡ä¹Ÿå¯ç”¨æ­¤æ–¹æ³•è§£å†³ã€‚\nexport LANG=zh_CN.UTF-8 ","date":"2022-07-14T00:00:00Z","permalink":"http://localhost:1313/docker/DockerFrequentlyAskedQuestions/","title":"Dockerå¸¸è§çš„å‡ ä¸ªé—®é¢˜å¤„ç†"},{"content":" playbookæ˜¯ç”±ä¸€ä¸ªæˆ–å¤šä¸ª\u0026quot;play\u0026quot;ç»„æˆçš„åˆ—è¡¨ playbookçš„ä¸»è¦åŠŸèƒ½åœ¨äºå°†é¢„å®šä¹‰çš„ä¸€ç»„ä¸»æœºï¼Œè£…æ‰®æˆäº‹å…ˆé€šè¿‡ansibleä¸­çš„taskå®šä¹‰å¥½çš„è§’è‰²ã€‚ Taskå®é™…æ˜¯è°ƒç”¨ansibleçš„ä¸€ä¸ªmoduleï¼Œå°†å¤šä¸ªplayç»„ç»‡åœ¨ä¸€ä¸ªplaybookä¸­ï¼Œ å³å¯ä»¥è®©å®ƒä»¬è”åˆèµ·æ¥ï¼ŒæŒ‰äº‹å…ˆç¼–æ’çš„æœºåˆ¶æ‰§è¡Œé¢„å®šä¹‰çš„åŠ¨ä½œ Playbooké‡‡ç”¨YAMLè¯­è¨€ç¼–å†™ --- - hosts: test # æŒ‡å®šä¸»æœºåˆ—è¡¨ remote_user: root # è¿œç¨‹æ“ä½œä»¥ä»€ä¹ˆèº«ä»½æ‰§è¡Œ tasks: - name: Install Redis # æç¤ºå­—æ®µ,è¡¨ç¤ºå½“å‰å¤„äºä»€ä¹ˆè¿›åº¦ command: install redis # å½“å‰æ‰§è¡Œçš„å…·ä½“å‘½ä»¤æ“ä½œ 1.0 PlayBookæ ¸å¿ƒå…ƒç´  Hostsï¼šplaybookä¸­çš„æ¯ä¸€ä¸ªplayçš„ç›®çš„éƒ½æ˜¯ä¸ºäº†è®©ç‰¹å®šä¸»æœºä»¥æŸä¸ªæŒ‡å®šçš„ç”¨æˆ·èº«ä»½æ‰§è¡Œä»»åŠ¡,hostsç”¨äºæŒ‡å®šè¦æ‰§è¡ŒæŒ‡å®šä»»åŠ¡çš„ä¸»æœºï¼Œé¡»äº‹å…ˆå®šä¹‰åœ¨ä¸»æœºæ¸…å•ä¸­.è¯¦ç»†è¯·çœ‹ remote_user: å¯ç”¨äºHostå’Œtaskä¸­ã€‚ä¹Ÿå¯ä»¥é€šè¿‡æŒ‡å®šå…¶é€šè¿‡sudoçš„æ–¹å¼åœ¨è¿œç¨‹ä¸»æœºä¸Šæ‰§è¡Œä»»åŠ¡ï¼Œå…¶å¯ç”¨äºplayå…¨å±€æˆ–æŸä»»åŠ¡.æ­¤å¤–ï¼Œç”šè‡³å¯ä»¥åœ¨sudoæ—¶ä½¿ç”¨sudo_useræŒ‡å®šsudoæ—¶åˆ‡æ¢çš„ç”¨æˆ·. varniables: å†…ç½®å˜é‡æˆ–è‡ªå®šä¹‰å˜é‡åœ¨playbookä¸­è°ƒç”¨ Templatesæ¨¡æ¿ : å¯æ›¿æ¢æ¨¡æ¿æ–‡ä»¶ä¸­çš„å˜é‡å¹¶å®ç°ä¸€äº›ç®€å•é€»è¾‘çš„æ–‡ä»¶ Handlerså’Œnotify: ç»“åˆä½¿ç”¨ï¼Œç”±ç‰¹å®šæ¡ä»¶è§¦å‘çš„æ“ä½œï¼Œæ»¡è¶³æ¡ä»¶æ–¹æ‰æ‰§è¡Œï¼Œå¦åˆ™ä¸æ‰§è¡Œ tags: æŒ‡å®šæŸæ¡ä»»åŠ¡æ‰§è¡Œï¼Œç”¨äºé€‰æ‹©è¿è¡Œplaybookä¸­çš„éƒ¨åˆ†ä»£ç . ansible-playbook -C hello.yaml -C é€‰é¡¹æ£€æŸ¥å‰§æœ¬æ˜¯å¦æˆåŠŸ,å¹¶ä¸å®é™…æ‰§è¡Œ 1.0.1 å¿½ç•¥é”™è¯¯ä¿¡æ¯ ä¹Ÿå¯ä»¥ä½¿ç”¨ignore_errorsæ¥å¿½ç•¥é”™è¯¯ä¿¡æ¯\ntasks: - name: run this shell: /usr/bin/ls || /bin/true ignore_errors: True 1.0.2 å¸¸ç”¨é€‰é¡¹ --check: åªæ£€æµ‹å¯èƒ½ä¼šå‘ç”Ÿçš„æ”¹å˜,ä½†æ˜¯ä¸ä¼šæ‰§è¡Œ --list-hosts: åˆ—å‡ºè¿è¡Œä»»åŠ¡çš„ä¸»æœº --limit: ä¸»æœºåˆ—è¡¨,åªé’ˆå¯¹ä¸»æœºåˆ—è¡¨ä¸­çš„ä¸»æœºæ‰§è¡Œ -v: æ˜¾ç¤ºè¿‡ç¨‹ --list-tasks: æŸ¥çœ‹ä»»åŠ¡åˆ—è¡¨ ansible-playbook hello.yaml --check ansible-playbook hello.yaml --list-hosts ansible-playbook hello.yaml --limit 10.1.6.111 2.0 Handlerså’Œnotify ç”±äºplaybookæ‰§è¡Œä¼šæœ‰æ¬¡åºé—®é¢˜,æ‰€ä»¥å½“å‡ºç°æ¬¡åºé—®é¢˜çš„æ—¶å€™,å¯ä»¥ä½¿ç”¨handlersç»“åˆnotify\nHandlers: æ˜¯taskåˆ—è¡¨,è¿™äº›taskä¸å‰è¿°çš„taskæ²¡æœ‰æœ¬è´¨çš„åŒºåˆ«,ç”¨äºå½“ä¸åŒçš„èµ„æºå‘ç”Ÿå˜åŒ–çš„æ—¶å€™,æ‰ä¼šé‡‡å–ä¸€å®šçš„æ“ä½œ. Notify: æ­¤actionå¯ä»¥ç”¨åœ¨æ¯ä¸ªplayçš„æœ€åè¢«è§¦å‘,è¿™æ ·å¯ä»¥é¿å…å¤šæ¬¡æœ‰æ”¹å˜çš„å‘ç”Ÿæ—¶æ¯æ¬¡éƒ½æ‰§è¡ŒæŒ‡å®šçš„æ“ä½œ,ä»…ä»…åœ¨æ‰€æœ‰å˜åŒ–å‘ç”Ÿå®Œå,ä¸€æ¬¡æ€§æ‰§è¡Œåˆ¶å®šæ“ä½œ,åœ¨notifyä¸­åˆ—å‡ºçš„æ“ä½œç§°ä¸ºhendlerï¼Œä¹Ÿå°±æ˜¯notifyä¸­å®šä¹‰çš„æ“ä½œ. Handlerså’Œnotifyå¯ä»¥å†™å¤šä¸ª\n--- - hosts: test remote_user: root tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/newfile state=touch - name: \u0026#34;create new user\u0026#34; user: name=test2 system=yes shell=/sbin/nologin - name: \u0026#34;install httpd\u0026#34; yum: name=httpd state=installed notify: restart service # è¡¨ç¤ºæ‰§è¡Œå®Œyumæ“ä½œä»¥åéœ€è¦æ‰§è¡Œhandlersçš„æ“ä½œ - name: \u0026#34;copy log\u0026#34; copy: src=/var/log/httpd/error_log dest=/data handlers: - name: restart service service: name=httpd state=restarted 3.0 PlayBookçš„tagsä½¿ç”¨ ç»™ç‰¹å®šçš„å†…å®¹æ‰“ä¸Štagså¯ä»¥å•ç‹¬çš„æ‰§è¡Œæ ‡ç­¾å†…å®¹ --- - hosts: test remote_user: root tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/newfile state=touch tags: newfile - name: \u0026#34;create new user\u0026#34; user: name=test2 system=yes shell=/sbin/nologin tags: newuser - name: \u0026#34;install httpd\u0026#34; yum: name=httpd state=installed notify: restart service # è¡¨ç¤ºæ‰§è¡Œå®Œyumæ“ä½œä»¥åéœ€è¦æ‰§è¡Œhandlersçš„æ“ä½œ - name: \u0026#34;copy log\u0026#34; copy: src=/var/log/httpd/error_log dest=/data handlers: - name: restart service service: name=httpd state=restarted ansible-playbook -t newfile test.yaml # è¡¨ç¤ºåªæ‰§è¡Œnewfileæ ‡ç­¾çš„åŠ¨ä½œ ansible-playbook -t newfile,newuser test.yaml # è¡¨ç¤ºåªæ‰§è¡Œnewfileæ ‡ç­¾çš„åŠ¨ä½œ 4.0 PlayBookä¸­å˜é‡çš„ä½¿ç”¨ å˜é‡åï¼šä»…èƒ½ç”±å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿ç»„æˆï¼Œä¸”åªèƒ½ä»¥å­—æ¯å¼€å¤´ å˜é‡çš„æ¥æº é€šè¿‡setupæ¨¡å— åœ¨/etc/ansible/hostsä¸­å®šä¹‰ æ™®é€šå˜é‡ï¼šä¸»æœºç»„ä¸­çš„ä¸»æœºå•ç‹¬å®šä¹‰,ä¼˜å…ˆçº§é«˜äºå…¬å…±å˜é‡ å…¬å…±å˜é‡ï¼šé’ˆå¯¹ä¸»æœºç»„æ‰€æœ‰ä¸»æœºå®šä¹‰ç»Ÿä¸€å˜é‡ é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šå˜é‡ï¼š ä¼˜å…ˆçº§æœ€é«˜ 4.0.1 é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šå˜é‡ --- - hosts: test remote_user: root tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/{{filename}} state=touch tags: newfile ansible-playbook -e \u0026#39;filename=app1\u0026#39; # /data/app1 4.0.2 åœ¨playbookä¸­å®šä¹‰ # åœ¨playbookä¸­å®šä¹‰ --- - hosts: test remote_user: root vars: - filename: app1 tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/{{filename}} state=touch tags: newfile 4.0.3 é€šè¿‡setupæ¨¡å—è·å–å˜é‡ ansible setup facts è¿œç¨‹ä¸»æœºçš„æ‰€æœ‰å˜é‡éƒ½å¯ç›´æ¥è°ƒç”¨ (ç³»ç»Ÿè‡ªå¸¦å˜é‡) setupæ¨¡å—å¯ä»¥å®ç°ç³»ç»Ÿä¸­å¾ˆå¤šç³»ç»Ÿä¿¡æ¯çš„æ˜¾ç¤º ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_nodename\u0026#34;\u0026#39; æŸ¥è¯¢ä¸»æœºå ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_memtotal_mb\u0026#34;\u0026#39; æŸ¥è¯¢ä¸»æœºå†…å­˜å¤§å° ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_distribution_major_version\u0026#34;\u0026#39; æŸ¥è¯¢ç³»ç»Ÿç‰ˆæœ¬ ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_processor_vcpus\u0026#34;\u0026#39; æŸ¥è¯¢ä¸»æœºcpuä¸ªæ•° 4.0.4 åœ¨hostsä¸­å®šä¹‰å˜é‡ å®šä¹‰ä¸»æœºç»„å•ç‹¬çš„å˜é‡ [test] 192.168.1.1 http_port=81 192.168.1.2 http_port=82 --- - hosts: test remote_user: root tasks: - name: \u0026#34;create new file\u0026#34; hostname: name=www{{http_port}}.baidu.com å®šä¹‰å…¬å…±å˜é‡ # é’ˆå¯¹testä¸»æœºç»„å½“ä¸­çš„æ‰€æœ‰ä¸»æœºéƒ½æœ‰æ•ˆ [test:vars] nodename=www domain=baidu.com 4.0.5 é€šè¿‡æ–‡ä»¶åŠ è½½å˜é‡ # vars.yaml filename: applications # playbook.yaml - hosts: test remote_user: root vars_files: - vars.yaml tasks: - name: \u0026#34;create new file\u0026#34; file: name=/data/{{filename}} 5.0 æ¨¡æ¿Templates é‡‡ç”¨Jinja2è¯­è¨€ï¼Œä½¿ç”¨å­—é¢é‡ï¼Œæœ‰ä¸‹é¢å½¢å¼ æ•°å­—ï¼šæ•´æ•°ï¼Œæµ®ç‚¹æ•° åˆ—è¡¨ï¼š[item1, item2, â€¦] å…ƒç»„ï¼š(item1, item2, â€¦) å­—å…¸ï¼š{key1:value1, key2:value2, â€¦} å¸ƒå°”å‹ï¼štrue/false ç®—æœ¯è¿ç®—ï¼š+, -, *, /, //, %, ** æ¯”è¾ƒæ“ä½œï¼š==, !=, \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;= é€»è¾‘è¿ç®—ï¼šandï¼Œorï¼Œnot æµè¡¨è¾¾å¼ï¼šForï¼ŒIfï¼ŒWhen # For more information on configuration, see: # * Official English Documentation: http://nginx.org/en/docs/ # * Official Russian Documentation: http://nginx.org/ru/docs/ user nginx; worker_processes {{ansible_processor_vcpus**2}}; # ä¾‹å¦‚,ä½ å¯ä»¥å°†nginxæ ¸å¿ƒæ•°åŠ¨æ€çš„è®¾ç½®ä¸ºä¸»æœºçš„CPUæ•°é‡ error_log /var/log/nginx/error.log; pid /run/nginx.pid; 5.0.1 Whenè¯­æ³• æ¡ä»¶æµ‹è¯•:å¦‚æœéœ€è¦æ ¹æ®å˜é‡ã€factsæˆ–æ­¤å‰ä»»åŠ¡çš„æ‰§è¡Œç»“æœæ¥åšä¸ºæŸtaskæ‰§è¡Œä¸å¦çš„å‰ææ—¶è¦ç”¨åˆ°æ¡ä»¶æµ‹è¯•, é€šè¿‡whenè¯­å¥å®ç°ï¼Œåœ¨taskä¸­ä½¿ç”¨ï¼Œjinja2çš„è¯­æ³•æ ¼å¼ åœ¨taskåæ·»åŠ whenå­å¥å³å¯ä½¿ç”¨æ¡ä»¶æµ‹è¯•ï¼›whenè¯­å¥æ”¯æŒJinja2è¡¨è¾¾å¼è¯­æ³• å½“ansible_distribution=CentOSçš„æ—¶å€™æ‰ä¼šå»æ‰§è¡Œtemplate\n--- - hosts: test remote_user: root tasks: - name: \u0026#34;Install Nginx\u0026#34; yum: name=nginx - name: Config conf template: src=/templates/nginx.conf.j2 dest=/etc/nginx/nginx.conf when: ansible_distribution == \u0026#34;CentOS\u0026#34; - name: start nginx service: name=nginx state=started enabled=yes 5.0.2 With_item è¿­ä»£å†™æ³• --- - hosts: test remote_user: root tasks: - name: \u0026#34;Create new file\u0026#34; file: name=/data/{{items}} state=touch with_items: - app1 - app2 - app3 è¿­ä»£åµŒå¥—å­å˜é‡ - hosts: test remote_user: root tasks: - name: \u0026#34;Create new file\u0026#34; file: name=/data/{{item.name}}_{{item.date}} state=touch with_items: - {name: \u0026#39;app1\u0026#39;, date: \u0026#39;2022\u0026#39;} 5.0.3 forå¾ªç¯ --- - hosts: test remote_user: root vars: ports: - 81 - 82 - 83 tasks: - name: copy template template: src=/root/templates/for.j2 dest=/data/for.conf # æˆ–è€… --- - hosts: test remote_user: root vars: ports: - listen:81 - listen:82 - listen:83 tasks: - name: copy template template: src=/root/templates/for.j2 dest=/data/for.conf # æˆ–è€… --- - hosts: test remote_user: root vars: config: - host1: port: 81 name: host1.do.com rootdir: /root/ tasks: - name: copy template template: src=/root/templates/for.j2 desc=/data/for.conf åˆ›å»ºä¸€ä¸ªæ¨¡æ¿æ–‡ä»¶\n{%for i in ports%} server { listen {{i}} } {%endfor%} # æˆ–è€… {%for i in ports%} server { listen {{i.listen}} } {%endfor%} # æˆ–è€… {%for i in ports%} server { listen {{ i.port }} name {{ i.name }} } {%endfor%} 5.0.4 ifåˆ¤æ–­ {%for i in ports%} server { listen {{ i.port }} {% if i.name is defind %} name {{ i.name }} {% endif %} } {%endfor%}a ","date":"2022-04-01T00:00:00Z","permalink":"http://localhost:1313/posts/playbook/","title":"Playbookçš„ä¸€äº›ç®€å•ä½¿ç”¨"},{"content":"é…ç½®nginxçš„work_process æŸ¥çœ‹å½“å‰æœåŠ¡çš„CPUæ ¸å¿ƒæ•°é‡\n[root@containerd-master1 ~]# grep processor /proc/cpuinfo | wc -l 8 å¦‚æœä½ éœ€è¦ä¿®æ”¹æ›´å¤šçš„å·¥ä½œè¿›ç¨‹,è¯·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„work_processå­—æ®µ\nauto: æ ¹æ®ç³»ç»Ÿçš„CPUè‡ªåŠ¨çš„è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°é‡ worker_processes 1; # å¯é€‰å€¼ auto é…ç½®work_connections è¯¥å‚æ•°è¡¨ç¤ºæ¯ä¸ªå·¥ä½œè¿›ç¨‹æœ€å¤§å¤„ç†çš„è¿æ¥æ•°,CentOSé»˜è®¤è¿æ¥æ•°ä¸º1024,è¿æ¥æ•°æ˜¯å¯ä»¥ä¿®æ”¹çš„ã€‚ å¦‚æœéœ€è¦ä¿®æ”¹ulimitå‚æ•°,è¯·ä¿®æ”¹é…ç½®æ–‡ä»¶/etc/security/limits.conf\nnoproc æ˜¯ä»£è¡¨æœ€å¤§è¿›ç¨‹æ•° nofile æ˜¯ä»£è¡¨æœ€å¤§æ–‡ä»¶æ‰“å¼€æ•° æœ¬æ¬¡ä¿®æ”¹ä»…ä»…ä»¥Rocky Linuxå’ŒCentOSä¸ºä¾‹,ä¸åŒçš„ç³»ç»Ÿä¿®æ”¹æ–¹æ³•å¯èƒ½æœ‰æ‰€å·®å¼‚.\n* soft nofile 65535 * hard nofile 65535 é…ç½®nginxå½“ä¸­çš„work_connections\nevents { worker_connections 65535; use epoll; } ç®€å•çš„æä¸€å˜´ulimitçš„ä½œç”¨: å½“è¿›ç¨‹æ‰“å¼€çš„æ–‡ä»¶æ•°ç›®è¶…è¿‡æ­¤é™åˆ¶æ—¶ï¼Œè¯¥è¿›ç¨‹å°±ä¼šé€€å‡ºã€‚\nå¯ç”¨gzipå‹ç¼© nginxä½¿ç”¨ gzip è¿›è¡Œæ–‡ä»¶å‹ç¼©å’Œè§£å‹ç¼©,æ‚¨å¯ä»¥èŠ‚çœå¸¦å®½å¹¶åœ¨è¿æ¥ç¼“æ…¢æ—¶æé«˜ç½‘ç«™çš„åŠ è½½æ—¶é—´ã€‚\nserver { gzip on; # å¼€å¯gzip gzip_vary on; gzip_min_length 10240; gzip_proxied expired no-cache no-store private auth; gzip_types text/plain text/css text/xml text/javascript application/x-javascript application/xml; gzip_disable \u0026#34;MSIE [1-6]\\.\u0026#34;; } é™åˆ¶nginxè¿æ¥çš„è¶…æ—¶ ä¸»è¦æ˜¯ä¸ºäº†å‡å°‘æ‰“å¼€å’Œå…³é—­è¿æ¥æ—¶çš„å¤„ç†å™¨å’Œç½‘ç»œå¼€é”€\nclient_body_timeout: è¯¥æŒ‡ä»¤è®¾ç½®è¯·æ±‚ä½“ï¼ˆrequest bodyï¼‰çš„è¯»è¶…æ—¶æ—¶é—´ã€‚ä»…å½“åœ¨ä¸€æ¬¡readstepä¸­ï¼Œæ²¡æœ‰å¾—åˆ°è¯·æ±‚ä½“ï¼Œå°±ä¼šè®¾ä¸ºè¶…æ—¶ã€‚è¶…æ—¶åï¼Œnginxè¿”å›HTTPçŠ¶æ€ç 408 client_header_timeout: æŒ‡å®šç­‰å¾…clientå‘é€ä¸€ä¸ªè¯·æ±‚å¤´çš„è¶…æ—¶æ—¶é—´ï¼ˆä¾‹å¦‚:GET / HTTP/1.1)ä»…å½“åœ¨ä¸€æ¬¡readä¸­ï¼Œæ²¡æœ‰æ”¶åˆ°è¯·æ±‚å¤´ï¼Œæ‰ä¼šè¢«è®°å½•ä¸ºè¶…æ—¶ keepalive_timeout: æŒ‡å®šäº†ä¸clientçš„keep-aliveè¿æ¥è¶…æ—¶æ—¶é—´,è¶…è¿‡è¿™ä¸ªæ—¶é—´å,æœåŠ¡å™¨ä¼šå…³å…³é—­è¿æ¥ã€‚ send_timeout: æŒ‡å®šå®¢æˆ·ç«¯çš„å“åº”è¶…æ—¶æ—¶é—´ã€‚è¿™ä¸ªè®¾ç½®ä¸ä¼šç”¨äºæ•´ä¸ªè½¬å‘å™¨ï¼Œè€Œæ˜¯åœ¨ä¸¤æ¬¡å®¢æˆ·ç«¯è¯»å–æ“ä½œä¹‹é—´ã€‚å¦‚æœåœ¨è¿™æ®µæ—¶é—´å†…ï¼Œå®¢æˆ·ç«¯æ²¡æœ‰è¯»å–ä»»ä½•æ•°æ®ï¼Œnginxå°±ä¼šå…³é—­è¿æ¥ã€‚ http { client_body_timeout 12; client_header_timeout 12; keepalive_timeout 15; send_timeout 10; } è°ƒæ•´ç¼“å†²åŒºå¤§å° è°ƒæ•´nginxç¼“å†²åŒºä»¥ä¼˜åŒ–æœåŠ¡å™¨æ€§èƒ½ã€‚å¦‚æœç¼“å†²åŒºå¤§å°å¤ªå°ï¼Œé‚£ä¹ˆnginxå°†å†™å…¥ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ï¼Œå¯¼è‡´å¤§é‡ I/O æ“ä½œä¸æ–­è¿è¡Œã€‚\nhttp { client_body_buffer_size 10K; client_header_buffer_size 1k; client_max_body_size 8m; large_client_header_buffers 4 4k; } å¯ç”¨æ—¥å¿—è®¿é—®ç¼“å†²åŒº æ—¥å¿—å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒä»¬æœ‰åŠ©äºè§£å†³é—®é¢˜ã€‚å®Œå…¨ç¦ç”¨æ—¥å¿—ä¸æ˜¯ä¸€ä¸ªå¥½çš„åšæ³•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥å¯ç”¨è®¿é—®æ—¥å¿—ç¼“å†²ã€‚è¿™å°†å…è®¸nginxç¼“å†²ä¸€ç³»åˆ—æ—¥å¿—å¹¶å°†å®ƒä»¬ä¸€æ¬¡å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œè€Œä¸æ˜¯å¯¹æ¯ä¸ªè¯·æ±‚åº”ç”¨ä¸åŒçš„æ—¥å¿—æ“ä½œã€‚åœ¨nginxé…ç½®æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹è¡Œä»¥å…è®¸è®¿é—®æ—¥å¿—ç¼“å†²\nhttp { log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; # å¼€å¯æ—¥å¿—ç¼“å†²è®¾ç½® access_log logs/access.log main buffer=32k flush=1m; } è°ƒæ•´é™æ€æ–‡ä»¶ç¼“å­˜ # é™æ€æ–‡ä»¶ç¼“å­˜å†…å®¹ location ~* \\.(jpg|jpeg|png|gif|ico|css|js)$ { expires 90d; } ","date":"2022-03-20T00:00:00Z","permalink":"http://localhost:1313/posts/nginx%E4%BC%98%E5%8C%96/","title":"Nginxç®€å•çš„å¸¸è§„ä¼˜åŒ–"},{"content":"å…ˆå‰äº†è§£ å‚è€ƒé“¾æ¥ Githubissue kubeletä¸­çš„Dockeræ”¯æŒç°åœ¨å·²å¼ƒç”¨ï¼Œå¹¶å°†åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚kubeletä½¿ç”¨äº†ä¸€ä¸ªåä¸ºdockershimçš„æ¨¡å—ï¼Œè¯¥æ¨¡å—å®ç°äº†å¯¹Dockerçš„CRIæ”¯æŒï¼Œå¹¶åœ¨Kubernetesç¤¾åŒºä¸­å‘ç°äº†ç»´æŠ¤é—®é¢˜ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨è¯„ä¼°è¿ç§»åˆ°ä¸€ä¸ªå®¹å™¨è¿è¡Œæ—¶çš„æƒ…å†µï¼Œè¯¥å®¹å™¨è¿è¡Œæ—¶æ˜¯CRIï¼ˆv1alpha1æˆ–v1å…¼å®¹ï¼‰çš„å®Œæ•´å®ç°ã€‚\nä¹Ÿå°±æ˜¯è¯´,åœ¨åç»­çš„Kubernetes1.20xç‰ˆæœ¬ä»¥åä¼šåˆ é™¤dockershimç»„ä»¶,ä½†æ˜¯ç”±äºç›®å‰Dockerçš„ä½¿ç”¨ç”¨æˆ·ä¼—å¤š,ä¸­é—´å¿…ç„¶ä¼šæœ‰æ›¿æ¢çš„ä¸€ä¸ªè¿‡æ¸¡æœŸ,æ‰€ä»¥å¤§å®¶å¯ä»¥æ›´å¤šçš„å…³æ³¨ä¸€ä¸‹å…¶ä»–çš„Container Runtimeã€‚ ä¾‹å¦‚æˆ‘ä»¬çš„Podmanã€Containerdã€cri-oç­‰å…¶ä»–å®¹å™¨è¿è¡Œæ—¶æ¥è¿è¡Œkubernetesã€‚\nä¸‹é¢æˆ‘ä»¬å°±å…·ä½“æ¥çœ‹çœ‹Kubernetesæ‰€æåˆ°çš„å¼ƒç”¨dockershimåˆ°åº•æ˜¯ä»€ä¹ˆä¸œè¥¿.\nCRIå®¹å™¨è¿è¡Œæ—¶æ¥å£ å‚è€ƒé“¾æ¥ CRIï¼šå®¹å™¨è¿è¡Œæ—¶æ¥å£ container runtime interfaceï¼ŒCRI ä¸­å®šä¹‰äº†å®¹å™¨å’Œé•œåƒä¸¤ä¸ªæ¥å£ï¼Œå®ç°äº†è¿™ä¸¤ä¸ªæ¥å£ç›®å‰ä¸»æµçš„æ˜¯ï¼šCRI-Oã€Containerdã€‚ï¼ˆç›®å‰ PCI äº§å“ä½¿ç”¨çš„å³ä¸º Containerdï¼‰ã€‚ CRIæ¥å£çš„å…·ä½“ç”¨å¤„å°±åœ¨äº\nå¯¹å®¹å™¨æ“ä½œçš„æ¥å£ï¼ŒåŒ…æ‹¬å®¹å™¨çš„åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢.å³createã€stopç­‰æ“ä½œã€‚ å¯¹é•œåƒçš„æ“ä½œï¼Œä¸‹è½½ã€åˆ é™¤é•œåƒç­‰. å³pullã€rmiç­‰æ“ä½œã€‚ podsandbox OCIå¼€æ”¾å®¹å™¨æ ‡å‡† OCIï¼šå¼€æ”¾å®¹å™¨æ ‡å‡† open container initiativeï¼ŒOCI ä¸­å®šä¹‰äº†ä¸¤ä¸ªæ ‡å‡†ï¼šå®¹å™¨è¿è¡Œæ—¶æ ‡å‡† å’Œ å®¹å™¨é•œåƒæ ‡å‡†ï¼Œå®ç°äº†è¿™ä¸€æ ‡å‡†çš„ä¸»æµæ˜¯ï¼šruncï¼ˆä¹Ÿå³æˆ‘ä»¬æ—¥å¸¸è¯´çš„ Dockerï¼‰ã€Kata-Containerã€‚ OCIçš„ä½œç”¨åœ¨äº\nImageSpec(å®¹å™¨æ ‡å‡†åŒ…) æ–‡ä»¶ç³»ç»Ÿï¼šä»¥ layer ä¿å­˜çš„æ–‡ä»¶ç³»ç»Ÿï¼Œæ¯ä¸ª layer ä¿å­˜äº†å’Œä¸Šå±‚ä¹‹é—´å˜åŒ–çš„éƒ¨åˆ†ï¼Œlayer åº”è¯¥ä¿å­˜å“ªäº›æ–‡ä»¶ï¼Œæ€ä¹ˆè¡¨ç¤ºå¢åŠ ã€ä¿®æ”¹å’Œåˆ é™¤çš„æ–‡ä»¶ç­‰ config æ–‡ä»¶ï¼šä¿å­˜äº†æ–‡ä»¶ç³»ç»Ÿçš„å±‚çº§ä¿¡æ¯ï¼ˆæ¯ä¸ªå±‚çº§çš„ hash å€¼ï¼Œä»¥åŠå†å²ä¿¡æ¯ï¼‰ï¼Œä»¥åŠå®¹å™¨è¿è¡Œæ—¶éœ€è¦çš„ä¸€äº›ä¿¡æ¯ï¼ˆæ¯”å¦‚ç¯å¢ƒå˜é‡ã€å·¥ä½œç›®å½•ã€å‘½ä»¤å‚æ•°ã€mount åˆ—è¡¨ï¼‰ï¼ŒæŒ‡å®šäº†é•œåƒåœ¨æŸä¸ªç‰¹å®šå¹³å°å’Œç³»ç»Ÿçš„é…ç½®ã€‚æ¯”è¾ƒæ¥è¿‘æˆ‘ä»¬ä½¿ç”¨ docker inspect \u0026lt;image_id\u0026gt; çœ‹åˆ°çš„å†…å®¹ manifest æ–‡ä»¶ï¼šé•œåƒçš„ config æ–‡ä»¶ç´¢å¼•ï¼Œæœ‰å“ªäº› layerï¼Œé¢å¤–çš„ annotation ä¿¡æ¯ï¼Œmanifest æ–‡ä»¶ä¸­ä¿å­˜äº†å¾ˆå¤šå’Œå½“å‰å¹³å°æœ‰å…³çš„ä¿¡æ¯ index æ–‡ä»¶ï¼šå¯é€‰çš„æ–‡ä»¶ï¼ŒæŒ‡å‘ä¸åŒå¹³å°çš„ manifest æ–‡ä»¶ï¼Œè¿™ä¸ªæ–‡ä»¶èƒ½ä¿è¯ä¸€ä¸ªé•œåƒå¯ä»¥è·¨å¹³å°ä½¿ç”¨ï¼Œæ¯ä¸ªå¹³å°æ‹¥æœ‰ä¸åŒçš„ manifest æ–‡ä»¶ï¼Œä½¿ç”¨ index ä½œä¸ºç´¢å¼• 2.runtimeSpec:\nociVersion(string, REQUIRED):æ˜¯è¯¥å·éµå®ˆçš„å¼€æ”¾å®¹å™¨å€¡è®®è¿è¡Œæ—¶è§„èŒƒçš„ç‰ˆæœ¬ã€‚ idï¼š å®¹å™¨çš„ IDã€‚è¿™åœ¨æ­¤ä¸»æœºä¸Šçš„æ‰€æœ‰å®¹å™¨ä¸­å¿…é¡»æ˜¯å”¯ä¸€çš„ã€‚ä¸è¦æ±‚å®ƒåœ¨ä¸»æœºä¹‹é—´æ˜¯å”¯ä¸€çš„ã€‚ status(string, REQUIRED): åŠ å†•æ—¶å®¹å™¨çš„å‡ ä¸ªçŠ¶æ€ 1. creating 2. created 3. running 4. stopped pid: hostä¸Šçœ‹åˆ°çš„å®¹å™¨è¿›ç¨‹ bundleï¼šhostä¸Šå®¹å™¨bundleç›®å½•çš„ç»å¯¹è·¯å¾„ annotationï¼šå®¹å™¨ç›¸å…³çš„æ ‡æ³¨ï¼Œå¯é€‰ æ‰€ä»¥åœ¨Jsonçš„åºåˆ—åŒ–æ—¶,å¿…é¡»éµå®ˆä»¥ä¸‹æ ¼å¼\n{ \u0026#34;ociVersion\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;oci-container1\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;pid\u0026#34;: 4422, \u0026#34;bundle\u0026#34;: \u0026#34;/containers/redis\u0026#34;, \u0026#34;annotations\u0026#34;: { \u0026#34;myKey\u0026#34;: \u0026#34;myValue\u0026#34; } } Dockershim Dockershim ä½œç”¨ï¼šæŠŠå¤–éƒ¨æ”¶åˆ°çš„è¯·æ±‚è½¬åŒ–æˆ docker daemon èƒ½å¬æ‡‚çš„è¯·æ±‚ï¼Œè®© Docker Daemon æ‰§è¡Œåˆ›å»ºã€åˆ é™¤ç­‰å®¹å™¨æ“ä½œã€‚\nå…·ä½“çœ‹ä¸€ä¸‹kubeletæ˜¯æ€æ ·åˆ›å»ºå®¹å™¨çš„\nKubelet é€šè¿‡ CRI æ¥å£ï¼ˆgRPCï¼‰è°ƒç”¨dockershim,è¯·æ±‚åˆ›å»ºä¸€ä¸ªå®¹å™¨ã€‚CRI å³å®¹å™¨è¿è¡Œæ—¶æ¥å£ï¼Œè¿™ä¸€æ­¥ä¸­ï¼ŒKubelet å¯ä»¥è§†ä½œä¸€ä¸ªç®€å•çš„CRI Clientï¼Œè€Œ dockershim å°±æ˜¯æ¥æ”¶è¯·æ±‚çš„ Serverã€‚ç›®å‰dockershimæ˜¯å†…åµŒåœ¨ Kubelet ä¸­çš„ï¼Œæ‰€ä»¥æ¥æ”¶è°ƒç”¨å°±æ˜¯ Kubelet è¿›ç¨‹ã€‚ dockershimæ”¶åˆ°è¯·æ±‚åï¼Œè½¬åŒ–æˆ docker daemonçš„è¯·æ±‚ï¼Œå‘åˆ°docker daemon ä¸Šè¯·æ±‚åˆ›å»ºä¸€ä¸ªå®¹å™¨ã€‚ Docker Daemon æ—©åœ¨ 1.12 ç‰ˆæœ¬ä¸­å°±å·²ç»å°†é’ˆå¯¹å®¹å™¨çš„æ“ä½œç§»åˆ°å¦ä¸€ä¸ªå®ˆæŠ¤è¿›ç¨‹ containerd ä¸­ï¼Œå› æ­¤ Docker Daemon ä»ç„¶ä¸èƒ½å¸®æˆ‘ä»¬åˆ›å»ºå®¹å™¨ï¼Œè€Œæ˜¯è¦è¯·æ±‚ containerd åˆ›å»ºä¸€ä¸ªå®¹å™¨ã€‚ containerd æ”¶åˆ°è¯·æ±‚åï¼Œå¹¶ä¸ä¼šè‡ªå·±ç›´æ¥å»æ“ä½œå®¹å™¨ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªå«åš containerd-shim çš„è¿›ç¨‹ï¼Œè®© containerd-shim å»æ“ä½œå®¹å™¨ã€‚æ˜¯å› ä¸ºå®¹å™¨è¿›ç¨‹éœ€è¦ä¸€ä¸ªçˆ¶è¿›ç¨‹æ¥åšè¯¸å¦‚æ”¶é›†çŠ¶æ€ï¼Œç»´æŒ stdin ç­‰ fd æ‰“å¼€ç­‰å·¥ä½œã€‚è€Œå‡å¦‚è¿™ä¸ªçˆ¶è¿›ç¨‹å°±æ˜¯ containerdï¼Œé‚£æ¯æ¬¡ containerd æŒ‚æ‰æˆ–å‡çº§ï¼Œæ•´ä¸ªå®¿ä¸»æœºä¸Šæ‰€æœ‰çš„å®¹å™¨éƒ½å¾—é€€å‡ºäº†ã€‚è€Œå¼•å…¥äº† containerd-shim å°±è§„é¿äº†è¿™ä¸ªé—®é¢˜ï¼ˆcontainerd å’Œ shim å¹¶ä¸æ˜¯çˆ¶å­è¿›ç¨‹å…³ç³»ï¼‰ã€‚ æˆ‘ä»¬çŸ¥é“åˆ›å»ºå®¹å™¨éœ€è¦åšä¸€äº›è®¾ç½® namespaces å’Œ cgroupsï¼ŒæŒ‚è½½ root filesystem ç­‰ç­‰æ“ä½œï¼Œè€Œè¿™äº›äº‹è¯¥æ€ä¹ˆåšå·²ç»æœ‰äº†å…¬å¼€çš„è§„èŒƒï¼Œé‚£å°±æ˜¯ OCIã€‚å®ƒçš„ä¸€ä¸ªå‚è€ƒå®ç°å«åš runCã€‚äºæ˜¯ï¼Œcontainerd-shim åœ¨è¿™ä¸€æ­¥éœ€è¦è°ƒç”¨ runC è¿™ä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼Œæ¥å¯åŠ¨å®¹å™¨ã€‚ runC å¯åŠ¨å®Œå®¹å™¨åæœ¬èº«ä¼šç›´æ¥é€€å‡ºï¼Œcontainerd-shim åˆ™ä¼šæˆä¸ºå®¹å™¨è¿›ç¨‹çš„çˆ¶è¿›ç¨‹ï¼Œè´Ÿè´£æ”¶é›†å®¹å™¨è¿›ç¨‹çš„çŠ¶æ€ï¼Œä¸ŠæŠ¥ç»™ containerdï¼Œå¹¶åœ¨å®¹å™¨ä¸­ pid ä¸º 1 çš„è¿›ç¨‹é€€å‡ºåæ¥ç®¡å®¹å™¨ä¸­çš„å­è¿›ç¨‹è¿›è¡Œæ¸…ç†ï¼Œç¡®ä¿ä¸ä¼šå‡ºç°åƒµå°¸è¿›ç¨‹ã€‚ å‚è€ƒé“¾æ¥ ä¸æ”¯æŒdockeræˆ‘è¯¥ä½•å»ä½•ä»ï¼Ÿ ","date":"2022-02-13T00:00:00Z","permalink":"http://localhost:1313/docker/DockerShimRead/","title":"ä»€ä¹ˆæ˜¯dockershim"},{"content":" æ­¤é—®é¢˜å¼•å‡ºçš„æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­æ‰€æœ‰çš„èµ„æºå®Œå…¨å……è¶³,ä½†æ˜¯ä¼šå‡ºç°æ›´æ–°Podã€åˆ é™¤Podã€æ–°å»ºPodæ— æ³•è°ƒåº¦çš„æƒ…å†µã€‚\nç”Ÿäº§ç¯å¢ƒè§£å†³é—®é¢˜åŠæ³• æ‰¾åˆ°é—®é¢˜è·ŸåŸæ‰€åœ¨,é»˜è®¤çš„maxPods: 110,K8Sé»˜è®¤ä¸€ä¸ªèŠ‚ç‚¹ä¸Šçš„podè°ƒåº¦æ•°æ˜¯110ï¼Œå½“å‰æœ‰é™åˆ¶podæ•°çš„éœ€æ±‚ã€‚ vim /var/lib/kubelet/config.yaml\nmaxPods: 110 # ä¿®æ”¹ä¸ºmaxPods: 330 å½±å“Podè°ƒåº¦çš„æƒ…å†µ requestsèµ„æºé™åˆ¶ requestsï¼šæ˜¯ä¸€ç§ç¡¬é™åˆ¶,Kubernetesåœ¨è¿›è¡ŒPodè¯·æ±‚è°ƒåº¦çš„æ—¶å€™,èŠ‚ç‚¹çš„å¯ç”¨èµ„æºå¿…é¡»æ»¡è¶³500mçš„CPUæ‰èƒ½è¿›è¡Œè°ƒåº¦,ä¸”ä½¿ç”¨æœ€å¤§é™åˆ¶ä¸º1ä¸ªCPU,å¦‚æœè¯¥Podè¶…è¿‡è¯·æ±‚çš„æœ€å¤§é™åˆ¶,åˆ™Kuberneteså°†ä¼šæŠŠè¯¥Podè¿›è¡ŒKillé‡å¯ã€‚ resources: limits: cpu: \u0026#39;1\u0026#39; requests: cpu: 500m å½“ä½ è®¾ç½®requestä¸º500mä»¥åŠlimitä¸º1000mçš„æ—¶å€™,å½“ä½ ä½¿ç”¨ kubectl describe nodeæŸ¥çœ‹èŠ‚ç‚¹èµ„æºçš„æ—¶å€™å¯èƒ½ä¼šä¸ä½ è®¾ç½®çš„è¯·æ±‚é‡ä¸ç¬¦åˆ,è¿™æ˜¯ä»¥ä½ Pod çš„å®é™…ä½¿ç”¨é‡ä¸ºæ ‡å‡†çš„ã€‚\nèŠ‚ç‚¹æ ‡ç­¾çš„Label æ ‡ç­¾é€‰æ‹©å™¨ï¼š kubectl label node kubernetes-node1 env_role=dev é€šè¿‡æ­¤å‘½ä»¤å¯¹ç›¸åº”çš„èŠ‚ç‚¹åŠ å…¥æ ‡ç­¾ kubectl label node èŠ‚ç‚¹åç§° æ ‡ç­¾åç§° spec: nodeSelector: env_role: dev å½“ç„¶,ä½ ä¹Ÿå¯ä»¥é€šè¿‡kubectl get node --show-labelså‘½ä»¤æŸ¥çœ‹å½“å‰èŠ‚ç‚¹çš„æ ‡ç­¾\nNAME STATUS ROLES AGE VERSION LABELS master1 Ready,SchedulingDisabled master 141d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master1,kubernetes.io/os=linux,node-role.kubernetes.io/master= master2 Ready,SchedulingDisabled master 139d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master2,kubernetes.io/os=linux,node-role.kubernetes.io/master= master3 Ready,SchedulingDisabled master 139d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master3,kubernetes.io/os=linux,node-role.kubernetes.io/master= node1 Ready worker 141d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux,node-role.kubernetes.io/worker= node2 Ready worker 141d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux,node-role.kubernetes.io/worker= node3 Ready worker 141d v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node3,kubernetes.io/os=linux,node-role.kubernetes.io/worker= èŠ‚ç‚¹äº²å’Œæ€§ èŠ‚ç‚¹äº²å’Œæ€§ï¼šnodeAffinityå’Œä¹‹å‰nodeSelectoråŸºæœ¬ä¸Šæ˜¯ä¸€æ ·çš„,æœ‰çš„è¯æ»¡è¶³è¿›è¡Œè°ƒåº¦,å¦‚æœæ²¡æœ‰çš„è¯åˆ™ä¾æ—§ä¹Ÿå¯ä»¥è°ƒåº¦ã€‚ ç¡¬äº²å’Œæ€§ï¼šrequiredDuringSchedulingIgnoreDuringExecution,å½“å‰çº¦æŸçš„æ¡ä»¶è¡¨ç¤ºä¸ºåœ¨env_roleè¿™ä¸ªé”®ä¸­æœ‰dev/test æœ‰çš„è¯å³æ»¡è¶³çš„è°ƒåº¦,å¦‚æœä¸æ»¡è¶³åˆ™ä¸è°ƒåº¦ã€‚ è½¯äº²å’Œæ€§: preferredDuringSchedulingIgnoredDuringExecution,è¿›è¡Œå°è¯•æ˜¯å¦æ»¡è¶³æµ‹è¯•,å¦‚æœæ»¡è¶³åˆ™æ»¡è¶³è°ƒåº¦,å¦‚æœä¸æ»¡è¶³åˆ™ä¾æ—§ä¼šè¿›è¡Œè°ƒåº¦ã€‚ æ”¯æŒçš„æ“ä½œç¬¦ï¼šIn/Not In/Gt/Lt/DoesNotExistsåˆ†åˆ«ä¸º å­˜åœ¨ã€ä¸å­˜åœ¨ã€å¤§äºã€å°äºã€ä¸å­˜åœ¨ã€‚ spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoreDuringExecution: nodeSelectorTerms: - metchExpressions: - key: env_role operator: In values: - dev - test preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 # è¡¨ç¤ºæƒé‡ æ¯”ä¾‹ preference: matchExpressions: - key: group operator: In # æ“ä½œç¬¦ In values: - otherprod æ±¡ç‚¹å’Œæ±¡ç‚¹å®¹å¿ æ±¡ç‚¹ï¼šnodeSelectorå’ŒnodeAffinityPodè°ƒåº¦åœ¨æŸäº›èŠ‚ç‚¹ä¸Š,æ˜¯å±äºPodçš„å±æ€§,åœ¨è°ƒåº¦çš„æ—¶å€™è¿›è¡Œå®ç°,è€Œæ±¡ç‚¹æ˜¯å¯¹èŠ‚ç‚¹åšä¸åˆ†é…è°ƒåº¦,æ˜¯èŠ‚ç‚¹å±æ€§ã€‚ æ±¡ç‚¹å®¹å¿ï¼šå½“ä¸€ä¸ªæ±¡ç‚¹ä¸å…è®¸è¢«è°ƒåº¦çš„æ—¶å€™,åŒæ—¶åˆæƒ³è®©ä»–å¯èƒ½ä¼šå‚ä¸è°ƒåº¦,ç±»ä¼¼äºè½¯äº²å’Œæ€§ã€‚ åœºæ™¯ï¼šä½œä¸ºä¸“ç”¨èŠ‚ç‚¹ã€é…ç½®ç‰¹å®šç¡¬ä»¶èŠ‚ç‚¹ã€åŸºäºTainté©±é€ NoScheduleï¼šä¸€å®šä¸è¢«è°ƒåº¦ PreferNoSchdule: å°½é‡ä¸è¢«è°ƒåº¦ NoExecute: ä¸è°ƒåº¦,å¹¶ä¸”ä¼šé©±é€åœ¨è¯¥èŠ‚ç‚¹ä¸ŠPod # æ±¡ç‚¹å®¹å¿ spec: tolerations: - key: \u0026#34;env_role\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;yes\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; ä½¿ç”¨kubectl describe node kubernetes-master1 | grep Taintsè¿›è¡ŒæŸ¥çœ‹æ˜¯å¦ä¸ºæ±¡ç‚¹ã€‚ ä½¿ç”¨kubectl taint node èŠ‚ç‚¹åç§° key=value:æ±¡ç‚¹å€¼\n","date":"2021-12-21T00:00:00Z","image":"https://bj.bcebos.com/baidu-rmb-video-cover-1/2b6495c8749e3f4e4369e28cb50eeb87.png","permalink":"http://localhost:1313/kubernetes/PodSchedulingIssues/","title":"æœ‰å…³äºKubernetesä¸­å½±å“Podè°ƒåº¦çš„é—®é¢˜"},{"content":" æ³¨æ„ï¼šè¯·å„ä½è®°ä½æŠŠæ‰€æœ‰ç¦»çº¿åŒ…å…¨æ‹¿åˆ°æœ¬åœ°â€¦\nåœ¨çº¿éƒ¨ç½²chartmuseum ç›´æ¥ä½¿ç”¨æœ€ç®€å•çš„ docker run æ–¹å¼ï¼Œä½¿ç”¨local æœ¬åœ°å­˜å‚¨æ–¹å¼ï¼Œé€šè¿‡ -v æ˜ å°„åˆ°å®¿ä¸»æœº /opt/charts æ›´å¤šæ”¯æŒå®‰è£…æ–¹å¼è§å®˜ç½‘\nmkdir /opt/charts docker run -d \\ -p 8080:8080 \\ -e DEBUG=1 \\ -e STORAGE=local \\ -e STORAGE_LOCAL_ROOTDIR=/charts \\ -v /opt/charts:/charts \\ chartmuseum/chartmuseum:latest ä¸‹è½½SkywalkingåŒ… git clone https://github.com/apache/skywalking-kubernetes.git # æ›´æ¢ä»“åº“ cd skywalking-kubernetes-master/chart/skywalking/ vim Chats.yaml dependencies: - name: elasticsearch version: ~7.12.1 # å®˜ç½‘çš„ç‰ˆæœ¬å·ä¸º7.5.1 æœ€æ–°çš„elasticç‰ˆæœ¬ä¸º7.12.1 repository: http://localhost:8080 # ä¿®æ”¹ä¸ºä½ æœ¬åœ°çš„Repoåœ°å€ condition: elasticsearch.enabled æ·»åŠ elasticsearchä»“åº“ helm repo add elastic https://helm.elastic.co helm pull elastic/elasticsearch # æŠŠelasticsearchå†…å®¹æ‹‰ä¸‹æ¥ ä¸Šä¼ æœ¬åœ°Helm ä»¥é˜²ä¸‡ä¸€è¯·å…ˆå®‰è£…helmpushæ’ä»¶\nhttps://github.com/chartmuseum/helm-push\nhelm repo add chartmuseum http://localhost:8080 curl --data-binary \u0026#34;@elasticsearch-7.12.1.tgz\u0026#34; http://localhost:8080/api/charts helm push /root/skywalking-kubernetes-master/chart/skywalking/ chartmuseum helm repo update # æ›´æ–°ä»“åº“ ä½ å¯ä»¥å°è¯•æœç´¢ä¸€ä¸‹\nä¿è¯ä»“åº“ä¸­å­˜åœ¨elasticsarchå’Œskywalking\n[root@k-master1 ~]# helm search repo NAME CHART VERSION APP VERSION DESCRIPTION chartmuseum/elasticsearch 7.12.1 7.12.1 Official Elastic helm chart for Elasticsearch chartmuseum/skywalking 4.0.0 Apache SkyWalking APM System éƒ¨ç½²skywalking cd skywalking-kubernetes/chart helm dep up skywalking # change the release name according to your scenario export SKYWALKING_RELEASE_NAME=skywalking # change the namespace according to your scenario export SKYWALKING_RELEASE_NAMESPACE=default helm install \u0026#34;${SKYWALKING_RELEASE_NAME}\u0026#34; skywalking -n \u0026#34;${SKYWALKING_RELEASE_NAMESPACE}\u0026#34; \\ --set oap.image.tag=8.1.0-es7 \\ --set oap.storageType=elasticsearch7 \\ --set ui.image.tag=8.1.0 \\ --set elasticsearch.imageTag=7.5.1 helm uninstall skywalking # å¸è½½skywalking å‡†å¤‡ç¦»çº¿é•œåƒ busybox:1.30 docker.elastic.co/elasticsearch/elasticsearch:7.5.1 apache/skywalking-oap-server:8.1.0-es7 apache/skywalking-ui:8.1.0 chartmuseum/chartmuseum:latest Helmä¸­çš„Elasticsearchå¯èƒ½ä¼šå­˜åœ¨é—®é¢˜ ä½ ä»¬ä¹Ÿå¯ä»¥ç”¨æˆ‘çš„è¿™ä¸ªelasticsearché…ç½® æ³¨æ„ä¿®æ”¹PVC\nkind: StatefulSet apiVersion: apps/v1 metadata: name: elasticsearch-master namespace: default labels: app: elasticsearch-master app.kubernetes.io/managed-by: Helm chart: elasticsearch heritage: Helm release: skywalking annotations: esMajorVersion: \u0026#39;7\u0026#39; meta.helm.sh/release-name: skywalking meta.helm.sh/release-namespace: default spec: replicas: 3 selector: matchLabels: app: elasticsearch-master template: metadata: name: elasticsearch-master creationTimestamp: null labels: app: elasticsearch-master chart: elasticsearch heritage: Helm release: skywalking spec: initContainers: - name: configure-sysctl image: \u0026#39;docker.elastic.co/elasticsearch/elasticsearch:7.5.1\u0026#39; command: - sysctl - \u0026#39;-w\u0026#39; - vm.max_map_count=262144 resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: IfNotPresent securityContext: privileged: true runAsUser: 0 containers: - name: elasticsearch image: \u0026#39;docker.elastic.co/elasticsearch/elasticsearch:7.5.1\u0026#39; ports: - name: http containerPort: 9200 protocol: TCP - name: transport containerPort: 9300 protocol: TCP volumeMounts: - name: datadir mountPath: /usr/share/elasticsearch/data env: - name: node.name valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: cluster.initial_master_nodes value: \u0026gt;- elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2, - name: discovery.seed_hosts value: elasticsearch-master-headless - name: cluster.name value: elasticsearch - name: network.host value: 0.0.0.0 - name: ES_JAVA_OPTS value: \u0026#39;-Xmx1g -Xms1g\u0026#39; - name: node.data value: \u0026#39;true\u0026#39; - name: node.ingest value: \u0026#39;true\u0026#39; - name: node.master value: \u0026#39;true\u0026#39; resources: limits: cpu: \u0026#39;1\u0026#39; memory: 2Gi requests: cpu: 100m memory: 2Gi readinessProbe: exec: command: - sh - \u0026#39;-c\u0026#39; - \u0026gt; #!/usr/bin/env bash -e # If the node is starting up wait for the cluster to be ready (request params: \u0026#39;wait_for_status=green\u0026amp;timeout=1s\u0026#39; ) # Once it has started only check that the node itself is responding START_FILE=/tmp/.es_start_file http () { local path=\u0026#34;${1}\u0026#34; if [ -n \u0026#34;${ELASTIC_USERNAME}\u0026#34; ] \u0026amp;\u0026amp; [ -n \u0026#34;${ELASTIC_PASSWORD}\u0026#34; ]; then BASIC_AUTH=\u0026#34;-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}\u0026#34; else BASIC_AUTH=\u0026#39;\u0026#39; fi curl -XGET -s -k --fail ${BASIC_AUTH} http://127.0.0.1:9200${path} } if [ -f \u0026#34;${START_FILE}\u0026#34; ]; then echo \u0026#39;Elasticsearch is already running, lets check the node is healthy and there are master nodes available\u0026#39; http \u0026#34;/_cluster/health?timeout=0s\u0026#34; else echo \u0026#39;Waiting for elasticsearch cluster to become cluster to be ready (request params: \u0026#34;wait_for_status=green\u0026amp;timeout=1s\u0026#34; )\u0026#39; if http \u0026#34;/_cluster/health?wait_for_status=green\u0026amp;timeout=1s\u0026#34; ; then touch ${START_FILE} exit 0 else echo \u0026#39;Cluster is not yet ready (request params: \u0026#34;wait_for_status=green\u0026amp;timeout=1s\u0026#34; )\u0026#39; exit 1 fi fi initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 10 successThreshold: 3 failureThreshold: 3 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: IfNotPresent securityContext: capabilities: drop: - ALL runAsUser: 1000 runAsNonRoot: true restartPolicy: Always terminationGracePeriodSeconds: 120 dnsPolicy: ClusterFirst securityContext: runAsUser: 1000 fsGroup: 1000 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - elasticsearch-master topologyKey: kubernetes.io/hostname schedulerName: default-scheduler volumeClaimTemplates: - metadata: name: datadir annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;managed-nfs-storage-class\u0026#34; spec: accessModes: [\u0026#34;ReadWriteMany\u0026#34;] resources: requests: storage: 10Gi serviceName: elasticsearch-master-headless podManagementPolicy: Parallel updateStrategy: type: RollingUpdate revisionHistoryLimit: 10 ","date":"2021-04-07T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/posts/kubernetes-skywallking/","title":"kubernetes-ç¦»çº¿éƒ¨ç½²Skywallking"},{"content":"Redis Clusterï¼ˆRedisé›†ç¾¤ï¼‰ç®€ä»‹ redisæ˜¯ä¸€ä¸ªå¼€æºçš„key valueå­˜å‚¨ç³»ç»Ÿï¼Œå—åˆ°äº†å¹¿å¤§äº’è”ç½‘å…¬å¸çš„é’çã€‚redis3.0ç‰ˆæœ¬ä¹‹å‰åªæ”¯æŒå•ä¾‹æ¨¡å¼ï¼Œåœ¨3.0ç‰ˆæœ¬åŠä»¥åæ‰æ”¯æŒé›†ç¾¤ï¼Œæˆ‘è¿™é‡Œç”¨çš„æ˜¯redis3.0.0ç‰ˆæœ¬ï¼› redisé›†ç¾¤é‡‡ç”¨P2Pæ¨¡å¼ï¼Œæ˜¯å®Œå…¨å»ä¸­å¿ƒåŒ–çš„ï¼Œä¸å­˜åœ¨ä¸­å¿ƒèŠ‚ç‚¹æˆ–è€…ä»£ç†èŠ‚ç‚¹ï¼› redisé›†ç¾¤æ˜¯æ²¡æœ‰ç»Ÿä¸€çš„å…¥å£çš„ï¼Œå®¢æˆ·ç«¯ï¼ˆclientï¼‰è¿æ¥é›†ç¾¤çš„æ—¶å€™è¿æ¥é›†ç¾¤ä¸­çš„ä»»æ„èŠ‚ç‚¹ï¼ˆnodeï¼‰å³å¯ï¼Œé›†ç¾¤å†…éƒ¨çš„èŠ‚ç‚¹æ˜¯ç›¸äº’é€šä¿¡çš„ï¼ˆPING-PONGæœºåˆ¶ï¼‰ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æ˜¯ä¸€ä¸ªrediså®ä¾‹ï¼› ä¸ºäº†å®ç°é›†ç¾¤çš„é«˜å¯ç”¨ï¼Œå³åˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦å¥åº·ï¼ˆèƒ½å¦æ­£å¸¸ä½¿ç”¨ï¼‰ï¼Œredis-clusteræœ‰è¿™ä¹ˆä¸€ä¸ªæŠ•ç¥¨å®¹é”™æœºåˆ¶ï¼šå¦‚æœé›†ç¾¤ä¸­è¶…è¿‡åŠæ•°çš„èŠ‚ç‚¹æŠ•ç¥¨è®¤ä¸ºæŸä¸ªèŠ‚ç‚¹æŒ‚äº†ï¼Œé‚£ä¹ˆè¿™ä¸ªèŠ‚ç‚¹å°±æŒ‚äº†ï¼ˆfailï¼‰ã€‚è¿™æ˜¯åˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦æŒ‚äº†çš„æ–¹æ³•ï¼› é‚£ä¹ˆå¦‚ä½•åˆ¤æ–­é›†ç¾¤æ˜¯å¦æŒ‚äº†å‘¢? -\u0026gt; å¦‚æœé›†ç¾¤ä¸­ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹æŒ‚äº†ï¼Œè€Œä¸”è¯¥èŠ‚ç‚¹æ²¡æœ‰ä»èŠ‚ç‚¹ï¼ˆå¤‡ä»½èŠ‚ç‚¹ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸ªé›†ç¾¤å°±æŒ‚äº†ã€‚è¿™æ˜¯åˆ¤æ–­é›†ç¾¤æ˜¯å¦æŒ‚äº†çš„æ–¹æ³•ï¼› é‚£ä¹ˆä¸ºä»€ä¹ˆä»»æ„ä¸€ä¸ªèŠ‚ç‚¹æŒ‚äº†ï¼ˆæ²¡æœ‰ä»èŠ‚ç‚¹ï¼‰è¿™ä¸ªé›†ç¾¤å°±æŒ‚äº†å‘¢ï¼Ÿ -\u0026gt; å› ä¸ºé›†ç¾¤å†…ç½®äº†16384ä¸ªslotï¼ˆå“ˆå¸Œæ§½ï¼‰ï¼Œå¹¶ä¸”æŠŠæ‰€æœ‰çš„ç‰©ç†èŠ‚ç‚¹æ˜ å°„åˆ°äº†è¿™16384[0-16383]ä¸ªslotä¸Šï¼Œæˆ–è€…è¯´æŠŠè¿™äº›slotå‡ç­‰çš„åˆ†é…ç»™äº†å„ä¸ªèŠ‚ç‚¹ã€‚å½“éœ€è¦åœ¨Redisé›†ç¾¤å­˜æ”¾ä¸€ä¸ªæ•°æ®ï¼ˆkey-valueï¼‰æ—¶ï¼Œredisä¼šå…ˆå¯¹è¿™ä¸ªkeyè¿›è¡Œcrc16ç®—æ³•ï¼Œç„¶åå¾—åˆ°ä¸€ä¸ªç»“æœã€‚å†æŠŠè¿™ä¸ªç»“æœå¯¹16384è¿›è¡Œæ±‚ä½™ï¼Œè¿™ä¸ªä½™æ•°ä¼šå¯¹åº”[0-16383]å…¶ä¸­ä¸€ä¸ªæ§½ï¼Œè¿›è€Œå†³å®škey-valueå­˜å‚¨åˆ°å“ªä¸ªèŠ‚ç‚¹ä¸­ã€‚æ‰€ä»¥ä¸€æ—¦æŸä¸ªèŠ‚ç‚¹æŒ‚äº†ï¼Œè¯¥èŠ‚ç‚¹å¯¹åº”çš„slotå°±æ— æ³•ä½¿ç”¨ï¼Œé‚£ä¹ˆå°±ä¼šå¯¼è‡´é›†ç¾¤æ— æ³•æ­£å¸¸å·¥ä½œã€‚ ç»¼ä¸Šæ‰€è¿°ï¼Œæ¯ä¸ªRedisé›†ç¾¤ç†è®ºä¸Šæœ€å¤šå¯ä»¥æœ‰16384ä¸ªèŠ‚ç‚¹ã€‚ Redisé›†ç¾¤è‡³å°‘éœ€è¦3ä¸ªèŠ‚ç‚¹ï¼Œå› ä¸ºæŠ•ç¥¨å®¹é”™æœºåˆ¶è¦æ±‚è¶…è¿‡åŠæ•°èŠ‚ç‚¹è®¤ä¸ºæŸä¸ªèŠ‚ç‚¹æŒ‚äº†è¯¥èŠ‚ç‚¹æ‰æ˜¯æŒ‚äº†ï¼Œæ‰€ä»¥2ä¸ªèŠ‚ç‚¹æ— æ³•æ„æˆé›†ç¾¤ã€‚ è¦ä¿è¯é›†ç¾¤çš„é«˜å¯ç”¨ï¼Œéœ€è¦æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä»èŠ‚ç‚¹ï¼Œä¹Ÿå°±æ˜¯å¤‡ä»½èŠ‚ç‚¹ï¼Œæ‰€ä»¥Redisé›†ç¾¤è‡³å°‘éœ€è¦6å°æœåŠ¡å™¨ã€‚å› ä¸ºæˆ‘æ²¡æœ‰é‚£ä¹ˆå¤šæœåŠ¡å™¨ï¼Œä¹Ÿå¯åŠ¨ä¸äº†é‚£ä¹ˆå¤šè™šæ‹Ÿæœºï¼Œæ‰€åœ¨è¿™é‡Œæ­å»ºçš„æ˜¯ä¼ªåˆ†å¸ƒå¼é›†ç¾¤ï¼Œå³ä¸€å°æœåŠ¡å™¨è™šæ‹Ÿè¿è¡Œ6ä¸ªrediså®ä¾‹ï¼Œä¿®æ”¹ç«¯å£å·ä¸ºï¼ˆ7001-7006ï¼‰1+1+1+1+1+1 = 6\næ­å»ºé›†ç¾¤ Redisç‰ˆæœ¬6.0.8 Gcc7x.x.x åˆ›å»ºç›®å½• mkdir /usr/local/redis-cluster cd /usr/local/redis-cluster wget http://download.redis.io/releases/redis-6.0.8.tar.gz mkdir {7001..7006} å¤åˆ¶é…ç½®æ–‡ä»¶ tar -zxf redis-6.0.8.tar.gz cd redis-6.0.8/ \u0026amp;\u0026amp; make install cp -a redis-6.0.8/redis.conf 7001/ # ä»¥æ­¤ç±»æ¨ cp -a redis-6.0.8/redis.conf 7002/ å¦‚æœä½ ä¸æƒ³ç¼–è¯‘å®‰è£…çš„è¯,ä½ å¯ä»¥æŠŠredisä¸­çš„/binç›®å½•çš„å‘½ä»¤ç§»åŠ¨åˆ°æ¯ä¸ªnodeèŠ‚ç‚¹æ–‡ä»¶å¤¹ä¸­ï¼Œè¿™æ ·ä»¥æ–¹ä¾¿ä½ ä½¿ç”¨redis-serverå‘½ä»¤ ç¼–è¾‘é…ç½®æ–‡ä»¶ æ­¤æ–‡ä»¶å†…å®¹ä¸ºé›†ç¾¤æ¨¡å¼æœ€å°é…ç½®æ–‡ä»¶å†…å®¹.\nvim 7001/redis.conf # ä»¥æ­¤ç±»æ¨,è®°å¾—æ›´æ”¹ç«¯å£å·å’Œæ—¥å¿—æ–‡ä»¶ bind 127.0.0.1 # IPå¯æ›´æ¢ä¸ºå†…ç½‘IP port 7001 cluster-enabled yes cluster-config-file nodes7001.conf cluster-node-timeout 5000 appendonly yes daemonize yes logfile /usr/local/redis-cluster/7001/redis-7001.log maxmemory 4GB requirepass ******* dir /usr/local/redis-cluster/7001 masterauth **** port 7001 Redisè¿è¡Œç«¯å£ cluster-enabled yeså¯ç”¨é›†ç¾¤æ¨¡å¼ cluster-config-file nodes.confé›†ç¾¤æ¨¡å¼é…ç½®æ–‡ä»¶ cluster-node-timeout 5000èŠ‚ç‚¹çš„è¶…æ—¶æ—¶é™ appendonly yeså¼€å¯AOFæŒä¹…åŒ– daemonize yeså¼€å¯åå°è¿è¡Œ maxmemory 4GBRedisæœ€å¤§å¯ç”¨å†…å­˜ requirepassè¿æ¥Rediså®¢æˆ·ç«¯å¯†ç  masterauth Slaveè¿æ¥masteréœ€è¦çš„è®¤è¯ å¯åŠ¨é›†ç¾¤ è‡ªå·±å»ºä¸€ä¸ªå¯åŠ¨è„šæœ¬,è¦ä¸ç„¶æ‰‹åŠ¨å¯åŠ¨å¤ªéº»çƒ¦äº†\n#!/bin/bash redis-server /usr/local/redis-cluster/7001/redis.conf redis-server /usr/local/redis-cluster/7002/redis.conf redis-server /usr/local/redis-cluster/7003/redis.conf redis-server /usr/local/redis-cluster/7004/redis.conf redis-server /usr/local/redis-cluster/7005/redis.conf redis-server /usr/local/redis-cluster/7006/redis.conf chmod +x start.sh sh start.sh [root@bogon redis-cluster]# ps -aux | grep redis root 65558 0.0 0.0 64864 6256 ? Ssl 09:54 0:00 redis-server *:7001 [cluster] root 65564 0.0 0.0 61792 4760 ? Ssl 09:54 0:00 redis-server *:7002 [cluster] root 65566 0.0 0.0 61792 4736 ? Ssl 09:54 0:00 redis-server *:7003 [cluster] root 65572 0.0 0.0 61792 4712 ? Ssl 09:54 0:00 redis-server *:7004 [cluster] root 65578 0.0 0.0 61792 4704 ? Ssl 09:54 0:00 redis-server *:7005 [cluster] root 65580 0.0 0.0 61792 4780 ? Ssl 09:54 0:00 redis-server *:7006 [cluster] åŠ å…¥é›†ç¾¤ ç°åœ¨æˆ‘ä»¬æœ‰è®¸å¤šå®ä¾‹æ­£åœ¨è¿è¡Œï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å‘èŠ‚ç‚¹å†™å…¥ä¸€äº›æœ‰æ„ä¹‰çš„é…ç½®æ¥åˆ›å»ºé›†ç¾¤ã€‚\nå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯Redis 5æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œè¿™å¾ˆå®¹æ˜“å®Œæˆï¼Œå› ä¸ºåµŒå…¥åˆ°ä¸­çš„Redis Clusterå‘½ä»¤è¡Œå®ç”¨ç¨‹åºä¸ºæˆ‘ä»¬æä¾›äº†å¸®åŠ©ï¼Œè¯¥å®ç”¨ç¨‹åºredis-cliå¯ç”¨äºåˆ›å»ºæ–°é›†ç¾¤ï¼Œæ£€æŸ¥æˆ–é‡æ–°åˆ†ç‰‡ç°æœ‰é›†ç¾¤ç­‰ã€‚\nå¯¹äºRedisç‰ˆæœ¬3æˆ–4ï¼Œæœ‰ä¸€ä¸ªç§°ä¸ºçš„æ—§å·¥å…·redis-trib.rbï¼Œå®ƒéå¸¸ç›¸ä¼¼ã€‚æ‚¨å¯ä»¥srcåœ¨Redisæºä»£ç åˆ†å‘çš„ç›®å½•ä¸­æ‰¾åˆ°å®ƒã€‚æ‚¨éœ€è¦å®‰è£…redisgemæ‰èƒ½è¿è¡Œredis-tribã€‚\nå¦‚æœä½ æ˜¯ç”¨çš„æ˜¯Redis3.xæˆ–è€…4.x è¯·å‰å¾€å®˜ç½‘é“¾æ¥ ç‚¹æˆ‘è¿›å…¥\næ­¤æ–¹æ³•ä¸ºRedis5æˆ–è€…æ›´é«˜ç‰ˆæœ¬ redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 \\ 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 \\ --cluster-replicas 1 Can I set the above configuration? (type \u0026#39;yes\u0026#39; to accept): yes --cluster-replicas 1ç»™Masteråªåˆ†é…ä¸€ä¸ªslave è¿æ¥é›†ç¾¤ redis-cli -c -p 7001 -a *** 127.0.0.1:7001\u0026gt; info # Replication role:master connected_slaves:1 127.0.0.1:7001\u0026gt; set Host Linux7 -\u0026gt; Redirected to slot [16156] located at 127.0.0.1:7003 OK -aæ˜¯ä½ è®¾ç½®çš„requirepasså¯†ç  æ³¨æ„ï¼šå‡ºç°connected_slaves:1 è¡¨ç¤ºè¿æ¥åˆ°äº†ä¸€ä¸ªä»æœåŠ¡å™¨ å¦‚æœä¸º0 è¯·æŸ¥çœ‹æœåŠ¡å™¨é”™è¯¯æ—¥å¿—\næ•…éšœåˆ‡æ¢ è¿æ¥åˆ°7003çš„ä»æœåŠ¡å™¨7005 æŸ¥çœ‹æ•°æ®æ˜¯å¦åŒæ­¥\nredis-cli -c -p 7005 -a *** master_host:127.0.0.1 master_port:7003 127.0.0.1:7005\u0026gt; get Host \u0026#34;Linux7\u0026#34; å®•æœº7003æœåŠ¡å™¨\n[root@bogon redis-cluster]# ps -aux | grep 7003 root 70467 0.2 0.0 64352 5120 ? Ssl 11:20 0:01 redis-server *:7003 [cluster] root 70871 0.0 0.0 12112 1052 pts/0 S+ 11:29 0:00 grep --color=auto 7003 [root@bogon redis-cluster]# kill -15 70467 é€šè¿‡infoå‘ç°7005å·²ç»æˆä¸ºä¸»æœåŠ¡å™¨\n127.0.0.1:7005\u0026gt; info # Replication role:master connected_slaves:0 å†æ¬¡å¯åŠ¨7003å‘ç°å·²ç»æ›´æ”¹ä¸ºä»æœåŠ¡å™¨ï¼Œå¹¶ä¸”å·²ç»è¢«7005è¿æ¥åˆ°\n127.0.0.1:7005\u0026gt; # Replication role:master connected_slaves:1 æ€»ç»“ é¦–å…ˆ å…ˆè¯´ç»“è®ºï¼šredisé›†ç¾¤æ— æ³•ä¿è¯å¼ºä¸€è‡´æ€§\næ—¢ç„¶æ— æ³•ä¿è¯å¼ºä¸€è‡´æ€§ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰å¯èƒ½å‡ºç°å†™æ•°æ®ä¸¢å¤±çš„æƒ…å†µï¼Œæ¯”å¦‚ä¸€ä¸ªå®¢æˆ·ç«¯å‘ä¸€ä¸ªå†™è¯·æ±‚ç»™masterï¼Œmasterå†åŒæ­¥åˆ°slaveä¹‹å‰å°±ç»™clientä¸€ä¸ªå›æ‰§ã€‚è¿™ä¸ªæ—¶å€™ä¼šå­˜åœ¨ä¸€ä¸ªæ—¶é—´çª—å£ï¼Œmaster å’Œ slaveä¹‹é—´çš„æ•°æ®æ˜¯ä¸ä¸€è‡´çš„ã€‚ä½†æ˜¯redisçš„æœ€ç»ˆä¸€è‡´æ€§ä¼šä½¿masterå’Œslaveçš„æ•°æ®æ˜¯æœ€ç»ˆä¸€è‡´ã€‚\nå¦å¤–è¿˜æœ‰ä¸€ä¸ªå¯èƒ½ï¼Œåœ¨å®¢æˆ·ç«¯æ”¶åˆ°äº†masterçš„ä¸€ä¸ªå†™è¯·æ±‚å›æ‰§ä¹‹åï¼Œæ­¤æ—¶masterå‡†å¤‡æŠŠæ•°æ®åŒæ­¥åˆ°slaveï¼ŒåŒæ­¥ä¹‹å‰çªç„¶æŒ‚äº†ï¼Œé‚£ä¹ˆè¿™ä¸ªæ•°æ®çœŸçš„å°±æ˜¯ä¼šä¸¢å¤±äº†ã€‚\nå¦‚æœä¸ºäº†ä¿è¯å¼ºä¸€è‡´ï¼Œæ¯”å¦‚æˆ‘ä»¬æ¯ç§’åˆ·ç›˜è¿›è¡ŒæŒä¹…åŒ–ï¼Œé‚£ä¹ˆç‰ºç‰²äº†è¿™ä¸ªååé‡ï¼Œå°±ç‰¹åˆ«ç±»ä¼¼æˆ‘ä»¬å¸¸è¯´çš„åŒæ­¥å¤åˆ¶äº†ã€‚ä½†æ˜¯redisé›†ç¾¤æ˜¯æ²¡æœ‰å®ç°å¼ºä¸€è‡´çš„ã€‚\n1ã€redisä¿è¯æœ€ç»ˆä¸€è‡´æ€§\n2ã€ç”¨æœ€ç»ˆä¸€è‡´æ€§æ¢å–äº†é«˜ååé‡\n3ã€ä¸»èŠ‚ç‚¹æŒ‚äº†çš„æ—¶å€™ï¼Œå¦‚æœæ•°æ®æ²¡æœ‰åŒæ­¥åˆ°å¤‡èŠ‚ç‚¹ï¼Œæ˜¯ä¼šå‡ºç°æ•°æ®ä¸¢å¤±çš„æƒ…å†µ\n4ã€å‘ç”Ÿç½‘ç»œåˆ†åŒºçš„æ—¶å€™ä¹Ÿå¯èƒ½ä¼šä¸¢æ•°æ®ï¼Œè¿™ä¸ªæ—¶å€™æœ‰ä¸ªnode timeoutæ—¶é—´æ¦‚å¿µ\n","date":"2021-02-19T00:00:00Z","permalink":"http://localhost:1313/posts/redis-cluster/","title":"Redisé›†ç¾¤æ­å»º"},{"content":"Serviceçš„ç®€å•ç†è§£ Service æ˜¯ä¸€ç§æŠ½è±¡çš„å¯¹è±¡ï¼Œå®ƒå®šä¹‰äº†ä¸€ç»„ Pod çš„é€»è¾‘é›†åˆå’Œä¸€ä¸ªç”¨äºè®¿é—®å®ƒä»¬çš„ç­–ç•¥ï¼Œå…¶å®è¿™ä¸ªæ¦‚å¿µå’Œå¾®æœåŠ¡éå¸¸ç±»ä¼¼ã€‚ä¸€ä¸ª Serivce ä¸‹é¢åŒ…å«çš„ Pod é›†åˆæ˜¯ç”± Label Selector æ¥å†³å®šçš„ã€‚\nå‡å¦‚æˆ‘ä»¬åç«¯è¿è¡Œäº†3ä¸ªå‰¯æœ¬ï¼Œè¿™äº›å‰¯æœ¬éƒ½æ˜¯å¯ä»¥æ›¿ä»£çš„ï¼Œå› ä¸ºå‰ç«¯å¹¶ä¸å…³å¿ƒå®ƒä»¬ä½¿ç”¨çš„æ˜¯å“ªä¸€ä¸ªåç«¯æœåŠ¡ã€‚å°½ç®¡ç”±äºå„ç§åŸå› åç«¯çš„ Pod é›†åˆä¼šå‘é€å˜åŒ–ï¼Œä½†æ˜¯å‰ç«¯å´ä¸éœ€è¦çŸ¥é“è¿™äº›å˜åŒ–ï¼Œä¹Ÿä¸éœ€è¦è‡ªå·±ç”¨ä¸€ä¸ªåˆ—è¡¨æ¥è®°å½•è¿™äº›åç«¯çš„æœåŠ¡ï¼ŒService çš„è¿™ç§æŠ½è±¡å°±å¯ä»¥å¸®æˆ‘ä»¬è¾¾åˆ°è¿™ç§è§£è€¦çš„ç›®çš„ã€‚\nä¸‰ç§IP åœ¨ç»§ç»­å¾€ä¸‹å­¦ä¹  Service ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå¼„æ˜ç™½ Kubernetes ç³»ç»Ÿä¸­çš„ä¸‰ç§IPï¼Œå› ä¸ºç»å¸¸æœ‰åŒå­¦æ··ä¹±ã€‚\nNodeIPï¼šNode èŠ‚ç‚¹çš„ IP åœ°å€ PodIP: Pod çš„ IP åœ°å€ ClusterIP: Service çš„ IP åœ°å€ é¦–å…ˆï¼ŒNodeIPæ˜¯Kubernetesé›†ç¾¤ä¸­èŠ‚ç‚¹çš„ç‰©ç†ç½‘å¡IPåœ°å€(ä¸€èˆ¬ä¸ºå†…ç½‘)ï¼Œæ‰€æœ‰å±äºè¿™ä¸ªç½‘ç»œçš„æœåŠ¡å™¨ä¹‹é—´éƒ½å¯ä»¥ç›´æ¥é€šä¿¡ï¼Œæ‰€ä»¥Kubernetesé›†ç¾¤å¤–è¦æƒ³è®¿é—®Kubernetesé›†ç¾¤å†…éƒ¨çš„æŸä¸ªèŠ‚ç‚¹æˆ–è€…æœåŠ¡ï¼Œè‚¯å®šå¾—é€šè¿‡Node Pè¿›è¡Œé€šä¿¡ï¼ˆè¿™ä¸ªæ—¶å€™ä¸€èˆ¬æ˜¯é€šè¿‡å¤–ç½‘ IP äº†ï¼‰\nç„¶åPodIPæ˜¯æ¯ä¸ªPodçš„IPåœ°å€ï¼Œå®ƒæ˜¯ç½‘ç»œæ’ä»¶è¿›è¡Œåˆ†é…çš„ï¼Œå‰é¢æˆ‘ä»¬å·²ç»è®²è§£è¿‡\næœ€åClusterIPæ˜¯ä¸€ä¸ªè™šæ‹Ÿçš„IPï¼Œä»…ä»…ä½œç”¨äºKubernetes Service è¿™ä¸ªå¯¹è±¡ï¼Œç”±Kubernetesè‡ªå·±æ¥è¿›è¡Œç®¡ç†å’Œåˆ†é…åœ°å€ã€‚\nå®šä¹‰Servcie å®šä¹‰ Service çš„æ–¹å¼å’Œæˆ‘ä»¬å‰é¢å®šä¹‰çš„å„ç§èµ„æºå¯¹è±¡çš„æ–¹å¼ç±»å‹ï¼Œä¾‹å¦‚ï¼Œå‡å®šæˆ‘ä»¬æœ‰ä¸€ç»„ Pod æœåŠ¡ï¼Œå®ƒä»¬å¯¹å¤–æš´éœ²äº† 8080 ç«¯å£ï¼ŒåŒæ—¶éƒ½è¢«æ‰“ä¸Šäº† app=beijing-nginx è¿™æ ·çš„æ ‡ç­¾ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥åƒä¸‹é¢è¿™æ ·æ¥å®šä¹‰ä¸€ä¸ª Service å¯¹è±¡\napiVersion: v1 kind: Service metadata: name: public-beijing-nginx-service spec: selector: app: beijing-nginx ports: - protocol: TCP port: 80 targetPort: 80 # å¯ä»¥ç†è§£æˆæ˜¯serviceçš„è®¿é—®ç«¯å£ name: beijing-nginx-http ç„¶åé€šè¿‡çš„ä½¿ç”¨ kubectl create -f myservice.yaml å°±å¯ä»¥åˆ›å»ºä¸€ä¸ªåä¸º myservice çš„ Service å¯¹è±¡ï¼Œå®ƒä¼šå°†è¯·æ±‚ä»£ç†åˆ°ä½¿ç”¨ TCP ç«¯å£ä¸º 80ï¼Œå…·æœ‰æ ‡ç­¾ app=beijing-nginx-http çš„ Pod ä¸Šï¼Œè¿™ä¸ª Service ä¼šè¢«ç³»ç»Ÿåˆ†é…ä¸€ä¸ªæˆ‘ä»¬ä¸Šé¢è¯´çš„ Cluster IPï¼Œè¯¥ Service è¿˜ä¼šæŒç»­çš„ç›‘å¬ selector ä¸‹é¢çš„ Podï¼Œä¼šæŠŠè¿™äº› Pod ä¿¡æ¯æ›´æ–°åˆ°ä¸€ä¸ªåä¸º myservice çš„Endpoints å¯¹è±¡ä¸Šå»ï¼Œè¿™ä¸ªå¯¹è±¡å°±ç±»ä¼¼äºæˆ‘ä»¬ä¸Šé¢è¯´çš„ Pod é›†åˆäº†ã€‚\néœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒService èƒ½å¤Ÿå°†ä¸€ä¸ªæ¥æ”¶ç«¯å£æ˜ å°„åˆ°ä»»æ„çš„ targetPortã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒtargetPort å°†è¢«è®¾ç½®ä¸ºä¸ port å­—æ®µç›¸åŒçš„å€¼ã€‚å¯èƒ½æ›´æœ‰è¶£çš„æ˜¯ï¼ŒtargetPort å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¼•ç”¨äº† backend Pod çš„ä¸€ä¸ªç«¯å£çš„åç§°ã€‚å› å®é™…æŒ‡æ´¾ç»™è¯¥ç«¯å£åç§°çš„ç«¯å£å·ï¼Œåœ¨æ¯ä¸ª backend Pod ä¸­å¯èƒ½å¹¶ä¸ç›¸åŒï¼Œæ‰€ä»¥å¯¹äºéƒ¨ç½²å’Œè®¾è®¡ Serviceï¼Œè¿™ç§æ–¹å¼ä¼šæä¾›æ›´å¤§çš„çµæ´»æ€§ã€‚\nå¦å¤– Service èƒ½å¤Ÿæ”¯æŒ TCP å’Œ UDP åè®®ï¼Œé»˜è®¤æ˜¯ TCP åè®®ã€‚\nkube-proxy å‰é¢æˆ‘ä»¬è®²åˆ°è¿‡ï¼Œåœ¨ Kubernetes é›†ç¾¤ä¸­ï¼Œæ¯ä¸ª Node ä¼šè¿è¡Œä¸€ä¸ª kube-proxy è¿›ç¨‹, è´Ÿè´£ä¸º Service å®ç°ä¸€ç§ VIPï¼ˆè™šæ‹Ÿ IPï¼Œå°±æ˜¯æˆ‘ä»¬ä¸Šé¢è¯´çš„ clusterIPï¼‰çš„ä»£ç†å½¢å¼ï¼Œç°åœ¨çš„ Kubernetes ä¸­é»˜è®¤æ˜¯ä½¿ç”¨çš„ iptables è¿™ç§æ¨¡å¼æ¥ä»£ç†ã€‚\niptables è¿™ç§æ¨¡å¼ï¼Œkube-proxy ä¼š watch apiserver å¯¹ Service å¯¹è±¡å’Œ Endpoints å¯¹è±¡çš„æ·»åŠ å’Œç§»é™¤ã€‚å¯¹æ¯ä¸ª Serviceï¼Œå®ƒä¼šæ·»åŠ ä¸Š iptables è§„åˆ™ï¼Œä»è€Œæ•è·åˆ°è¾¾è¯¥ Service çš„ clusterIPï¼ˆè™šæ‹Ÿ IPï¼‰å’Œç«¯å£çš„è¯·æ±‚ï¼Œè¿›è€Œå°†è¯·æ±‚é‡å®šå‘åˆ° Service çš„ä¸€ç»„ backend ä¸­çš„æŸä¸€ä¸ª Pod ä¸Šé¢ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ Pod readiness æ¢é’ˆ éªŒè¯åç«¯ Pod å¯ä»¥æ­£å¸¸å·¥ä½œï¼Œä»¥ä¾¿ iptables æ¨¡å¼ä¸‹çš„ kube-proxy ä»…çœ‹åˆ°æµ‹è¯•æ­£å¸¸çš„åç«¯ï¼Œè¿™æ ·åšæ„å‘³ç€å¯ä»¥é¿å…å°†æµé‡é€šè¿‡ kube-proxy å‘é€åˆ°å·²çŸ¥å¤±è´¥çš„ Pod ä¸­ï¼Œæ‰€ä»¥å¯¹äºçº¿ä¸Šçš„åº”ç”¨æ¥è¯´ä¸€å®šè¦åš readiness æ¢é’ˆã€‚\nptables æ¨¡å¼çš„ kube-proxy é»˜è®¤çš„ç­–ç•¥æ˜¯ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªåç«¯ Podã€‚\næ¯”å¦‚å½“åˆ›å»º backend Service æ—¶ï¼ŒKubernetes ä¼šç»™å®ƒæŒ‡æ´¾ä¸€ä¸ªè™šæ‹Ÿ IP åœ°å€ï¼Œæ¯”å¦‚ 10.0.0.1ã€‚å‡è®¾ Service çš„ç«¯å£æ˜¯ 1234ï¼Œè¯¥ Service ä¼šè¢«é›†ç¾¤ä¸­æ‰€æœ‰çš„ kube-proxy å®ä¾‹è§‚å¯Ÿåˆ°ã€‚å½“ kube-proxy çœ‹åˆ°ä¸€ä¸ªæ–°çš„ Serviceï¼Œå®ƒä¼šå®‰è£…ä¸€ç³»åˆ—çš„ iptables è§„åˆ™ï¼Œä» VIP é‡å®šå‘åˆ° per-Service è§„åˆ™ã€‚ è¯¥ per-Service è§„åˆ™è¿æ¥åˆ° per-Endpoint è§„åˆ™ï¼Œè¯¥ per-Endpoint è§„åˆ™ä¼šé‡å®šå‘ï¼ˆç›®æ ‡ NATï¼‰åˆ°åç«¯çš„ Podã€‚\nä¼˜åŒ–iptablesæ¨¡å¼æ€§èƒ½ åœ¨å¤§å‹é›†ç¾¤ï¼ˆæœ‰æ•°ä¸‡ä¸ª Pod å’Œ Serviceï¼‰ä¸­ï¼Œå½“ Serviceï¼ˆæˆ–å…¶ EndpointSlicesï¼‰å‘ç”Ÿå˜åŒ–æ—¶ iptables æ¨¡å¼çš„ kube-proxy åœ¨æ›´æ–°å†…æ ¸ä¸­çš„è§„åˆ™æ—¶å¯èƒ½è¦ç”¨è¾ƒé•¿æ—¶é—´ã€‚ ä½ å¯ä»¥é€šè¿‡ä¿®æ”¹kube-proxyçš„ConfigMapä¸­çš„é€‰é¡¹æ¥è°ƒæ•´ kube-proxy çš„åŒæ­¥è¡Œä¸ºï¼š\niptables: minSyncPeriod: 1s syncPeriod: 30s minSyncPeriod: å‚æ•°è®¾ç½®å°è¯•åŒæ­¥ iptables è§„åˆ™ä¸å†…æ ¸ä¹‹é—´çš„æœ€çŸ­æ—¶é•¿ã€‚å¦‚æœæ˜¯ 0sï¼Œé‚£ä¹ˆæ¯æ¬¡æœ‰ä»»ä¸€ Service æˆ– Endpoint å‘ç”Ÿå˜æ›´æ—¶ï¼Œkube-proxy éƒ½ä¼šç«‹å³åŒæ­¥è¿™äº›è§„åˆ™ã€‚ è¿™ç§æ–¹å¼åœ¨è¾ƒå°çš„é›†ç¾¤ä¸­å¯ä»¥å·¥ä½œå¾—å¾ˆå¥½ï¼Œä½†å¦‚æœåœ¨å¾ˆçŸ­çš„æ—¶é—´å†…å¾ˆå¤šä¸œè¥¿å‘ç”Ÿå˜æ›´æ—¶ï¼Œå®ƒä¼šå¯¼è‡´å¤§é‡å†—ä½™å·¥ä½œã€‚ ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªç”± Deployment æ”¯æŒçš„ Serviceï¼Œå…±æœ‰ 100 ä¸ª Podï¼Œä½ åˆ é™¤äº†è¿™ä¸ª Deploymentï¼Œ ä¸”è®¾ç½®äº† minSyncPeriod: 0sï¼Œkube-proxy æœ€ç»ˆä¼šä» iptables è§„åˆ™ä¸­é€ä¸ªåˆ é™¤ Service çš„ Endpointï¼Œ æ€»å…±æ›´æ–° 100 æ¬¡ã€‚ä½¿ç”¨è¾ƒå¤§çš„ minSyncPeriod å€¼æ—¶ï¼Œå¤šä¸ª Pod åˆ é™¤äº‹ä»¶å°†è¢«èšåˆåœ¨ä¸€èµ·ï¼Œ å› æ­¤ kube-proxy æœ€ç»ˆå¯èƒ½ä¼šè¿›è¡Œä¾‹å¦‚ 5 æ¬¡æ›´æ–°ï¼Œæ¯æ¬¡ç§»é™¤ 20 ä¸ªç«¯ç‚¹ï¼Œ è¿™æ ·åœ¨ CPU åˆ©ç”¨ç‡æ–¹é¢æ›´æœ‰æ•ˆç‡ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°åŒæ­¥æ‰€æœ‰å˜æ›´ã€‚ é»˜è®¤å€¼ 1s å¯¹äºä¸­å°å‹é›†ç¾¤æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŠ˜è¡·æ–¹æ¡ˆã€‚ åœ¨å¤§å‹é›†ç¾¤ä¸­ï¼Œå¯èƒ½éœ€è¦å°†å…¶è®¾ç½®ä¸ºæ›´å¤§çš„å€¼ã€‚ ï¼ˆç‰¹åˆ«æ˜¯ï¼Œå¦‚æœ kube-proxy çš„ sync_proxy_rules_duration_seconds æŒ‡æ ‡è¡¨æ˜å¹³å‡æ—¶é—´è¿œå¤§äº 1 ç§’ï¼Œ é‚£ä¹ˆæé«˜ minSyncPeriod å¯èƒ½ä¼šä½¿æ›´æ–°æ›´æœ‰æ•ˆç‡ã€‚ï¼‰\nsyncPeriod: å‚æ•°æ§åˆ¶ä¸å•æ¬¡ Service å’Œ Endpoint çš„å˜æ›´æ²¡æœ‰ç›´æ¥å…³ç³»çš„å°‘æ•°åŒæ­¥æ“ä½œã€‚ ç‰¹åˆ«æ˜¯ï¼Œå®ƒæ§åˆ¶ kube-proxy åœ¨å¤–éƒ¨ç»„ä»¶å·²å¹²æ¶‰ kube-proxy çš„ iptables è§„åˆ™æ—¶é€šçŸ¥çš„é€Ÿåº¦ã€‚ åœ¨å¤§å‹é›†ç¾¤ä¸­ï¼Œkube-proxy ä¹Ÿä»…åœ¨æ¯éš” syncPeriod æ—¶é•¿æ‰§è¡ŒæŸäº›æ¸…ç†æ“ä½œï¼Œä»¥é¿å…ä¸å¿…è¦çš„å·¥ä½œã€‚ IPVS åœ¨ ipvs æ¨¡å¼ä¸‹ï¼Œkube-proxy ç›‘è§† Kubernetes Service å’Œ EndpointSliceï¼Œ ç„¶åè°ƒç”¨ netlink æ¥å£åˆ›å»º IPVS è§„åˆ™ï¼Œ å¹¶å®šæœŸä¸ Kubernetes Service å’Œ EndpointSlice åŒæ­¥ IPVS è§„åˆ™ã€‚ è¯¥æ§åˆ¶å›è·¯ç¡®ä¿ IPVS çŠ¶æ€ä¸æœŸæœ›çš„çŠ¶æ€ä¿æŒä¸€è‡´ã€‚ è®¿é—® Service æ—¶ï¼ŒIPVS ä¼šå°†æµé‡å¯¼å‘åˆ°æŸä¸€ä¸ªåç«¯ Podã€‚\nIPVS ä»£ç†æ¨¡å¼åŸºäº netfilter å›è°ƒå‡½æ•°ï¼Œç±»ä¼¼äº iptables æ¨¡å¼ï¼Œ ä½†å®ƒä½¿ç”¨å“ˆå¸Œè¡¨ä½œä¸ºåº•å±‚æ•°æ®ç»“æ„ï¼Œåœ¨å†…æ ¸ç©ºé—´ä¸­ç”Ÿæ•ˆã€‚ è¿™æ„å‘³ç€ IPVS æ¨¡å¼ä¸‹çš„ kube-proxy æ¯” iptables æ¨¡å¼ä¸‹çš„ kube-proxy é‡å®šå‘æµé‡çš„å»¶è¿Ÿæ›´ä½ï¼ŒåŒæ­¥ä»£ç†è§„åˆ™æ—¶æ€§èƒ½ä¹Ÿæ›´å¥½ã€‚ ä¸å…¶ä»–ä»£ç†æ¨¡å¼ç›¸æ¯”ï¼ŒIPVS æ¨¡å¼è¿˜æ”¯æŒæ›´é«˜çš„ç½‘ç»œæµé‡ååé‡ã€‚\nIPVS æä¾›äº†æ›´å¤šé€‰é¡¹æ¥å¹³è¡¡åç«¯ Pod çš„æµé‡ï¼Œé»˜è®¤æ˜¯ rrï¼Œæœ‰å¦‚ä¸‹ä¸€äº›ç­–ç•¥ï¼š\nrr: è½®è¯¢ lc: æœ€å°‘è¿æ¥ï¼ˆæ‰“å¼€è¿æ¥æ•°æœ€å°‘ï¼‰ dh: ç›®æ ‡åœ°å€å“ˆå¸Œ sh: æºåœ°å€å“ˆå¸Œ sed: æœ€çŸ­é¢„æœŸå»¶è¿Ÿ nq:æœ€å°‘é˜Ÿåˆ— ä¸è¿‡ç°åœ¨åªèƒ½æ•´ä½“ä¿®æ”¹ç­–ç•¥ï¼Œå¯ä»¥é€šè¿‡ kube-proxy ä¸­é…ç½® â€“ipvs-scheduler å‚æ•°æ¥å®ç°ï¼Œæš‚æ—¶ä¸æ”¯æŒç‰¹å®šçš„ Service è¿›è¡Œé…ç½®ã€‚\nå¼€å¯ipvsæ¨¡å—\nmodprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 yum install ipvsadm ipset -y ä¿®æ”¹kube-proxyçš„configMap\nkubectl edit configmap kube-proxy -n kube-system # ä¿®æ”¹modeä¸º\u0026#34;ipvs\u0026#34; minSyncPeriod: 0s scheduler: \u0026#34;\u0026#34; syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \u0026#34;ipvs\u0026#34; # ä¿®æ”¹æ­¤å¤„ nodePortAddresses: null ä¿®æ”¹å®Œæˆåè®°å¾—é‡å¯kube-proxyï¼Œç„¶åä½¿ç”¨ipvsadm -lnæ ¡éªŒã€‚æ­£å¸¸å¯ä»¥å‡ºç°å¾ˆå¤šçš„è§„åˆ™é“¾æ¡ã€‚\nä¼šè¯äº²å’Œæ€§ åœ¨è¿™äº›ä»£ç†æ¨¡å‹ä¸­ï¼Œç»‘å®šåˆ° Service IP:Port çš„æµé‡è¢«ä»£ç†åˆ°åˆé€‚çš„åç«¯ï¼Œ å®¢æˆ·ç«¯ä¸éœ€è¦çŸ¥é“ä»»ä½•å…³äº Kubernetesã€Service æˆ– Pod çš„ä¿¡æ¯ã€‚\nå¦‚æœè¦ç¡®ä¿æ¥è‡ªç‰¹å®šå®¢æˆ·ç«¯çš„è¿æ¥æ¯æ¬¡éƒ½ä¼ é€’ç»™åŒä¸€ä¸ª Podï¼Œ ä½ å¯ä»¥é€šè¿‡è®¾ç½® Service çš„ .spec.sessionAffinity ä¸º ClientIP æ¥è®¾ç½®åŸºäºå®¢æˆ·ç«¯ IP åœ°å€çš„ä¼šè¯äº²å’Œæ€§ï¼ˆé»˜è®¤ä¸º Noneï¼‰ã€‚\napiVersion: v1 kind: Service metadata: name: demo spec: sessionAffinity: ClientIP ä¼šè¯è¶…æ—¶ ä½ è¿˜å¯ä»¥é€šè¿‡è®¾ç½® Service çš„ .spec.sessionAffinityConfig.clientIP.timeoutSeconds æ¥è®¾ç½®æœ€å¤§ä¼šè¯ç²˜æ€§æ—¶é—´ï¼ˆé»˜è®¤å€¼ä¸º 10800ï¼Œå³ 3 å°æ—¶ï¼‰ã€‚\napiVersion: v1 kind: Service metadata: name: demo spec: sessionAffinityConfig: clientIP: imeoutSeconds: 10800 Service å°†è¿è¡Œåœ¨ä¸€ç»„ Pods ä¸Šçš„åº”ç”¨ç¨‹åºå…¬å¼€ä¸ºç½‘ç»œæœåŠ¡çš„æŠ½è±¡æ–¹æ³•ã€‚\nä½¿ç”¨ Kubernetesï¼Œä½ æ— éœ€ä¿®æ”¹åº”ç”¨ç¨‹åºå»ä½¿ç”¨ä¸ç†Ÿæ‚‰çš„æœåŠ¡å‘ç°æœºåˆ¶ã€‚ Kubernetes ä¸º Pod æä¾›è‡ªå·±çš„ IP åœ°å€ï¼Œå¹¶ä¸ºä¸€ç»„ Pod æä¾›ç›¸åŒçš„ DNS åï¼Œ å¹¶ä¸”å¯ä»¥åœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œè´Ÿè½½å‡è¡¡ã€‚\nKubernetes ServiceTypes å…è®¸æŒ‡å®šä½ æ‰€éœ€è¦çš„ Service ç±»å‹ã€‚\nClusterIPï¼šé€šè¿‡é›†ç¾¤çš„å†…éƒ¨ IP æš´éœ²æœåŠ¡ï¼Œé€‰æ‹©è¯¥å€¼æ—¶æœåŠ¡åªèƒ½å¤Ÿåœ¨é›†ç¾¤å†…éƒ¨è®¿é—®ã€‚ è¿™ä¹Ÿæ˜¯ä½ æ²¡æœ‰ä¸ºæœåŠ¡æ˜¾å¼æŒ‡å®š type æ—¶ä½¿ç”¨çš„é»˜è®¤å€¼ã€‚ ä½ å¯ä»¥ä½¿ç”¨ Ingress æˆ–è€… Gateway API å‘å…¬ä¼—æš´éœ²æœåŠ¡ã€‚ NodePortï¼š é€šè¿‡æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ IP å’Œé™æ€ç«¯å£ï¼ˆNodePortï¼‰æš´éœ²æœåŠ¡ã€‚ ä¸ºäº†è®©èŠ‚ç‚¹ç«¯å£å¯ç”¨ï¼ŒKubernetes è®¾ç½®äº†é›†ç¾¤ IP åœ°å€ï¼Œè¿™ç­‰åŒäºä½ è¯·æ±‚ type: ClusterIP çš„æœåŠ¡ã€‚ LoadBalancerï¼šä½¿ç”¨äº‘æä¾›å•†çš„è´Ÿè½½å‡è¡¡å™¨å‘å¤–éƒ¨æš´éœ²æœåŠ¡ã€‚ å¤–éƒ¨è´Ÿè½½å‡è¡¡å™¨å¯ä»¥å°†æµé‡è·¯ç”±åˆ°è‡ªåŠ¨åˆ›å»ºçš„ NodePort æœåŠ¡å’Œ ClusterIP æœåŠ¡ä¸Šã€‚ ExternalNameï¼šé€šè¿‡è¿”å› CNAME è®°å½•å’Œå¯¹åº”å€¼ï¼Œå¯ä»¥å°†æœåŠ¡æ˜ å°„åˆ° externalName å­—æ®µçš„å†…å®¹ï¼ˆä¾‹å¦‚ï¼Œfoo.bar.example.comï¼‰ã€‚ æ— éœ€åˆ›å»ºä»»ä½•ç±»å‹ä»£ç†ã€‚ NodePort å¦‚æœä½ å°† type å­—æ®µè®¾ç½®ä¸º NodePortï¼Œåˆ™ Kubernetes æ§åˆ¶å¹³é¢å°†åœ¨ --service-node-port-range æ ‡å¿—æŒ‡å®šçš„èŒƒå›´å†…åˆ†é…ç«¯å£ï¼ˆé»˜è®¤å€¼ï¼š30000-32767ï¼‰ã€‚ æ¯ä¸ªèŠ‚ç‚¹å°†é‚£ä¸ªç«¯å£ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ç›¸åŒç«¯å£å·ï¼‰ä»£ç†åˆ°ä½ çš„æœåŠ¡ä¸­ã€‚ ä½ çš„æœåŠ¡åœ¨å…¶ .spec.ports[*].nodePort å­—æ®µä¸­æŠ¥å‘Šå·²åˆ†é…çš„ç«¯å£ã€‚\nä½¿ç”¨ NodePort å¯ä»¥è®©ä½ è‡ªç”±è®¾ç½®è‡ªå·±çš„è´Ÿè½½å‡è¡¡è§£å†³æ–¹æ¡ˆï¼Œ é…ç½® Kubernetes ä¸å®Œå…¨æ”¯æŒçš„ç¯å¢ƒï¼Œ ç”šè‡³ç›´æ¥æš´éœ²ä¸€ä¸ªæˆ–å¤šä¸ªèŠ‚ç‚¹çš„ IP åœ°å€ã€‚\nå¯¹äº NodePort æœåŠ¡ï¼ŒKubernetes é¢å¤–åˆ†é…ä¸€ä¸ªç«¯å£ï¼ˆTCPã€UDP æˆ– SCTP ä»¥åŒ¹é…æœåŠ¡çš„åè®®ï¼‰ã€‚ é›†ç¾¤ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½å°†è‡ªå·±é…ç½®ä¸ºç›‘å¬åˆ†é…çš„ç«¯å£å¹¶å°†æµé‡è½¬å‘åˆ°ä¸è¯¥æœåŠ¡å…³è”çš„æŸä¸ªå°±ç»ªç«¯ç‚¹ã€‚ é€šè¿‡ä½¿ç”¨é€‚å½“çš„åè®®ï¼ˆä¾‹å¦‚ TCPï¼‰å’Œé€‚å½“çš„ç«¯å£ï¼ˆåˆ†é…ç»™è¯¥æœåŠ¡ï¼‰è¿æ¥åˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼Œ ä½ å°†èƒ½å¤Ÿä»é›†ç¾¤å¤–éƒ¨ä½¿ç”¨ type: NodePort æœåŠ¡ã€‚\napiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort selector: app.kubernetes.io/name: MyApp ports: # é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œ`targetPort` è¢«è®¾ç½®ä¸ºä¸ `port` å­—æ®µç›¸åŒçš„å€¼ã€‚ - port: 80 targetPort: 80 # å¯é€‰å­—æ®µ # é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼ŒKubernetes æ§åˆ¶å¹³é¢ä¼šä»æŸä¸ªèŒƒå›´å†…åˆ†é…ä¸€ä¸ªç«¯å£å·ï¼ˆé»˜è®¤ï¼š30000-32767ï¼‰ nodePort: 30007 LoadBalancer åœ¨ä½¿ç”¨æ”¯æŒå¤–éƒ¨è´Ÿè½½å‡è¡¡å™¨çš„äº‘æä¾›å•†çš„æœåŠ¡æ—¶ï¼Œè®¾ç½® type çš„å€¼ä¸º \u0026quot;LoadBalancer\u0026quot;ï¼Œ å°†ä¸º Service æä¾›è´Ÿè½½å‡è¡¡å™¨ã€‚ è´Ÿè½½å‡è¡¡å™¨æ˜¯å¼‚æ­¥åˆ›å»ºçš„ï¼Œå…³äºè¢«æä¾›çš„è´Ÿè½½å‡è¡¡å™¨çš„ä¿¡æ¯å°†ä¼šé€šè¿‡ Service çš„ status.loadBalancer å­—æ®µå‘å¸ƒå‡ºå»ã€‚\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 clusterIP: 10.0.171.239 type: LoadBalancer status: loadBalancer: ingress: - ip: 192.0.2.127 è‡ªå¤–éƒ¨è´Ÿè½½å‡è¡¡å™¨çš„æµé‡å°†ç›´æ¥é‡å®šå‘åˆ°åç«¯ Pod ä¸Šï¼Œä¸è¿‡å®é™…å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè¿™è¦ä¾èµ–äºäº‘æä¾›å•†ã€‚\næŸäº›äº‘æä¾›å•†å…è®¸è®¾ç½® loadBalancerIPã€‚ åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå°†æ ¹æ®ç”¨æˆ·è®¾ç½®çš„ loadBalancerIP æ¥åˆ›å»ºè´Ÿè½½å‡è¡¡å™¨ã€‚ å¦‚æœæ²¡æœ‰è®¾ç½® loadBalancerIP å­—æ®µï¼Œå°†ä¼šç»™è´Ÿè½½å‡è¡¡å™¨æŒ‡æ´¾ä¸€ä¸ªä¸´æ—¶ IPã€‚ å¦‚æœè®¾ç½®äº† loadBalancerIPï¼Œä½†äº‘æä¾›å•†å¹¶ä¸æ”¯æŒè¿™ç§ç‰¹æ€§ï¼Œé‚£ä¹ˆè®¾ç½®çš„ loadBalancerIP å€¼å°†ä¼šè¢«å¿½ç•¥æ‰ã€‚\nè¦å®ç° type: LoadBalancer çš„æœåŠ¡ï¼ŒKubernetes é€šå¸¸é¦–å…ˆè¿›è¡Œä¸è¯·æ±‚ type: NodePort æœåŠ¡ç­‰æ•ˆçš„æ›´æ”¹ã€‚ cloud-controller-manager ç»„ä»¶ç„¶åé…ç½®å¤–éƒ¨è´Ÿè½½å‡è¡¡å™¨ä»¥å°†æµé‡è½¬å‘åˆ°å·²åˆ†é…çš„èŠ‚ç‚¹ç«¯å£ã€‚\nExternalName ç±»å‹ä¸º ExternalName çš„æœåŠ¡å°†æœåŠ¡æ˜ å°„åˆ° DNS åç§°ï¼Œè€Œä¸æ˜¯å…¸å‹çš„é€‰æ‹©ç®—ç¬¦ï¼Œä¾‹å¦‚ my-service æˆ–è€… cassandraã€‚ ä½ å¯ä»¥ä½¿ç”¨ spec.externalName å‚æ•°æŒ‡å®šè¿™äº›æœåŠ¡ã€‚\nä¾‹å¦‚ï¼Œä»¥ä¸‹ Service å®šä¹‰å°† prod åç§°ç©ºé—´ä¸­çš„ my-service æœåŠ¡æ˜ å°„åˆ° my.database.example.com\napiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com è‡ªå®šä¹‰Service å‡è®¾æˆ‘ä»¬çš„etcdé›†ç¾¤åœ¨å¤–éƒ¨ï¼Œæˆ‘ä»¬æƒ³è¦é€šè¿‡Serviceè¿›è¡Œè®¿é—®ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œè‡ªå®šä¹‰çš„Serviceã€‚\napiVersion: v1 kind: Service metadata: name: my-service spec: type: ClusterIP ClusterIP: None ports: - name: etcd-port port: 2379 --- apiVersion: v1 kind: Endpoints metadata: name: custom-etcd-svc subsets: - address: - ip: 10.151.30.11 ports: - name: etcd-port port: 2379 è·å–å®¢æˆ·ç«¯IP é€šå¸¸ï¼Œå½“é›†ç¾¤å†…çš„å®¢æˆ·ç«¯è¿æ¥åˆ°æœåŠ¡çš„æ—¶å€™ï¼Œæ˜¯æ”¯æŒæœåŠ¡çš„ Pod å¯ä»¥è·å–åˆ°å®¢æˆ·ç«¯çš„ IP åœ°å€çš„ï¼Œä½†æ˜¯ï¼Œå½“é€šè¿‡èŠ‚ç‚¹ç«¯å£æ¥æ”¶åˆ°è¿æ¥æ—¶ï¼Œç”±äºå¯¹æ•°æ®åŒ…æ‰§è¡Œäº†æºç½‘ç»œåœ°å€è½¬æ¢ï¼ˆSNATï¼‰ï¼Œå› æ­¤æ•°æ®åŒ…çš„æº IP åœ°å€ä¼šå‘ç”Ÿå˜åŒ–ï¼Œåç«¯çš„ Pod æ— æ³•çœ‹åˆ°å®é™…çš„å®¢æˆ·ç«¯ IPï¼Œå¯¹äºæŸäº›åº”ç”¨æ¥è¯´æ˜¯ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚ï¼Œnginx çš„è¯·æ±‚æ—¥å¿—å°±æ— æ³•è·å–å‡†ç¡®çš„å®¢æˆ·ç«¯è®¿é—® IP äº†ã€‚\nå‡è®¾æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ç»„nginxé›†ç¾¤æœåŠ¡ï¼Œå½“æˆ‘ä»10.1.6.48è¿›è¡Œè®¿é—®çš„æ—¶å€™æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹æœ€ç»ˆå‘ˆç°ç»™æˆ‘ä»¬çš„åœ°å€\n10.10.207.192 - - [23/Feb/2023:06:09:24 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.61.1\u0026#34; \u0026#34;-\u0026#34; 10.10.207.192 - - [23/Feb/2023:06:10:50 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.61.1\u0026#34; \u0026#34;-\u0026#34; 10.10.207.192 - - [23/Feb/2023:06:10:52 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.61.1\u0026#34; \u0026#34;-\u0026#34; 10.10.207.192 - - [23/Feb/2023:06:10:53 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.61.1\u0026#34; \u0026#34;-\u0026#34; æ­£å¸¸æ¥è¯´æˆ‘å¾—åˆ°çš„åº”è¯¥æ˜¯å®¢æˆ·ç«¯çš„çœŸå®IPåœ°å€ï¼Œè€Œæˆ‘ç°åœ¨å¾—åˆ°çš„å´æ˜¯tunl0@NONEçš„IPåœ°å€\nè¿™ä¸ªæ—¶å€™æˆ‘ä»¬å¯ä»¥åœ¨ Service è®¾ç½® externalTrafficPolicy æ¥å‡å°‘ç½‘ç»œè·³æ•°\nkind: Service apiVersion: v1 metadata: name: public-beijing-nginx-service namespace: default spec: externalTrafficPolicy: Local ports: - name: beijing-nginx-http protocol: TCP port: 80 targetPort: 80 selector: app: beijing-nginx type: ClusterIP sessionAffinity: ClusterIP ipFamilies: - IPv4 ipFamilyPolicy: SingleStack internalTrafficPolicy: Cluster status: loadBalancer: {} ä½†è¿™å¯èƒ½å¯¼è‡´æµé‡åˆ†é…ä¸å‡ã€‚ æ²¡æœ‰é’ˆå¯¹ç‰¹å®š LoadBalancer æœåŠ¡çš„ä»»ä½• Pod çš„èŠ‚ç‚¹å°†æ— æ³•é€šè¿‡è‡ªåŠ¨åˆ†é…çš„ .spec.healthCheckNodePort è¿›è¡Œ NLB ç›®æ ‡ç»„çš„è¿è¡ŒçŠ¶å†µæ£€æŸ¥ï¼Œå¹¶ä¸”ä¸ä¼šæ”¶åˆ°ä»»ä½•æµé‡ã€‚\n","date":"2020-04-13T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/ServiceReader/","title":"kubernetes-Serviceè§£è¯»"},{"content":"ç®€å•å®‰è£…ä½¿ç”¨ æœ€æ–°ç‰ˆæœ¬åº”è¯¥æ˜¯1.4.1\ngit clone https://github.com/nacos-group/nacos-k8s.git ç®€å•ä½¿ç”¨ å¦‚æœä½ ä½¿ç”¨ç®€å•æ–¹å¼å¿«é€Ÿå¯åŠ¨,è¯·æ³¨æ„è¿™æ˜¯æ²¡æœ‰ä½¿ç”¨æŒä¹…åŒ–å·çš„,å¯èƒ½å­˜åœ¨æ•°æ®ä¸¢å¤±é£é™©:!!! cd nacos-k8s chmod +x quick-startup.sh ./quick-startup.sh æ¼”ç¤ºä½¿ç”¨ æœåŠ¡æ³¨å†Œ curl -X PUT \u0026#39;http://cluster-ip:8848/nacos/v1/ns/instance?serviceName=nacos.naming.serviceName\u0026amp;ip=20.18.7.10\u0026amp;port=8080\u0026#39; æœåŠ¡å‘ç° curl -X GET \u0026#39;http://cluster-ip:8848/nacos/v1/ns/instance/list?serviceName=nacos.naming.serviceName\u0026#39; å‘å¸ƒé…ç½® curl -X POST \u0026#34;http://cluster-ip:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId\u0026amp;group=test\u0026amp;content=helloWorld\u0026#34; è·å–é…ç½® curl -X GET \u0026#34;http://cluster-ip:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId\u0026amp;group=test\u0026#34; é«˜çº§ç”¨æ³• åœ¨é«˜çº§ä½¿ç”¨ä¸­,Nacosåœ¨K8Sæ‹¥æœ‰è‡ªåŠ¨æ‰©å®¹ç¼©å®¹å’Œæ•°æ®æŒä¹…ç‰¹æ€§,è¯·æ³¨æ„å¦‚æœéœ€è¦ä½¿ç”¨è¿™éƒ¨åˆ†åŠŸèƒ½è¯·ä½¿ç”¨PVCæŒä¹…å·,Nacosçš„è‡ªåŠ¨æ‰©å®¹ç¼©å®¹éœ€è¦ä¾èµ–æŒä¹…å·,ä»¥åŠæ•°æ®æŒä¹…åŒ–ä¹Ÿæ˜¯ä¸€æ ·,æœ¬ä¾‹ä¸­ä½¿ç”¨çš„æ˜¯NFSæ¥ä½¿ç”¨PVC.\néƒ¨ç½²NFS nfs-client-provisioner å¯åŠ¨æ€ä¸ºkubernetesæä¾›pvå·ï¼Œæ˜¯Kubernetesçš„ç®€æ˜“NFSçš„å¤–éƒ¨provisionerï¼Œæœ¬èº«ä¸æä¾›NFSï¼Œéœ€è¦ç°æœ‰çš„NFSæœåŠ¡å™¨æä¾›å­˜å‚¨ã€‚æŒä¹…å·ç›®å½•çš„å‘½åè§„åˆ™ä¸º: ${namespace}-${pvcName}-${pvName}\nåˆ›å»ºè§’è‰²\nkubectl create -f deploy/nfs/rbac.yaml ä¿®æ”¹NFSçš„yaml\nvim nacos-k8s/deploy/nfs/deployment.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 10.1.6.93 # ä¿®æ”¹NFSçš„IPä¸ºä½ æœ¬åœ°çš„NFSIPåœ°å€ - name: NFS_PATH value: /server/nacos volumes: - name: nfs-client-root nfs: server: 10.1.6.93 # åŒä¸Š path: /server/nacos éƒ¨ç½²NFS-client\nkubectl create -f deploy/nfs/deployment.yaml éƒ¨ç½²NFS StorageClass\nkubectl create -f deploy/nfs/class.yaml éªŒè¯nfs-client-provisioneræ˜¯å¦æˆåŠŸ\nkubectl get pod -l app=nfs-client-provisioner éƒ¨ç½²mysql kubectl create -f deploy/mysql/mysql-nfs.yaml éƒ¨ç½²Nacos ä¿®æ”¹ deploy/nacos/nacos-pvc-nfs.yaml å¯ä»¥è‡ªè¡Œé€‰æ‹©æ›´æ”¹\ndata: mysql.master.db.name: \u0026#34;ä¸»åº“åç§°\u0026#34; mysql.master.port: \u0026#34;ä¸»åº“ç«¯å£\u0026#34; mysql.slave.port: \u0026#34;ä»åº“ç«¯å£\u0026#34; mysql.master.user: \u0026#34;ä¸»åº“ç”¨æˆ·å\u0026#34; mysql.master.password: \u0026#34;ä¸»åº“å¯†ç \u0026#34; åˆ›å»ºNacos kubectl create -f nacos-k8s/deploy/nacos/nacos-pvc-nfs.yaml æ‰©å®¹æµ‹è¯• åœ¨æ‰©å®¹å‰ï¼Œä½¿ç”¨ kubectl execè·å–åœ¨podä¸­çš„Nacosé›†ç¾¤é…ç½®æ–‡ä»¶ä¿¡æ¯ for i in 0 1; do echo nacos-$i; kubectl exec nacos-$i cat conf/cluster.conf; done StatefulSetæ§åˆ¶å™¨æ ¹æ®å…¶åºæ•°ç´¢å¼•ä¸ºæ¯ä¸ªPodæä¾›å”¯ä¸€çš„ä¸»æœºåã€‚ ä¸»æœºåé‡‡ç”¨ - çš„å½¢å¼ã€‚ å› ä¸ºnacos StatefulSetçš„å‰¯æœ¬å­—æ®µè®¾ç½®ä¸º3ï¼Œæ‰€ä»¥å½“å‰é›†ç¾¤æ–‡ä»¶ä¸­åªæœ‰ä¸‰ä¸ªNacosèŠ‚ç‚¹åœ°å€\nä½¿ç”¨kubectl scale å¯¹NacosåŠ¨æ€æ‰©å®¹ kubectl scale sts nacos --replicas=5 ","date":"2020-04-07T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/kubernetes/InstallNacosCluster/","title":"åŸºäºkuberneteséƒ¨ç½²nacosé›†ç¾¤"},{"content":"kube-scheduler æ˜¯ kubernetes çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ï¼Œä¸»è¦è´Ÿè´£æ•´ä¸ªé›†ç¾¤èµ„æºçš„è°ƒåº¦åŠŸèƒ½ï¼Œæ ¹æ®ç‰¹å®šçš„è°ƒåº¦ç®—æ³•å’Œç­–ç•¥ï¼Œå°† Pod è°ƒåº¦åˆ°æœ€ä¼˜çš„å·¥ä½œèŠ‚ç‚¹ä¸Šé¢å»ï¼Œä»è€Œæ›´åŠ åˆç†ã€æ›´åŠ å……åˆ†çš„åˆ©ç”¨é›†ç¾¤çš„èµ„æºï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ kubernetes ä¸€ä¸ªéå¸¸é‡è¦çš„ç†ç”±ã€‚å¦‚æœä¸€é—¨æ–°çš„æŠ€æœ¯ä¸èƒ½å¸®åŠ©ä¼ä¸šèŠ‚çº¦æˆæœ¬ã€æä¾›æ•ˆç‡ï¼Œæˆ‘ç›¸ä¿¡æ˜¯å¾ˆéš¾æ¨è¿›çš„ã€‚\nè°ƒåº¦æµç¨‹ kube-scheduler æä¾›çš„é»˜è®¤è°ƒåº¦å™¨èƒ½å¤Ÿæ»¡è¶³æˆ‘ä»¬ç»å¤§å¤šæ•°çš„è¦æ±‚ï¼Œæˆ‘ä»¬å‰é¢å’Œå¤§å®¶æ¥è§¦çš„ç¤ºä¾‹ä¹ŸåŸºæœ¬ä¸Šç”¨çš„é»˜è®¤çš„ç­–ç•¥ï¼Œéƒ½å¯ä»¥ä¿è¯æˆ‘ä»¬çš„ Pod å¯ä»¥è¢«åˆ†é…åˆ°èµ„æºå……è¶³çš„èŠ‚ç‚¹ä¸Šè¿è¡Œã€‚ä½†æ˜¯åœ¨å®é™…çš„çº¿ä¸Šé¡¹ç›®ä¸­ï¼Œå¯èƒ½æˆ‘ä»¬è‡ªå·±ä¼šæ¯” kubernetes æ›´åŠ äº†è§£æˆ‘ä»¬è‡ªå·±çš„åº”ç”¨ï¼Œæ¯”å¦‚æˆ‘ä»¬å¸Œæœ›ä¸€ä¸ª Pod åªèƒ½è¿è¡Œåœ¨ç‰¹å®šçš„å‡ ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæˆ–è€…è¿™å‡ ä¸ªèŠ‚ç‚¹åªèƒ½ç”¨æ¥è¿è¡Œç‰¹å®šç±»å‹çš„åº”ç”¨ï¼Œè¿™å°±éœ€è¦æˆ‘ä»¬çš„è°ƒåº¦å™¨èƒ½å¤Ÿå¯æ§ã€‚\nå‘èµ·åˆ›å»ºDeploymentè¯·æ±‚-\u0026gt;API Server,è¿™ä¸ªæ—¶å€™APIServerä¼šè¿›è¡Œä¸€ç³»åˆ—çš„é€»è¾‘å¤„ç†,ä¾‹å¦‚: é‰´æƒã€æŸ¥çœ‹ä½ æ˜¯å¦æœ‰æƒé™æ“ä½œã€Deploymentåˆ›å»ºæ˜¯å¦åˆæ³•ç­‰ç­‰,ç„¶åå°†è¯·æ±‚å­˜å‚¨åˆ°etcdå½“ä¸­å¹¶ä¸”è½¬å‘ç»™Controller Manager Controller Managerä¼šç›‘å¬API Server,è¿™ä¸ªæ—¶å€™å‡è®¾ç›‘å¬åˆ°çš„æ˜¯ä¸€ä¸ªåˆ›å»ºDeploymentçš„è¯·æ±‚,åˆ™ä¼šæŠŠè¯·æ±‚è½¬å‘åˆ°Deployment Controller Deployment Controlleræ¥å—åˆ°è¯·æ±‚ååˆ›å»ºReplicaSet,ç„¶åReplicaSet Controllerä¼šæ ¹æ®yamlå½“ä¸­å®šä¹‰çš„templateæ¨¡æ¿æ¥è¿›è¡Œåˆ›å»ºPod,ç„¶åè¿”å›ç»™API Server åœ¨åˆ›å»ºä¹‹åˆçš„Podå±æ€§ä¸­nodeNameä¸ºç©º,ä¹Ÿå°±æ˜¯æ²¡æœ‰è¢«è°ƒåº¦è¿‡çš„,è¿™ä¸ªæ—¶å€™è°ƒåº¦å™¨å°±ä¼šå¯¹å®ƒè¿›è¡Œè°ƒåº¦,è°ƒåº¦å»watchPodå¯¹è±¡,ç„¶ååˆ†æé‚£ä¸ªèŠ‚ç‚¹æœ€é€‚åˆè¿™ä¸ªPod,ç„¶åå°†èŠ‚ç‚¹çš„åå­—é€šè¿‡ç±»ä¼¼äºbindçš„è¿™ç§æ–¹æ³•å†™å…¥åˆ°nodeNameå½“ä¸­ã€‚ ç„¶åè¯¥èŠ‚ç‚¹çš„kubeletä¼šè¿›è¡Œä¸€ç³»åˆ—çš„åˆ¤æ–­,ç„¶åè¿›å…¥Create Podçš„æµç¨‹,ç„¶åè¿›è¡Œä¸€ç³»åˆ—çš„CNIå’ŒCSIçš„è¿‡ç¨‹ã€‚ è¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„å¾€å¾€è¶Šç®€å•çš„ä¸œè¥¿,èƒŒåå®ç°çš„è¶Šå¤æ‚ã€‚\nè°ƒåº¦é˜¶æ®µ kube-schedulerè°ƒåº¦åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µ\npredicate: è¿‡æ»¤é˜¶æ®µï¼Œè¿‡æ»¤ä¸ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹ã€‚ priority: ä¼˜å…ˆçº§æ’åºï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„èŠ‚ç‚¹ï¼Œä¹Ÿå°±æ˜¯ç»™èŠ‚ç‚¹æ‰“åˆ†ã€‚ Predicatesç­–ç•¥ PodFitsHostPorts: æ£€æŸ¥æ˜¯å¦æœ‰Host Portså†²çª PodFitsPorts: åŒä¸Š PodFitsResources: æ£€æŸ¥Nodeçš„èµ„æºæ˜¯å¦å……è¶³ï¼ŒåŒ…æ‹¬å…è®¸çš„Podæ•°é‡ã€CPUã€å†…å­˜ã€GPUä¸ªæ•°ä»¥åŠå…¶ä»–çš„OpaqueIntResourcesã€‚ HostName:æ£€æŸ¥pod.Spec.NodeNameæ˜¯å¦ä¸å€™é€‰èŠ‚ç‚¹ä¸€è‡´ MatchNodeSelector:æ£€æŸ¥å€™é€‰èŠ‚ç‚¹çš„pod.Spec.NodeSelectoræ˜¯å¦åŒ¹é… NoVolumeZoneConflict:æ£€æŸ¥volume zoneæ˜¯å¦å†²çª Priorityç­–ç•¥ SelectorSpreadPriority: ä¼˜å…ˆå‡å°‘èŠ‚ç‚¹ä¸Šå±äºåŒä¸€ä¸ªServiceæˆ–Replication Controllerçš„Podæ•°é‡ã€‚ InterPodAffinityPriority: ä¼˜å…ˆå°†Podè°ƒåº¦åˆ°ç›¸åŒçš„æ‹“æ‰‘ä¸Š LeastRequestedPriority:ä¼˜å…ˆè°ƒåº¦åˆ°è¯·æ±‚èµ„æºå°‘çš„èŠ‚ç‚¹ä¸Š BalancedResourceAllocation: ä¼˜å…ˆå¹³è¡¡å„èŠ‚ç‚¹çš„èµ„æºä½¿ç”¨ NodePreferAvoidPodsPriority:æƒé‡åˆ¤æ–­ å¤ªå¤šäº†å¯ä»¥è‡ªå·±å»å®˜ç½‘äº†è§£ä¸€ä¸‹ï¼Œè¿™äº›ç­–ç•¥éƒ½å¯ä»¥é€šè¿‡scheduleré…ç½®æ–‡ä»¶å»é…ç½®ï¼Œå…¶å®ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬ä¸å¤ªéœ€è¦ï¼Œæˆ‘è§‰å¾—kubernetesçš„è°ƒåº¦æ˜¯æœ€è®©æˆ‘ä»¬çœå¿ƒçš„ã€‚\nèµ„æºéœ€æ±‚ requests:å±äºè°ƒåº¦å™¨è°ƒåº¦çš„æ—¶å€™æ‰€å‚è€ƒçš„æŒ‡æ ‡ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘è¿™ä¸ªåº”ç”¨æœ€å°‘éœ€è¦250mçš„cpuå’Œ256mçš„å†…å­˜æ‰èƒ½è¿è¡Œã€‚ kind: Deployment apiVersion: apps/v1 metadata: name: nginx-deployment namespace: default labels: app: nginx version: qa spec: replicas: 2 selector: matchLabels: app: nginx version: qa template: metadata: creationTimestamp: null labels: app: nginx version: qa spec: volumes: - name: host-time hostPath: path: /etc/localtime type: \u0026#39;\u0026#39; containers: - name: nginx image: nginx:latest ports: - name: http-web containerPort: 80 protocol: TCP resources: limits: cpu: \u0026#39;1\u0026#39; memory: 2Gi requests: cpu: 250m memory: 256Mi å¯ä»¥æŸ¥çœ‹ä½ èŠ‚ç‚¹çš„ä¸€äº›èµ„æºçŠ¶æ€\n[root@master1 ~]# kubectl get nodes -o yaml allocatable: cpu: 15600m ephemeral-storage: 104278276Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: \u0026#34;38390677064\u0026#34; pods: \u0026#34;330\u0026#34; capacity: cpu: \u0026#34;16\u0026#34; ephemeral-storage: 104278276Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: 40003048Ki pods: \u0026#34;330\u0026#34; å¯ä»¥çœ‹çœ‹è¿™ä¸ªdeploymentè¿è¡Œä»¥åæˆ‘ä»¬çš„cgroupå¯¹ä»–åšäº†å¦‚ä½•çš„é™åˆ¶\n\u0026#34;CgroupParent\u0026#34;: \u0026#34;/kubepods/burstable/pod1ceee26d-2ec2-43a8-96ef-5aa9ac99779b\u0026#34; è¿›å…¥è¿™ä¸ªç›®å½•\n[root@node1 ~]# cd /sys/fs/cgroup/cpu/kubepods/burstable/pod1ceee26d-2ec2-43a8-96ef-5aa9ac99779b [root@node1 pod1ceee26d-2ec2-43a8-96ef-5aa9ac99779b]# cat cpu.shares 358 kuberneteså¯¹äºä¸åŒçš„QOSçš„å¤„ç†æ–¹å¼æ˜¯ä¸ä¸€æ ·çš„ã€‚\nLimit-range ä¸€ä¸ª LimitRangeï¼ˆé™åˆ¶èŒƒå›´ï¼‰ å¯¹è±¡æä¾›çš„é™åˆ¶èƒ½å¤Ÿåšåˆ°ï¼š\nåœ¨ä¸€ä¸ªå‘½åç©ºé—´ä¸­å®æ–½å¯¹æ¯ä¸ª Pod æˆ– Container æœ€å°å’Œæœ€å¤§çš„èµ„æºä½¿ç”¨é‡çš„é™åˆ¶ã€‚ åœ¨ä¸€ä¸ªå‘½åç©ºé—´ä¸­å®æ–½å¯¹æ¯ä¸ª PersistentVolumeClaim èƒ½ç”³è¯·çš„æœ€å°å’Œæœ€å¤§çš„å­˜å‚¨ç©ºé—´å¤§å°çš„é™åˆ¶ã€‚ åœ¨ä¸€ä¸ªå‘½åç©ºé—´ä¸­å®æ–½å¯¹ä¸€ç§èµ„æºçš„ç”³è¯·å€¼å’Œé™åˆ¶å€¼çš„æ¯”å€¼çš„æ§åˆ¶ã€‚ è®¾ç½®ä¸€ä¸ªå‘½åç©ºé—´ä¸­å¯¹è®¡ç®—èµ„æºçš„é»˜è®¤ç”³è¯·/é™åˆ¶å€¼ï¼Œå¹¶ä¸”è‡ªåŠ¨çš„åœ¨è¿è¡Œæ—¶æ³¨å…¥åˆ°å¤šä¸ª Container ä¸­ã€‚ å½“æŸå‘½åç©ºé—´ä¸­æœ‰ä¸€ä¸ª LimitRange å¯¹è±¡æ—¶ï¼Œå°†åœ¨è¯¥å‘½åç©ºé—´ä¸­å®æ–½ LimitRange é™åˆ¶ã€‚\napiVersion: v1 kind: LimitRange metadata: name: cpu-resource-constraint spec: limits: - default: # æ­¤å¤„å®šä¹‰é»˜è®¤é™åˆ¶å€¼ cpu: 500m defaultRequest: # æ­¤å¤„å®šä¹‰é»˜è®¤è¯·æ±‚å€¼ cpu: 500m max: # max å’Œ min å®šä¹‰é™åˆ¶èŒƒå›´ cpu: \u0026#34;1\u0026#34; min: cpu: 100m type: Container è¿™ä¸œè¥¿å…¶å®æ˜¯ä¸å¤ªå¸¸ç”¨çš„\nç”Ÿäº§ç¯å¢ƒéœ€è¦è€ƒè™‘çš„é—®é¢˜ æ˜¯å¦å…¬å¹³è°ƒåº¦ èµ„æºæ˜¯å¦é«˜æ•ˆåˆ©ç”¨ QOS affinityå’Œanti-affinity æ•°æ®æœ¬åœ°åŒ– å†…éƒ¨è´Ÿè½½å¹²æ‰°(inter-workload interference) deadlines ","date":"2020-03-14T00:00:00Z","image":"https://img14.360buyimg.com/ddimg/jfs/t1/164569/9/40677/14419/65bc6e4cFa1d8c0c3/5ccf7e6caadc9b83.jpg","permalink":"http://localhost:1313/posts/scheduler/","title":"kubernetes-Schedulerç®€å•è¯¦è§£"}]